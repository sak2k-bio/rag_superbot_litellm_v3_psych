This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/workflows/deploy.yml
.gitignore
check-collection.mjs
config.env
deploy-cloudflare.sh
deploy-railway.sh
deploy-render.sh
deploy-vercel.sh
DEPLOYMENT_SUMMARY.md
DEPLOYMENT.md
DOCKER-COMPOSE-TO-RAILWAY.md
DOCKER-COMPOSE-TO-RENDER.md
docker-compose.railway.yml
docker-compose.render.yml
docker-compose.yml
Dockerfile.fastapi
Dockerfile.railway
Dockerfile.render
eslint.config.mjs
fastapi_server.py
FRONTEND_SETUP.md
INTEGRATION_SUMMARY.md
list-collections.mjs
LITELLM_INTEGRATION.md
next.config.ts
package-cloudflare.json
package.json
postcss.config.mjs
public/file.svg
public/globe.svg
public/next.svg
public/vercel.svg
public/window.svg
QUICKSTART.md
railway-env-template.txt
railway.json
README.md
RENDER-DEPLOYMENT-GUIDE.md
render-env-template.txt
RENDER-FREE-TIER.md
render-python311.yaml
RENDER-SOLUTION.md
render.yaml
requirements-minimal.txt
requirements-render-minimal.txt
requirements-render.txt
requirements.txt
runtime.txt
setup-deployment.sh
setup-qdrant-collection.mjs
simple_server.py
src/app/api/chat/route.ts
src/app/api/documents/route.ts
src/app/api/sample-documents/route.ts
src/app/api/status/route.ts
src/app/favicon.ico
src/app/globals.css
src/app/layout.tsx
src/app/page.tsx
src/lib/agents.ts
src/lib/litellm-client.ts
src/lib/pipelines.ts
src/lib/vectorstore.ts
src/worker.js
start-services.ps1
stop-services.ps1
test_1minai.py
test_alternative_api.py
test_fastapi_1minai.py
test-google-embedding.mjs
tsconfig.json
VERCEL_DEPLOYMENT.md
vercel-env-production.txt
vercel-env-template.txt
vercel.json
VPS-DEPLOYMENT-GUIDE.md
wrangler.toml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/workflows/deploy.yml">
name: Deploy Psychiatry Therapy SuperBot

on:
  push:
    branches: [main]
  workflow_dispatch:

jobs:
  deploy-backend:
    name: Deploy Backend to Railway
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      - name: Install Railway CLI
        run: npm install -g @railway/cli

      - name: Deploy to Railway
        run: railway up --service ${{ secrets.RAILWAY_SERVICE_ID }}
        env:
          RAILWAY_TOKEN: ${{ secrets.RAILWAY_TOKEN }}

  deploy-frontend:
    name: Deploy Frontend to Vercel
    runs-on: ubuntu-latest
    needs: deploy-backend
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      - name: Install dependencies
        run: |
          cd frontend
          npm install --legacy-peer-deps

      - name: Build project
        run: |
          cd frontend
          npm run build

      - name: Deploy to Vercel
        run: |
          cd frontend
          npx vercel --prod --token ${{ secrets.VERCEL_TOKEN }} --yes
        env:
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
          VERCEL_ORG_ID: ${{ secrets.VERCEL_ORG_ID }}
          VERCEL_PROJECT_ID: ${{ secrets.VERCEL_PROJECT_ID }}
</file>

<file path="check-collection.mjs">
#!/usr/bin/env node

import { QdrantClient } from '@qdrant/js-client-rest';
import { GoogleGenerativeAI } from '@google/generative-ai';
import dotenv from 'dotenv';

dotenv.config({ path: './.env.local' });

const QDRANT_CLOUD_URL = process.env.NEXT_PUBLIC_QDRANT_CLOUD_URL;
const QDRANT_CLOUD_API_KEY = process.env.NEXT_PUBLIC_QDRANT_CLOUD_API_KEY;
const COLLECTION_NAME = process.env.NEXT_PUBLIC_COLLECTION_NAME;
const GOOGLE_API_KEY = process.env.NEXT_PUBLIC_GOOGLE_API_KEY;
const EMBEDDING_MODEL = process.env.NEXT_PUBLIC_EMBEDDING_MODEL;

async function main() {
  console.log("üîç Checking collection contents (READ-ONLY)...\n");

  const client = new QdrantClient({
    url: QDRANT_CLOUD_URL,
    apiKey: QDRANT_CLOUD_API_KEY
  });

  try {
    // Get collection info
    const info = await client.getCollection(COLLECTION_NAME);
    console.log(`üìä Collection '${COLLECTION_NAME}' stats:`);
    console.log(`  - Points count: ${info.points_count}`);
    console.log(`  - Vector dimensions: ${info.config?.params?.vectors?.size}`);
    console.log(`  - Distance metric: ${info.config?.params?.vectors?.distance}`);

    if (info.points_count === 0) {
      console.log("\n‚ùå Collection is empty - that's why retrieval isn't working!");
      console.log("üí° You need to load documents first using the app's 'Load Sample Docs' button or API");
      return;
    }

    // Get a few sample points to see what's in there
    console.log("\nüìã Sample documents in collection:");
    try {
      const points = await client.scroll(COLLECTION_NAME, {
        limit: 5,
        with_payload: true,
        with_vector: false
      });

      if (points.points && points.points.length > 0) {
        points.points.forEach((point, index) => {
          console.log(`\nüìÑ Document ${index + 1} (ID: ${point.id}):`);
          if (point.payload.title) console.log(`  Title: ${point.payload.title}`);
          if (point.payload.category) console.log(`  Category: ${point.payload.category}`);
          if (point.payload.content) {
            const preview = point.payload.content.substring(0, 150);
            console.log(`  Content: ${preview}${point.payload.content.length > 150 ? '...' : ''}`);
          }
        });
      }
    } catch (scrollError) {
      console.log("‚ö†Ô∏è  Could not retrieve sample documents:", scrollError.message);
    }

    // Test search for "thoracocentesis"
    console.log("\nüîç Testing search for 'thoracocentesis'...");
    await testSearch(client, "thoracocentesis");

    // Test search for a more general term
    console.log("\nüîç Testing search for 'lung'...");
    await testSearch(client, "lung");

  } catch (error) {
    console.error("‚ùå Error:", error.message);
  }
}

async function testSearch(client, searchTerm) {
  try {
    // Generate embedding for search term
    const genAI = new GoogleGenerativeAI(GOOGLE_API_KEY);
    const model = genAI.getGenerativeModel({ model: EMBEDDING_MODEL });
    const result = await model.embedContent(searchTerm);
    const queryEmbedding = result.embedding.values;

    console.log(`  Generated ${queryEmbedding.length}-dimensional embedding for "${searchTerm}"`);

    const searchResults = await client.search(COLLECTION_NAME, {
      vector: queryEmbedding,
      limit: 3,
      with_payload: true,
      score_threshold: 0.1  // Lower threshold to see more results
    });

    if (searchResults.length === 0) {
      console.log(`  ‚ùå No documents found for "${searchTerm}"`);
    } else {
      console.log(`  ‚úÖ Found ${searchResults.length} documents:`);
      searchResults.forEach((result, index) => {
        console.log(`    ${index + 1}. Score: ${result.score.toFixed(4)}`);
        if (result.payload.title) console.log(`       Title: ${result.payload.title}`);
        if (result.payload.content) {
          const preview = result.payload.content.substring(0, 100);
          console.log(`       Content: ${preview}...`);
        }
      });
    }
  } catch (error) {
    console.error(`  ‚ùå Search error for "${searchTerm}":`, error.message);
  }
}

main().catch(console.error);
</file>

<file path="config.env">
# Google Gemini Configuration (Fallback/Embeddings only)
NEXT_PUBLIC_GOOGLE_API_KEY=your_gemini_api_key_here
NEXT_PUBLIC_GEMINI_MODEL=gemini-1.5-flash
NEXT_PUBLIC_GEMINI_TEMPERATURE=0.7
NEXT_PUBLIC_GEMINI_MAX_TOKENS=2048
NEXT_PUBLIC_EMBEDDING_MODEL=gemini-embedding-001
NEXT_PUBLIC_EMBEDDING_DIM=3072

# LiteLLM / 1minAI Configuration (Primary)
NEXT_PUBLIC_USE_LITELLM=true
NEXT_PUBLIC_LITELLM_API_URL=http://localhost:8000
NEXT_PUBLIC_LITELLM_MODEL=gemini-2.0-flash-lite
NEXT_PUBLIC_LITELLM_TEMPERATURE=0.7
NEXT_PUBLIC_LITELLM_MAX_TOKENS=2048
ONEMINAI_API_KEY=your_1minai_api_key_here

# Qdrant Cloud Configuration
NEXT_PUBLIC_QDRANT_CLOUD_URL=https://your-cluster-id.eu-central.aws.cloud.qdrant.io
NEXT_PUBLIC_QDRANT_CLOUD_API_KEY=your_qdrant_cloud_api_key_here

# Vector Store Configuration
NEXT_PUBLIC_VECTOR_STORE=qdrant
NEXT_PUBLIC_COLLECTION_NAME=rag_a2a_collection

# Fallback Configuration (for local development)
NEXT_PUBLIC_QDRANT_HOST=localhost
NEXT_PUBLIC_QDRANT_PORT=6333
NEXT_PUBLIC_OLLAMA_HOST=http://localhost:11434

# Chroma Configuration (if using Chroma)
NEXT_PUBLIC_CHROMA_PATH=./chroma_db
</file>

<file path="deploy-cloudflare.sh">
#!/bin/bash

# Deployment script for Psychiatry Therapy SuperBot to Cloudflare Workers

echo "üöÄ Deploying Psychiatry Therapy SuperBot API to Cloudflare Workers..."

# Check if wrangler is installed
if ! command -v wrangler &> /dev/null; then
    echo "‚ùå Wrangler CLI not found. Installing..."
    npm install -g wrangler
fi

# Check if logged in to Cloudflare
echo "üîê Checking Cloudflare authentication..."
if ! wrangler whoami &> /dev/null; then
    echo "‚ùå Not logged in to Cloudflare. Please run: wrangler login"
    exit 1
fi

# Set secrets (you'll need to run these manually first time)
echo "üîë Setting up secrets..."
echo "Please make sure you've set the following secrets:"
echo "  wrangler secret put ONEMINAI_API_KEY"
echo ""

# Deploy to staging first
echo "üß™ Deploying to staging environment..."
wrangler deploy --env staging

if [ $? -eq 0 ]; then
    echo "‚úÖ Staging deployment successful!"
    echo "üåê Staging URL: https://psychiatry-therapy-superbot-api-staging.YOUR_SUBDOMAIN.workers.dev"
    echo ""
    
    # Ask for production deployment
    read -p "Deploy to production? (y/N): " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        echo "üöÄ Deploying to production..."
        wrangler deploy --env production
        
        if [ $? -eq 0 ]; then
            echo "‚úÖ Production deployment successful!"
            echo "üåê Production URL: https://psychiatry-therapy-superbot-api.YOUR_SUBDOMAIN.workers.dev"
            echo ""
            echo "üéâ Deployment complete!"
            echo ""
            echo "üìã Next steps:"
            echo "1. Update your Vercel environment variables with the new API URL"
            echo "2. Test the API endpoints"
            echo "3. Deploy your frontend to Vercel"
        else
            echo "‚ùå Production deployment failed!"
            exit 1
        fi
    else
        echo "‚è≠Ô∏è  Skipping production deployment"
    fi
else
    echo "‚ùå Staging deployment failed!"
    exit 1
fi
</file>

<file path="deploy-railway.sh">
#!/bin/bash

# Deployment script for Psychiatry Therapy SuperBot Docker Compose to Railway

echo "üöÇ Deploying Psychiatry Therapy SuperBot Docker Compose to Railway..."

# Check if railway CLI is installed
if ! command -v railway &> /dev/null; then
    echo "‚ùå Railway CLI not found. Installing..."
    npm install -g @railway/cli
fi

# Check if logged in to Railway
echo "üîê Checking Railway authentication..."
if ! railway whoami &> /dev/null; then
    echo "‚ùå Not logged in to Railway. Please run: railway login"
    exit 1
fi

# Check if project exists, if not create one
echo "üìã Setting up Railway project..."
if [ ! -f ".railway" ] && [ ! -d ".railway" ]; then
    echo "Creating new Railway project..."
    railway init --name "psychiatry-therapy-superbot"
else
    echo "‚úÖ Railway project already configured"
fi

# Set environment variables from docker-compose
echo "üîë Setting up environment variables from docker-compose..."
echo "This will set all the environment variables defined in your docker-compose.yml"
echo ""

# Ask if user wants to set variables via CLI
read -p "Do you want to set environment variables via CLI now? (y/N): " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "Setting environment variables from docker-compose configuration..."
    
    # Prompt for 1minAI API key (the only secret needed)
    read -p "Enter your 1minAI API key: " ONEMINAI_KEY
    if [ ! -z "$ONEMINAI_KEY" ]; then
        railway variables set ONEMINAI_API_KEY="$ONEMINAI_KEY"
    fi
    
    # Set all other variables from docker-compose.yml
    echo "Setting FastAPI configuration..."
    railway variables set FASTAPI_HOST=0.0.0.0
    railway variables set FASTAPI_PORT=8000
    railway variables set FASTAPI_RELOAD=false
    railway variables set FASTAPI_LOG_LEVEL=info
    
    echo "Setting LiteLLM configuration..."
    railway variables set LITELLM_BASE_URL=https://api.1min.ai
    railway variables set DEFAULT_MODEL=gemini-2.0-flash-lite
    railway variables set MAX_TOKENS=4096
    railway variables set TEMPERATURE=0.7
    
    echo "Setting CORS configuration..."
    railway variables set CORS_ORIGINS="*"
    railway variables set CORS_ALLOW_CREDENTIALS=true
    
    echo "Setting health check configuration..."
    railway variables set HEALTH_CHECK_INTERVAL=30
    
    echo "‚úÖ All environment variables set!"
else
    echo "‚è≠Ô∏è  Skipping environment variable setup."
    echo "üìù Please set these variables in Railway dashboard:"
    echo "  ONEMINAI_API_KEY=your_1minai_api_key_here"
    echo "  FASTAPI_HOST=0.0.0.0"
    echo "  FASTAPI_PORT=8000"
    echo "  FASTAPI_RELOAD=false"
    echo "  FASTAPI_LOG_LEVEL=info"
    echo "  LITELLM_BASE_URL=https://api.1min.ai"
    echo "  DEFAULT_MODEL=gemini-2.0-flash-lite"
    echo "  MAX_TOKENS=4096"
    echo "  TEMPERATURE=0.7"
    echo "  CORS_ORIGINS=*"
    echo "  CORS_ALLOW_CREDENTIALS=true"
    echo "  HEALTH_CHECK_INTERVAL=30"
fi

# Deploy to Railway using the Dockerfile (Railway will build the same image as docker-compose)
echo ""
echo "üöÄ Deploying Docker container to Railway..."
echo "üì¶ Railway will build using Dockerfile.fastapi (same as docker-compose)"
railway up

if [ $? -eq 0 ]; then
    echo "‚úÖ Railway deployment successful!"
    echo ""
    
    # Get the deployment URL
    echo "üîç Getting deployment URL..."
    RAILWAY_URL=$(railway domain 2>/dev/null || echo "")
    
    if [ -z "$RAILWAY_URL" ]; then
        echo "üåê Getting Railway service URL..."
        railway status
        echo ""
        echo "üí° Your service is deployed! Get the URL from 'railway status' or Railway dashboard"
    else
        echo "üåê Your API is deployed at: https://$RAILWAY_URL"
        echo ""
        echo "üß™ Test your deployment:"
        echo "  Health check: curl https://$RAILWAY_URL/health"
        echo "  Models: curl https://$RAILWAY_URL/v1/models"
        echo ""
        echo "üîß Update your frontend .env with:"
        echo "  NEXT_PUBLIC_LITELLM_API_URL=https://$RAILWAY_URL"
    fi
    
    echo ""
    echo "üéâ Docker Compose deployment to Railway complete!"
    echo ""
    echo "üìã Next steps:"
    echo "1. Test your API endpoints"
    echo "2. Update your Vercel environment variables with the Railway URL"
    echo "3. Deploy your frontend to Vercel"
    echo ""
    echo "üìä Monitor your deployment:"
    echo "  View logs: railway logs"
    echo "  Check status: railway status"
    echo "  Open dashboard: railway open"
else
    echo "‚ùå Railway deployment failed!"
    echo "üîç Check logs with: railway logs"
    exit 1
fi
</file>

<file path="DEPLOYMENT_SUMMARY.md">
# Vercel Deployment Summary

## ‚úÖ What's Been Done

### 1. Package Configuration
- ‚úÖ Updated `package.json` with proper scripts and dependencies
- ‚úÖ Removed unnecessary dependencies (cors, express, dotenv, ollama)
- ‚úÖ Added Node.js engine requirements
- ‚úÖ Added postinstall script to disable telemetry

### 2. Vercel Configuration
- ‚úÖ Created `vercel.json` with proper configuration
- ‚úÖ Updated `next.config.ts` for Vercel optimization
- ‚úÖ Fixed deprecated `serverComponentsExternalPackages` configuration
- ‚úÖ Added CORS headers and redirects

### 3. Code Updates for Vercel
- ‚úÖ Updated `vectorstore.ts` with Vercel-compatible embedding fallback
- ‚úÖ Updated `agents.ts` to handle Vercel environment (no Ollama fallback)
- ‚úÖ Added environment detection for Vercel vs local development
- ‚úÖ Implemented fallback embedding generation for Vercel

### 4. Environment Variables
- ‚úÖ Created `vercel-env-template.txt` with all required variables
- ‚úÖ Updated configuration to use `NEXT_PUBLIC_` prefixed variables
- ‚úÖ Documented Qdrant Cloud requirements for Vercel

### 5. Build Optimization
- ‚úÖ Fixed ESLint configuration to handle TypeScript warnings
- ‚úÖ Verified successful build process
- ‚úÖ Optimized for production deployment

### 6. Documentation
- ‚úÖ Created comprehensive `VERCEL_DEPLOYMENT.md` guide
- ‚úÖ Updated `README.md` with Vercel deployment instructions
- ‚úÖ Added deployment checklist

## üöÄ Ready for Deployment

The frontend is now fully ready for Vercel deployment with:

### Required Environment Variables
```bash
NEXT_PUBLIC_GOOGLE_API_KEY=your_gemini_api_key
NEXT_PUBLIC_QDRANT_CLOUD_URL=your_qdrant_cloud_url
NEXT_PUBLIC_QDRANT_CLOUD_API_KEY=your_qdrant_cloud_key
NEXT_PUBLIC_VECTOR_STORE=qdrant
NEXT_PUBLIC_COLLECTION_NAME=rag_a2a_collection
```

### Key Features for Vercel
- ‚úÖ Google Gemini 1.5 Flash integration
- ‚úÖ Qdrant Cloud vector database support
- ‚úÖ Automatic environment detection
- ‚úÖ Fallback embedding generation
- ‚úÖ Optimized build configuration
- ‚úÖ CORS and security headers

### Deployment Steps
1. Push code to GitHub
2. Connect repository to Vercel
3. Set environment variables in Vercel dashboard
4. Deploy automatically

## üìÅ Files Created/Modified

### New Files
- `vercel.json` - Vercel configuration
- `vercel-env-template.txt` - Environment variables template
- `VERCEL_DEPLOYMENT.md` - Comprehensive deployment guide
- `DEPLOYMENT_SUMMARY.md` - This summary

### Modified Files
- `package.json` - Updated dependencies and scripts
- `next.config.ts` - Vercel optimization
- `eslint.config.mjs` - Fixed TypeScript warnings
- `src/lib/vectorstore.ts` - Vercel-compatible embeddings
- `src/lib/agents.ts` - Vercel environment handling
- `README.md` - Added Vercel deployment section

## üéØ Next Steps

1. **Get API Keys**:
   - Google Gemini API key from [Google AI Studio](https://makersuite.google.com/app/apikey)
   - Qdrant Cloud account and cluster from [Qdrant Cloud](https://cloud.qdrant.io/)

2. **Deploy to Vercel**:
   - Follow the instructions in `VERCEL_DEPLOYMENT.md`
   - Use the checklist in `README.md`

3. **Test Deployment**:
   - Load sample documents
   - Test different pipeline modes
   - Verify all functionality works

## ‚ö†Ô∏è Important Notes

- **Ollama is not supported on Vercel** - Use Google Gemini as primary LLM
- **Qdrant Cloud is required** - Local Qdrant won't work on Vercel
- **Environment variables must be set** - Without them, the app won't function
- **Build warnings are acceptable** - They don't prevent deployment

The application is now production-ready for Vercel deployment! üöÄ
</file>

<file path="DOCKER-COMPOSE-TO-RAILWAY.md">
# Docker Compose to Railway Migration Guide

This document explains how your `docker-compose.yml` configuration translates to Railway deployment.

## üîÑ Configuration Mapping

### Docker Compose Configuration
```yaml
# docker-compose.yml
services:
  fastapi-litellm:
    build:
      context: .
      dockerfile: Dockerfile.fastapi
    container_name: rag-superbot-litellm-proxy
    ports:
      - "8000:8000"
    environment:
      - ONEMINAI_API_KEY=${ONEMINAI_API_KEY}
      - FASTAPI_HOST=0.0.0.0
      - FASTAPI_PORT=8000
      # ... other environment variables
    volumes:
      - ./fastapi_server.py:/app/fastapi_server.py
      - ./requirements.txt:/app/requirements.txt
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
    restart: unless-stopped
```

### Railway Equivalent
```json
// railway.json
{
  "build": {
    "builder": "DOCKERFILE",
    "dockerfilePath": "Dockerfile.fastapi"
  },
  "deploy": {
    "healthcheckPath": "/health",
    "restartPolicyType": "ON_FAILURE"
  }
}
```

## üîß Key Differences

| Docker Compose | Railway | Notes |
|----------------|---------|-------|
| `ports: "8000:8000"` | Uses `PORT` env var | Railway automatically assigns port |
| `container_name` | Auto-generated | Railway manages container naming |
| `volumes` | Not needed | Railway builds from source |
| `networks` | Auto-managed | Railway handles networking |
| `restart: unless-stopped` | Built-in | Railway auto-restarts on failure |

## üåê Environment Variables

All your docker-compose environment variables work the same way in Railway:

```bash
# Set via Railway CLI (same as docker-compose)
railway variables set ONEMINAI_API_KEY=your_key
railway variables set FASTAPI_HOST=0.0.0.0
railway variables set FASTAPI_PORT=8000
railway variables set LITELLM_BASE_URL=https://api.1min.ai
railway variables set DEFAULT_MODEL=gemini-2.0-flash-lite
railway variables set MAX_TOKENS=4096
railway variables set TEMPERATURE=0.7
railway variables set CORS_ORIGINS="*"
railway variables set CORS_ALLOW_CREDENTIALS=true
railway variables set HEALTH_CHECK_INTERVAL=30
```

## üöÄ Deployment Process

### Docker Compose (Local)
```bash
docker-compose up --build
```

### Railway (Cloud)
```bash
railway up
```

Both commands:
1. Build the Docker image using `Dockerfile.fastapi`
2. Set environment variables
3. Start the FastAPI server
4. Enable health checks
5. Handle automatic restarts

## üìä Monitoring & Logs

### Docker Compose
```bash
docker-compose logs -f fastapi-litellm
docker stats
```

### Railway
```bash
railway logs --follow
railway status
```

## üîÑ Development Workflow

### Local Development (Docker Compose)
```bash
# Start services
docker-compose up

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

### Railway Development
```bash
# Deploy changes
railway up

# View logs
railway logs

# Check status
railway status
```

## üéØ Benefits of Railway vs Local Docker Compose

| Feature | Docker Compose | Railway |
|---------|----------------|---------|
| **Deployment** | Manual server setup | Automatic cloud deployment |
| **Scaling** | Manual container management | Auto-scaling based on traffic |
| **Monitoring** | Basic Docker stats | Built-in metrics & alerts |
| **SSL/HTTPS** | Manual setup required | Automatic SSL certificates |
| **Domain** | Manual DNS setup | Automatic Railway domain + custom domains |
| **Logs** | Local only | Persistent cloud logs |
| **Backups** | Manual | Automatic |
| **Updates** | Manual rebuild | Git-based deployments |

## üîß Migration Checklist

- [x] ‚úÖ Same Dockerfile (`Dockerfile.fastapi`)
- [x] ‚úÖ Same environment variables
- [x] ‚úÖ Same health check endpoint (`/health`)
- [x] ‚úÖ Same FastAPI application code
- [x] ‚úÖ Same port configuration (with Railway's PORT handling)
- [x] ‚úÖ Same restart policies
- [x] ‚úÖ Same CORS configuration

## üöÄ Quick Migration

1. **Keep your docker-compose.yml** (for local development)
2. **Deploy to Railway** using the same configuration
3. **Test both environments** to ensure consistency

```bash
# Local testing
docker-compose up

# Railway deployment
railway up

# Both should work identically!
```

Your Docker Compose setup is now running in the cloud with Railway's additional benefits like auto-scaling, monitoring, and automatic deployments! üéâ
</file>

<file path="docker-compose.railway.yml">
# Railway-optimized Docker Compose for Psychiatry Therapy SuperBot
# This file shows the equivalent configuration that Railway will use

version: '3.8'

services:
  # FastAPI server for LiteLLM 1minAI proxy (Railway deployment)
  fastapi-litellm:
    build:
      context: .
      dockerfile: Dockerfile.fastapi
    container_name: psychiatry-therapy-superbot-api
    ports:
      - "${PORT:-8000}:8000"  # Railway uses PORT env var
    environment:
      # Railway will inject these from environment variables
      - ONEMINAI_API_KEY=${ONEMINAI_API_KEY}
      - FASTAPI_HOST=0.0.0.0
      - FASTAPI_PORT=8000
      - FASTAPI_RELOAD=false
      - FASTAPI_LOG_LEVEL=info
      - LITELLM_BASE_URL=https://api.1min.ai
      - DEFAULT_MODEL=gemini-2.0-flash-lite
      - MAX_TOKENS=4096
      - TEMPERATURE=0.7
      - CORS_ORIGINS=*
      - CORS_ALLOW_CREDENTIALS=true
      - HEALTH_CHECK_INTERVAL=30
      # Railway-specific
      - PORT=${PORT:-8000}
      - RAILWAY_ENVIRONMENT=${RAILWAY_ENVIRONMENT:-production}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

# Note: Railway handles networking automatically, no custom networks needed
</file>

<file path="docker-compose.render.yml">
# Render-equivalent Docker Compose for Psychiatry Therapy SuperBot
# This shows how your docker-compose.yml translates to Render deployment

version: '3.8'

services:
  # FastAPI server for LiteLLM 1minAI proxy (Render deployment equivalent)
  fastapi-litellm:
    build:
      context: .
      dockerfile: Dockerfile.fastapi  # Render uses the same Dockerfile
    container_name: psychiatry-therapy-superbot-api
    ports:
      - "${PORT:-10000}:10000"  # Render uses PORT env var (default 10000)
    environment:
      # Same environment variables as original docker-compose.yml
      - ONEMINAI_API_KEY=${ONEMINAI_API_KEY}
      - FASTAPI_HOST=0.0.0.0
      - FASTAPI_PORT=${PORT:-10000}  # Render uses PORT instead of 8000
      - FASTAPI_RELOAD=false
      - FASTAPI_LOG_LEVEL=info
      - LITELLM_BASE_URL=https://api.1min.ai
      - DEFAULT_MODEL=gemini-2.0-flash-lite
      - MAX_TOKENS=4096
      - TEMPERATURE=0.7
      - CORS_ORIGINS=*
      - CORS_ALLOW_CREDENTIALS=true
      - HEALTH_CHECK_INTERVAL=30
      # Render-specific
      - PORT=${PORT:-10000}
      - RENDER_SERVICE_NAME=psychiatry-therapy-superbot-api
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-10000}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

# Note: Render handles networking, SSL, and scaling automatically
</file>

<file path="docker-compose.yml">
version: '3.8'

services:
  # FastAPI server for LiteLLM 1minAI proxy
  fastapi-litellm:
    build:
      context: .
      dockerfile: Dockerfile.fastapi
    container_name: rag-superbot-litellm-proxy
    ports:
      - "8000:8000"
    environment:
      - ONEMINAI_API_KEY=${ONEMINAI_API_KEY}
      - FASTAPI_HOST=0.0.0.0
      - FASTAPI_PORT=8000
      - FASTAPI_RELOAD=false
      - FASTAPI_LOG_LEVEL=info
      - LITELLM_BASE_URL=https://api.1min.ai
      - DEFAULT_MODEL=gemini-2.0-flash-lite
      - MAX_TOKENS=4096
      - TEMPERATURE=0.7
      - CORS_ORIGINS=*
      - CORS_ALLOW_CREDENTIALS=true
      - HEALTH_CHECK_INTERVAL=30
    volumes:
      - ./fastapi_server.py:/app/fastapi_server.py
      - ./requirements.txt:/app/requirements.txt
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    networks:
      - rag-superbot-network

networks:
  rag-superbot-network:
    name: rag-superbot-network
    driver: bridge
</file>

<file path="Dockerfile.railway">
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first for better caching
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy application files
COPY fastapi_server.py .

# Railway uses PORT environment variable
ENV FASTAPI_PORT=${PORT:-8000}
ENV FASTAPI_HOST=0.0.0.0

# Expose the port (Railway will override this)
EXPOSE $PORT

# Health check (Railway will handle this, but good to have)
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
  CMD curl -f http://localhost:${PORT:-8000}/health || exit 1

# Run the FastAPI server
CMD ["python", "fastapi_server.py"]
</file>

<file path="Dockerfile.render">
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first for better caching
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy application files
COPY fastapi_server.py .

# Render uses PORT environment variable (defaults to 10000)
ENV FASTAPI_HOST=0.0.0.0
ENV FASTAPI_PORT=${PORT:-10000}

# Expose the port
EXPOSE $PORT

# Health check for Render
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
  CMD curl -f http://localhost:${PORT:-10000}/health || exit 1

# Run the FastAPI server
CMD ["python", "fastapi_server.py"]
</file>

<file path="eslint.config.mjs">
import { dirname } from "path";
import { fileURLToPath } from "url";
import { FlatCompat } from "@eslint/eslintrc";

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

const compat = new FlatCompat({
  baseDirectory: __dirname,
});

const eslintConfig = [
  ...compat.extends("next/core-web-vitals", "next/typescript"),
  {
    ignores: [
      "node_modules/**",
      ".next/**",
      "out/**",
      "build/**",
      "next-env.d.ts",
    ],
  },
  {
    rules: {
      "@typescript-eslint/no-explicit-any": "warn",
      "@typescript-eslint/no-unused-vars": "warn",
    },
  },
];

export default eslintConfig;
</file>

<file path="FRONTEND_SETUP.md">
# Frontend Next.js Setup Guide - Google Gemini & Qdrant Cloud

## Overview

This guide explains how to set up the Next.js frontend with Google Gemini 1.5 Flash and Qdrant Cloud integration for the RAG A2A Superbot.

## Features

### ‚úÖ Google Gemini 1.5 Flash Integration
- **Model**: `gemini-1.5-flash` (configurable)
- **Temperature**: 0.7 (configurable)
- **Max Tokens**: 2048 (configurable)
- **Fallback**: Automatic fallback to Ollama if Gemini fails
- **Error Handling**: Comprehensive error handling and logging

### ‚úÖ Qdrant Cloud Database Support
- **Cloud Support**: Full Qdrant Cloud integration with API key authentication
- **Local Support**: Maintains compatibility with local Qdrant instances
- **Auto-Detection**: Automatically detects and uses cloud vs local based on configuration
- **Security**: Secure API key management through environment variables

## Quick Start

### 1. Install Dependencies

```bash
cd frontend
npm install
```

### 2. Configure Environment

```bash
# Copy the configuration template
cp config.env .env.local

# Edit .env.local with your API keys
nano .env.local
```

### 3. Test Integration

```bash
# Run the integration test
npm test
```

### 4. Start Development Server

```bash
# Start the Next.js development server
npm run dev
```

Visit `http://localhost:3000` to see your RAG application.

## Configuration

### Environment Variables

Create a `.env.local` file in the frontend directory with the following variables:

```bash
# Google Gemini Configuration
NEXT_PUBLIC_GOOGLE_API_KEY=your_gemini_api_key_here
NEXT_PUBLIC_GEMINI_MODEL=gemini-1.5-flash
NEXT_PUBLIC_GEMINI_TEMPERATURE=0.7
NEXT_PUBLIC_GEMINI_MAX_TOKENS=2048

# Qdrant Cloud Configuration
NEXT_PUBLIC_QDRANT_CLOUD_URL=https://your-cluster-id.eu-central.aws.cloud.qdrant.io
NEXT_PUBLIC_QDRANT_CLOUD_API_KEY=your_qdrant_cloud_api_key_here

# Vector Store Configuration
NEXT_PUBLIC_VECTOR_STORE=qdrant
NEXT_PUBLIC_COLLECTION_NAME=rag_a2a_collection

# Fallback Configuration (for local development)
NEXT_PUBLIC_QDRANT_HOST=localhost
NEXT_PUBLIC_QDRANT_PORT=6333
NEXT_PUBLIC_OLLAMA_HOST=http://localhost:11434

# Chroma Configuration (if using Chroma)
NEXT_PUBLIC_CHROMA_PATH=./chroma_db
```

### Getting API Keys

#### Google Gemini API Key
1. Go to [Google AI Studio](https://makersuite.google.com/app/apikey)
2. Sign in with your Google account
3. Click "Create API Key"
4. Copy the generated API key
5. Add it to your `.env.local` file as `NEXT_PUBLIC_GOOGLE_API_KEY`

#### Qdrant Cloud API Key
1. Go to [Qdrant Cloud](https://cloud.qdrant.io/)
2. Sign up or sign in to your account
3. Create a new cluster
4. Go to your cluster dashboard
5. Copy the cluster URL and API key
6. Add them to your `.env.local` file as `NEXT_PUBLIC_QDRANT_CLOUD_URL` and `NEXT_PUBLIC_QDRANT_CLOUD_API_KEY`

## Architecture

### Frontend Structure

```
frontend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chat/
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ route.ts          # API route for chat
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page.tsx                  # Main page
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ layout.tsx                # App layout
‚îÇ   ‚îî‚îÄ‚îÄ lib/
‚îÇ       ‚îú‚îÄ‚îÄ agents.ts                 # AI agents with Gemini integration
‚îÇ       ‚îú‚îÄ‚îÄ vectorstore.ts            # Vector store with Qdrant Cloud
‚îÇ       ‚îî‚îÄ‚îÄ pipelines.ts              # RAG pipelines
‚îú‚îÄ‚îÄ config.env                        # Configuration template
‚îú‚îÄ‚îÄ test-gemini-qdrant.mjs           # Integration test script
‚îî‚îÄ‚îÄ package.json                      # Dependencies
```

### Vector Store Selection Logic

```typescript
// Automatic detection based on environment variables
if (QDRANT_CLOUD_URL && QDRANT_CLOUD_API_KEY) {
  // Use Qdrant Cloud
  client = new QdrantClient({
    url: QDRANT_CLOUD_URL,
    apiKey: QDRANT_CLOUD_API_KEY
  });
} else {
  // Use local Qdrant
  client = new QdrantClient({
    host: QDRANT_HOST,
    port: QDRANT_PORT
  });
}
```

### LLM Selection Logic

```typescript
// Try Google Gemini first
if (genAI) {
  try {
    const model = genAI.getGenerativeModel({ 
      model: GEMINI_MODEL,
      generationConfig: {
        temperature: GEMINI_TEMPERATURE,
        maxOutputTokens: GEMINI_MAX_TOKENS,
      }
    });
    return await model.generateContent(prompt);
  } catch (error) {
    // Fallback to Ollama
    return await ollamaGenerate(prompt);
  }
}
```

## API Routes

### Chat API (`/api/chat`)

**POST** `/api/chat`

**Request Body:**
```json
{
  "message": "Your question here",
  "pipelineMode": "meta"
}
```

**Response:**
```json
{
  "success": true,
  "answer": "Generated answer",
  "thinkingSteps": [...],
  "pipelineInfo": "Pipeline used",
  "sources": [...],
  "timestamp": "2024-01-01T00:00:00.000Z"
}
```

**Pipeline Modes:**
- `phase1`: Basic A2A pipeline
- `phase2`: Smart A2A pipeline with critique
- `phase3`: Self-refinement pipeline
- `auto`: Automatic pipeline selection
- `meta`: Intelligent pipeline selection (default)

## Testing

### Run Integration Tests

```bash
# Test all components
npm test

# Test specific components
node test-gemini-qdrant.mjs
```

### Test Results

The test script will verify:
- ‚úÖ Google Gemini API connectivity
- ‚úÖ Qdrant Cloud/Local connectivity
- ‚úÖ Ollama fallback functionality
- ‚úÖ Embedding generation
- ‚úÖ Environment variable configuration
- ‚úÖ End-to-end integration

## Development

### Available Scripts

```bash
npm run dev          # Start development server
npm run build        # Build for production
npm run start        # Start production server
npm run lint         # Run ESLint
npm test             # Run integration tests
```

### Development Workflow

1. **Start Ollama** (for local fallback):
   ```bash
   ollama serve
   ollama pull nomic-embed-text
   ollama pull gemma3:1b
   ```

2. **Configure environment**:
   ```bash
   cp config.env .env.local
   # Edit .env.local with your API keys
   ```

3. **Test integration**:
   ```bash
   npm test
   ```

4. **Start development**:
   ```bash
   npm run dev
   ```

5. **Visit application**:
   Open `http://localhost:3000`

## Production Deployment

### Environment Variables

For production, ensure all environment variables are set:

```bash
# Required
NEXT_PUBLIC_GOOGLE_API_KEY=your_production_key
NEXT_PUBLIC_VECTOR_STORE=qdrant
NEXT_PUBLIC_COLLECTION_NAME=your_collection

# Optional (for cloud features)
NEXT_PUBLIC_QDRANT_CLOUD_URL=your_cloud_url
NEXT_PUBLIC_QDRANT_CLOUD_API_KEY=your_cloud_key
```

### Build and Deploy

```bash
# Build the application
npm run build

# Start production server
npm run start
```

### Deployment Platforms

#### Vercel
1. Connect your GitHub repository
2. Set environment variables in Vercel dashboard
3. Deploy automatically

#### Netlify
1. Connect your GitHub repository
2. Set environment variables in Netlify dashboard
3. Deploy automatically

#### Docker
```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production
COPY . .
RUN npm run build
EXPOSE 3000
CMD ["npm", "start"]
```

## Troubleshooting

### Common Issues

#### 1. Environment Variables Not Loading
```
Error: NEXT_PUBLIC_GOOGLE_API_KEY not found
```
**Solution**: Ensure variables start with `NEXT_PUBLIC_` and are in `.env.local`

#### 2. CORS Issues
```
Error: CORS policy blocked
```
**Solution**: Check that Ollama is running and accessible

#### 3. Qdrant Connection Issues
```
Error: Qdrant connection failed
```
**Solution**: Verify your Qdrant Cloud credentials or local Qdrant instance

#### 4. Gemini API Issues
```
Error: Google Gemini API failed
```
**Solution**: Check your API key and quota limits

### Debug Mode

Enable debug logging by setting:
```bash
NODE_ENV=development
```

### Logs

Check the browser console and terminal for detailed error messages.

## Performance

### Optimization Tips

1. **Caching**: Implement response caching for repeated queries
2. **Streaming**: Use streaming for long responses
3. **CDN**: Use a CDN for static assets
4. **Database**: Optimize vector database queries

### Monitoring

Monitor key metrics:
- API response times
- Error rates
- Token usage (Gemini)
- Vector search performance

## Security

### API Key Management
- Never commit API keys to version control
- Use environment variables for all sensitive data
- Rotate API keys regularly
- Monitor API usage

### Network Security
- Use HTTPS in production
- Implement rate limiting
- Validate all inputs
- Monitor for unusual activity

## Support

### Getting Help

1. Check the troubleshooting section
2. Review the test results
3. Check browser console for errors
4. Verify environment configuration

### Common Solutions

- **API Key Issues**: Verify keys are correct and have proper permissions
- **Connection Issues**: Check network connectivity and service availability
- **Configuration Issues**: Ensure all required environment variables are set

---

This setup provides a robust, scalable frontend for your RAG A2A Superbot with enterprise-grade LLM and vector database capabilities.
</file>

<file path="INTEGRATION_SUMMARY.md">
# LiteLLM Integration Summary

## ‚úÖ Completed Integration Tasks

### 1. FastAPI LiteLLM Proxy Server ‚úÖ
**Files Created:**
- `fastapi_server.py` - Main FastAPI server with OpenAI-compatible endpoints
- `Dockerfile.fastapi` - Docker configuration for the proxy
- `docker-compose.yml` - Docker Compose orchestration
- `requirements.txt` - Python dependencies (already existed)

**Features:**
- OpenAI-compatible `/v1/chat/completions` endpoint
- Health check endpoint at `/health`
- Models list endpoint at `/v1/models`
- Request transformation to 1minAI format
- Response transformation back to OpenAI format
- Comprehensive error handling and logging
- Default model: `gemini-2.0-flash-lite`

### 2. LiteLLM Client Library ‚úÖ
**File Created:**
- `src/lib/litellm-client.ts` - TypeScript client for the proxy

**Functions:**
- `generateChatCompletion()` - Multi-message chat completions
- `generateTextCompletion()` - Simple text generation
- `checkHealth()` - Health check utility
- `getAvailableModels()` - List available models
- `testConnection()` - Connection testing utility
- `getLiteLLMConfig()` - Get current configuration

### 3. Agent Integration ‚úÖ
**File Modified:**
- `src/lib/agents.ts` - Updated AnswerAgent and RefineAgent

**Changes:**
- Added LiteLLM as primary LLM provider
- Maintained Google Gemini as fallback
- Preserved Ollama as secondary fallback
- Added comprehensive logging for debugging
- No changes to agent interfaces (backward compatible)

### 4. Environment Configuration ‚úÖ
**Files Updated:**
- `.env.local` - Added LiteLLM and 1minAI configuration
- `config.env` - Updated template with new variables

**New Environment Variables:**
```bash
NEXT_PUBLIC_USE_LITELLM=true
NEXT_PUBLIC_LITELLM_API_URL=http://localhost:8000
NEXT_PUBLIC_LITELLM_MODEL=gemini-2.0-flash-lite
NEXT_PUBLIC_LITELLM_TEMPERATURE=0.7
NEXT_PUBLIC_LITELLM_MAX_TOKENS=2048
ONEMINAI_API_KEY=your_1minai_api_key_here
```

### 5. Deployment Scripts ‚úÖ
**Files Created:**
- `start-services.ps1` - Start all services with health checks
- `stop-services.ps1` - Stop all services
- `logs.ps1` - View service logs

**Features:**
- Automatic Docker status checking
- Health check verification
- Helpful status messages
- Error handling
- Usage instructions

### 6. Documentation ‚úÖ
**Files Created/Updated:**
- `LITELLM_INTEGRATION.md` - Comprehensive integration guide
- `QUICKSTART.md` - 5-minute quick start guide  
- `INTEGRATION_SUMMARY.md` - This file
- `README.md` - Updated with LiteLLM information

## üéØ What Was NOT Changed

### Unchanged Components ‚úÖ
1. **Vector Store** (`src/lib/vectorstore.ts`)
   - Still uses Google Gemini for embeddings
   - Still uses same Qdrant database
   - No changes to embedding dimensions (3072)
   - No migration needed for existing collections

2. **RAG Pipeline** (`src/lib/pipelines.ts`)
   - All pipeline logic unchanged
   - Document retrieval unchanged
   - Agent orchestration unchanged

3. **Frontend UI** (`src/app/page.tsx`)
   - No changes to user interface
   - No changes to chat functionality
   - No changes to thinking steps display

4. **API Routes**
   - `/api/chat` - Unchanged
   - `/api/documents` - Unchanged
   - `/api/sample-documents` - Unchanged
   - `/api/status` - Unchanged

5. **Qdrant Database**
   - Same collections
   - Same vectors
   - Same data
   - No migration required

## üîÑ Request Flow

### Text Generation (LLM Calls)
```
User Query
    ‚Üì
Agent (AnswerAgent/RefineAgent)
    ‚Üì
litellm-client.ts (generateTextCompletion)
    ‚Üì
HTTP Request to localhost:8000
    ‚Üì
fastapi_server.py (FastAPI proxy)
    ‚Üì
Transform to 1minAI format
    ‚Üì
POST to api.1min.ai/api/features
    ‚Üì
1minAI processes with gemini-2.0-flash-lite
    ‚Üì
Response from 1minAI
    ‚Üì
Transform to OpenAI format
    ‚Üì
Return to TypeScript client
    ‚Üì
Agent processes response
    ‚Üì
Display to user
```

### Vector Embeddings (Unchanged)
```
Document/Query Text
    ‚Üì
vectorstore.ts (getEmbedding)
    ‚Üì
Direct call to Google Gemini API
    ‚Üì
gemini-embedding-001 (3072 dims)
    ‚Üì
Store/Search in Qdrant Cloud
```

## üß™ Testing Instructions

### Step 1: Environment Setup
```bash
# 1. Ensure you have your API keys
#    - 1minAI API key from https://1min.ai
#    - Google Gemini API key (for embeddings)
#    - Qdrant Cloud credentials (already configured)

# 2. Update .env.local with your keys
notepad .env.local
```

### Step 2: Start Services
```powershell
# Start LiteLLM proxy
.\start-services.ps1

# Wait for "‚úÖ LiteLLM proxy is healthy!" message
```

### Step 3: Test LiteLLM Proxy
```bash
# Test 1: Health check
curl http://localhost:8000/health
# Expected: {"status":"healthy",...}

# Test 2: List models
curl http://localhost:8000/v1/models
# Expected: List of available models including gemini-2.0-flash-lite

# Test 3: Chat completion
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"gemini-2.0-flash-lite","messages":[{"role":"user","content":"Hello!"}]}'
# Expected: Response with assistant message
```

### Step 4: Start Next.js
```bash
# In a new terminal
npm run dev

# Wait for "Ready" message
# Open http://localhost:3000
```

### Step 5: Test RAG Application
1. **Load Sample Documents**
   - Click "Load Sample Docs" button
   - Wait for confirmation

2. **Test Query**
   - Enter: "What is machine learning?"
   - Observe console logs for LiteLLM activity
   - Check for "üöÄ Using LiteLLM proxy..." messages

3. **Verify Response Quality**
   - Response should be coherent
   - Should reference retrieved documents
   - Should show thinking steps

4. **Test Different Models**
   - Edit `.env.local`
   - Change `NEXT_PUBLIC_LITELLM_MODEL=gpt-4o-mini`
   - Restart Next.js
   - Test again

### Step 6: Monitor Logs
```powershell
# View real-time logs
.\logs.ps1

# Look for:
# - "Making request to: https://api.1min.ai/api/features"
# - "1minAI API response status: 200"
# - "1minAI API request successful"
```

### Step 7: Test Fallback Mechanism
```bash
# Stop LiteLLM proxy
docker-compose down

# Try a query in the app
# Should see fallback to Google Gemini:
# "‚ö†Ô∏è LiteLLM failed, falling back to Google Gemini"
```

## üêõ Troubleshooting Checklist

### Issue: LiteLLM proxy not starting
- [ ] Docker Desktop is running
- [ ] Port 8000 is not in use
- [ ] ONEMINAI_API_KEY is set in .env.local
- [ ] Run: `docker-compose logs fastapi-litellm`

### Issue: Connection refused errors
- [ ] LiteLLM proxy is running: `curl http://localhost:8000/health`
- [ ] Check `.env.local` has `NEXT_PUBLIC_USE_LITELLM=true`
- [ ] Check `NEXT_PUBLIC_LITELLM_API_URL=http://localhost:8000`

### Issue: Always falling back to Gemini
- [ ] LiteLLM proxy is healthy
- [ ] ONEMINAI_API_KEY is valid
- [ ] Check browser console for error messages
- [ ] Check proxy logs: `.\logs.ps1`

### Issue: 1minAI API errors
- [ ] API key is correct in .env.local
- [ ] Check 1minAI service status
- [ ] Try different model: `NEXT_PUBLIC_LITELLM_MODEL=gpt-4o-mini`

### Issue: Embeddings not working
- [ ] Google Gemini API key is set
- [ ] Qdrant Cloud credentials are correct
- [ ] Collection name matches your Qdrant collection
- [ ] Note: Embeddings don't use LiteLLM!

## üìä Verification Checklist

### Before Testing
- [ ] Docker Desktop installed and running
- [ ] Node.js 18+ installed
- [ ] 1minAI API key obtained
- [ ] Google Gemini API key configured
- [ ] Qdrant Cloud accessible
- [ ] `.env.local` configured with all keys
- [ ] Dependencies installed: `npm install`

### Service Status
- [ ] LiteLLM proxy running on port 8000
- [ ] Health check passes: `curl http://localhost:8000/health`
- [ ] Models endpoint working: `curl http://localhost:8000/v1/models`
- [ ] Next.js running on port 3000
- [ ] Can access http://localhost:3000

### Functionality
- [ ] Can load sample documents
- [ ] Can submit queries
- [ ] Receives LLM-generated responses
- [ ] Console shows LiteLLM activity
- [ ] Thinking steps display correctly
- [ ] Retrieved documents shown
- [ ] No errors in browser console
- [ ] No errors in proxy logs

### Performance
- [ ] Response time < 5 seconds
- [ ] No timeout errors
- [ ] Smooth UI interaction
- [ ] Logs show successful API calls

## üéâ Success Criteria

Integration is successful when:

1. ‚úÖ LiteLLM proxy starts without errors
2. ‚úÖ Health check returns `healthy` status
3. ‚úÖ Can list available models
4. ‚úÖ Test curl command returns valid response
5. ‚úÖ Next.js app connects to proxy
6. ‚úÖ Can ask questions and get responses
7. ‚úÖ Browser console shows "üöÄ Using LiteLLM proxy..." logs
8. ‚úÖ Proxy logs show successful 1minAI API calls
9. ‚úÖ Response quality is good
10. ‚úÖ Fallback to Gemini works when proxy is down

## üöÄ Next Steps After Integration

1. **Get Your 1minAI API Key**
   - Sign up at https://1min.ai
   - Add to `.env.local`

2. **Test Different Models**
   - Try `gemini-2.0-flash` for better quality
   - Try `gpt-4o-mini` for OpenAI experience
   - Try `claude-3-haiku` for Anthropic

3. **Optimize Configuration**
   - Adjust temperature for creativity
   - Increase max_tokens for longer responses
   - Fine-tune based on your use case

4. **Monitor Usage**
   - Watch logs for patterns
   - Track response times
   - Monitor error rates

5. **Scale Up**
   - Deploy to production
   - Add rate limiting
   - Implement caching
   - Set up monitoring

## üìù Files Modified/Created

### New Files (12)
1. `fastapi_server.py`
2. `Dockerfile.fastapi`
3. `docker-compose.yml`
4. `src/lib/litellm-client.ts`
5. `start-services.ps1`
6. `stop-services.ps1`
7. `logs.ps1`
8. `LITELLM_INTEGRATION.md`
9. `QUICKSTART.md`
10. `INTEGRATION_SUMMARY.md`
11. `.env.local` (updated)
12. `config.env` (updated)

### Modified Files (2)
1. `src/lib/agents.ts` - Added LiteLLM integration
2. `README.md` - Added LiteLLM information

### Unchanged Files
- All other source files
- All configuration files (except .env.local and config.env)
- All UI components
- All API routes
- Vector store implementation
- RAG pipeline logic

## üéì Key Learnings

1. **Hybrid Approach Works Well**
   - LiteLLM for text generation (free, multi-model)
   - Google Gemini for embeddings (consistent vectors)
   - Best of both worlds

2. **Fallback Strategy is Critical**
   - Multiple fallback layers ensure reliability
   - Graceful degradation enhances user experience
   - Error messages help debugging

3. **Docker Simplifies Deployment**
   - One command to start proxy
   - Health checks ensure readiness
   - Easy to manage lifecycle

4. **OpenAI Compatibility is Powerful**
   - Standard interface works everywhere
   - Easy to swap backends
   - Future-proof architecture

## üéØ Summary

‚úÖ **Successfully Integrated:**
- FastAPI/LiteLLM proxy server
- 1minAI API for free text generation
- Multi-model support (Gemini, GPT, Claude)
- Intelligent fallback mechanism
- Deployment scripts and documentation

‚úÖ **Maintained:**
- Same Qdrant database
- Same Google Gemini embeddings
- Same RAG quality
- Same user interface
- Backward compatibility

‚úÖ **Benefits:**
- Free API usage
- Multiple model options
- Easy deployment
- Professional architecture
- Production-ready

üöÄ **Ready to use RAG Superbot with LiteLLM and 1minAI!**
</file>

<file path="list-collections.mjs">
#!/usr/bin/env node

import { QdrantClient } from '@qdrant/js-client-rest';
import dotenv from 'dotenv';

dotenv.config({ path: './.env.local' });

const QDRANT_CLOUD_URL = process.env.NEXT_PUBLIC_QDRANT_CLOUD_URL;
const QDRANT_CLOUD_API_KEY = process.env.NEXT_PUBLIC_QDRANT_CLOUD_API_KEY;
const COLLECTION_NAME = process.env.NEXT_PUBLIC_COLLECTION_NAME;

async function main() {
  console.log("üìã Listing all collections...\n");
  console.log(`üéØ Looking for: '${COLLECTION_NAME}'\n`);

  const client = new QdrantClient({
    url: QDRANT_CLOUD_URL,
    apiKey: QDRANT_CLOUD_API_KEY
  });

  try {
    const collections = await client.getCollections();

    console.log(`üìä Found ${collections.collections.length} collections:\n`);

    collections.collections.forEach((col, index) => {
      const isTarget = col.name === COLLECTION_NAME;
      const marker = isTarget ? 'üéØ' : 'üìÑ';
      console.log(`${marker} ${index + 1}. "${col.name}"`);

      if (col.name.includes('psychiatry') || col.name.includes('therapy')) {
        console.log(`   üëÜ This looks like a psychiatry/therapy collection!`);
      }

      // Check for similar names
      if (col.name.includes(COLLECTION_NAME.replace(/\s/g, '')) ||
        COLLECTION_NAME.includes(col.name.replace(/\s/g, ''))) {
        console.log(`   ‚ö†Ô∏è  Similar to target collection name`);
      }
    });

    // Try to get info for each psychiatry-related collection
    console.log("\nüîç Checking psychiatry/therapy collections for content:\n");

    for (const col of collections.collections) {
      if (col.name.includes('psychiatry') || col.name.includes('therapy')) {
        try {
          const info = await client.getCollection(col.name);
          console.log(`üìä Collection: "${col.name}"`);
          console.log(`   - Points: ${info.points_count}`);
          console.log(`   - Dimensions: ${info.config?.params?.vectors?.size}`);
          console.log(`   - Distance: ${info.config?.params?.vectors?.distance}\n`);
        } catch (error) {
          console.log(`‚ùå Error accessing "${col.name}": ${error.message}\n`);
        }
      }
    }

  } catch (error) {
    console.error("‚ùå Error listing collections:", error.message);
  }
}

main().catch(console.error);
</file>

<file path="LITELLM_INTEGRATION.md">
# LiteLLM Integration Guide - RAG Superbot

## Overview

This document describes the integration of **LiteLLM proxy with 1minAI** into the RAG Superbot application. The integration replaces direct Google Gemini API calls with a FastAPI/LiteLLM proxy server that routes requests to 1minAI's free API service.

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Next.js App   ‚îÇ
‚îÇ  (RAG Superbot) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ HTTP Requests
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FastAPI Server     ‚îÇ
‚îÇ  (LiteLLM Proxy)    ‚îÇ
‚îÇ  localhost:8000     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îÇ 1minAI API
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   1minAI Service    ‚îÇ
‚îÇ  api.1min.ai        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Components

1. **FastAPI LiteLLM Proxy** (`fastapi_server.py`)
   - OpenAI-compatible API endpoints
   - Transforms requests to 1minAI format
   - Handles model mapping and error handling
   - Runs on `http://localhost:8000`

2. **LiteLLM Client** (`src/lib/litellm-client.ts`)
   - TypeScript client library
   - Provides clean interface for agents
   - Handles errors and retries
   - Logs all operations

3. **Agent Integration** (`src/lib/agents.ts`)
   - Modified to use LiteLLM as primary
   - Falls back to Google Gemini if LiteLLM fails
   - Maintains existing interface

4. **Vector Store** (`src/lib/vectorstore.ts`)
   - **UNCHANGED** - Still uses Google Gemini embeddings
   - **UNCHANGED** - Still uses same Qdrant database
   - Embeddings remain at 3072 dimensions

## Supported Models

The integration supports the following models through 1minAI:

### Gemini Models (Recommended)
- `gemini-2.0-flash-lite` ‚≠ê **DEFAULT - Fast and efficient**
- `gemini-2.0-flash` - More capable version
- `gemini-1.5-flash` - Previous generation
- `gemini-1.5-pro` - Most capable

### OpenAI Models
- `gpt-4o-mini` - Fast and cost-effective
- `gpt-4o` - Most capable GPT-4

### Anthropic Models
- `claude-3-5-sonnet` - Balanced performance
- `claude-3-haiku` - Fast responses

## Setup Instructions

### 1. Prerequisites

- Docker Desktop installed and running
- Node.js 18+ installed
- 1minAI API key (get from https://1min.ai)
- Existing Qdrant Cloud setup (no changes needed)

### 2. Environment Configuration

Edit `.env.local` with your API keys:

```bash
# LiteLLM / 1minAI Configuration (Primary)
NEXT_PUBLIC_USE_LITELLM=true
NEXT_PUBLIC_LITELLM_API_URL=http://localhost:8000
NEXT_PUBLIC_LITELLM_MODEL=gemini-2.0-flash-lite
NEXT_PUBLIC_LITELLM_TEMPERATURE=0.7
NEXT_PUBLIC_LITELLM_MAX_TOKENS=2048
ONEMINAI_API_KEY=your_1minai_api_key_here

# Google Gemini Configuration (Fallback/Embeddings only)
NEXT_PUBLIC_GOOGLE_API_KEY=your_gemini_api_key_here
NEXT_PUBLIC_EMBEDDING_MODEL=gemini-embedding-001
NEXT_PUBLIC_EMBEDDING_DIM=3072

# Qdrant Configuration (Unchanged)
NEXT_PUBLIC_QDRANT_CLOUD_URL=https://your-cluster-url.cloud.qdrant.io:6333
NEXT_PUBLIC_QDRANT_CLOUD_API_KEY=your_qdrant_api_key
NEXT_PUBLIC_COLLECTION_NAME=your_collection_name
```

### 3. Start Services

#### Option A: Using PowerShell Scripts (Recommended)

```powershell
# Start all services
.\start-services.ps1

# In another terminal, start Next.js
npm run dev

# View logs
.\logs.ps1

# Stop services
.\stop-services.ps1
```

#### Option B: Manual Start

```bash
# Start LiteLLM proxy
docker-compose up -d fastapi-litellm

# Check health
curl http://localhost:8000/health

# Start Next.js
npm run dev
```

### 4. Verify Integration

1. **Check LiteLLM Proxy**
   ```bash
   curl http://localhost:8000/health
   # Should return: {"status":"healthy",...}
   ```

2. **List Available Models**
   ```bash
   curl http://localhost:8000/v1/models
   ```

3. **Test Chat Completion**
   ```bash
   curl -X POST http://localhost:8000/v1/chat/completions \
     -H "Content-Type: application/json" \
     -d '{
       "model": "gemini-2.0-flash-lite",
       "messages": [{"role": "user", "content": "Hello!"}]
     }'
   ```

4. **Test via Next.js App**
   - Navigate to `http://localhost:3000`
   - Ask a question in the chat
   - Check browser console for LiteLLM logs

## How It Works

### Request Flow

1. **User Query** ‚Üí RAG Superbot frontend
2. **Agent Processing** ‚Üí Agents (QueryAgent, AnswerAgent, RefineAgent)
3. **LLM Call** ‚Üí `generateTextCompletion()` from litellm-client
4. **HTTP Request** ‚Üí FastAPI proxy at localhost:8000
5. **Transform** ‚Üí FastAPI converts to 1minAI format
6. **API Call** ‚Üí 1minAI service (api.1min.ai)
7. **Response** ‚Üí Transform back to OpenAI format
8. **Return** ‚Üí Agent receives response
9. **RAG Pipeline** ‚Üí Continue with retrieved documents
10. **Final Answer** ‚Üí Display to user

### Fallback Strategy

```typescript
try {
  // Try LiteLLM proxy (1minAI)
  return await generateTextCompletion(prompt, {...});
} catch (error) {
  try {
    // Fallback to Google Gemini
    return await geminiAPI.generate(prompt);
  } catch (error) {
    try {
      // Fallback to Ollama (local only)
      return await ollamaAPI.generate(prompt);
    } catch (error) {
      // Return error message
      return "Unable to generate response";
    }
  }
}
```

## Embedding Strategy

**IMPORTANT:** Embeddings still use Google Gemini directly!

- Vector embeddings: Google Gemini `embedding-001` (3072 dims)
- RAG retrieval: Same Qdrant database
- Text generation: LiteLLM proxy (1minAI)

This hybrid approach ensures:
- Fast, free text generation via 1minAI
- Consistent embeddings for RAG retrieval
- No changes to existing Qdrant collections

## Configuration Options

### Switching Between Models

Edit `.env.local`:

```bash
# Use Gemini 2.0 Flash Lite (fastest, recommended)
NEXT_PUBLIC_LITELLM_MODEL=gemini-2.0-flash-lite

# Use GPT-4o-mini (OpenAI via 1minAI)
NEXT_PUBLIC_LITELLM_MODEL=gpt-4o-mini

# Use Claude Sonnet (Anthropic via 1minAI)
NEXT_PUBLIC_LITELLM_MODEL=claude-3-5-sonnet
```

### Disabling LiteLLM (Use Gemini Directly)

```bash
NEXT_PUBLIC_USE_LITELLM=false
```

### Adjusting Temperature

```bash
# More creative (0.8-1.0)
NEXT_PUBLIC_LITELLM_TEMPERATURE=0.9

# More deterministic (0.0-0.3)
NEXT_PUBLIC_LITELLM_TEMPERATURE=0.2
```

### Increasing Max Tokens

```bash
# Longer responses
NEXT_PUBLIC_LITELLM_MAX_TOKENS=4096
```

## Troubleshooting

### LiteLLM Proxy Not Starting

```bash
# Check Docker status
docker ps

# View logs
docker-compose logs fastapi-litellm

# Rebuild container
docker-compose build fastapi-litellm
docker-compose up -d fastapi-litellm
```

### Connection Refused Error

**Problem:** `LiteLLM proxy server is not running`

**Solution:**
```bash
# Ensure Docker service is running
.\start-services.ps1

# Check health endpoint
curl http://localhost:8000/health
```

### 1minAI API Errors

**Problem:** `1minAI API is currently unavailable`

**Solutions:**
1. Check your ONEMINAI_API_KEY in `.env.local`
2. Verify API key at https://1min.ai
3. Check 1minAI service status
4. Review logs: `.\logs.ps1`

### Fallback to Gemini

**Problem:** Always falling back to Gemini

**Check:**
1. Is `NEXT_PUBLIC_USE_LITELLM=true`?
2. Is LiteLLM proxy running? `curl http://localhost:8000/health`
3. Is ONEMINAI_API_KEY set correctly?
4. Check browser console for error messages

## Performance Comparison

| Metric | Google Gemini Direct | LiteLLM + 1minAI | Improvement |
|--------|---------------------|------------------|-------------|
| Cost | Paid API | **Free** | ‚úÖ 100% savings |
| Latency | ~1-2s | ~1-3s | Similar |
| Rate Limits | High | Moderate | OK for dev |
| Models | Gemini only | Multiple providers | ‚úÖ More choice |
| Setup | Simple | Docker required | One-time setup |

## Best Practices

1. **Development**: Use LiteLLM + 1minAI (free, fast)
2. **Production**: Consider paid APIs for higher rate limits
3. **Embeddings**: Always use Google Gemini (consistent vectors)
4. **Monitoring**: Check logs regularly (`.\logs.ps1`)
5. **Fallback**: Keep Google Gemini key as backup
6. **Testing**: Test with different models to find best fit

## API Endpoints

### LiteLLM Proxy Endpoints

- `GET /` - Service information
- `GET /health` - Health check
- `GET /v1/models` - List available models
- `POST /v1/chat/completions` - Chat completion
- `POST /chat/completions` - Chat completion (alias)

### Example Request

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-2.0-flash-lite",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is RAG?"}
    ],
    "temperature": 0.7,
    "max_tokens": 500
  }'
```

### Example Response

```json
{
  "id": "chatcmpl-1234567890",
  "object": "chat.completion",
  "created": 1234567890,
  "model": "gemini-2.0-flash-lite",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "RAG stands for Retrieval-Augmented Generation..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 25,
    "completion_tokens": 150,
    "total_tokens": 175
  }
}
```

## Docker Configuration

### docker-compose.yml Overview

```yaml
services:
  fastapi-litellm:
    build:
      context: .
      dockerfile: Dockerfile.fastapi
    ports:
      - "8000:8000"
    environment:
      - ONEMINAI_API_KEY=${ONEMINAI_API_KEY}
      - DEFAULT_MODEL=gemini-2.0-flash-lite
    restart: unless-stopped
```

### Dockerfile Overview

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY fastapi_server.py .
CMD ["python", "fastapi_server.py"]
```

## Next Steps

1. **Get 1minAI API Key**: Sign up at https://1min.ai
2. **Update Environment**: Add your ONEMINAI_API_KEY to `.env.local`
3. **Start Services**: Run `.\start-services.ps1`
4. **Test Integration**: Open http://localhost:3000 and chat
5. **Monitor Logs**: Use `.\logs.ps1` to watch requests
6. **Optimize**: Adjust temperature and tokens for your use case

## Support

- **LiteLLM Docs**: https://docs.litellm.ai
- **1minAI**: https://1min.ai
- **FastAPI**: https://fastapi.tiangolo.com
- **Qdrant**: https://qdrant.tech/documentation

## Summary

‚úÖ **What Changed:**
- Added FastAPI/LiteLLM proxy server
- Modified agents to use LiteLLM as primary
- Added fallback to Google Gemini
- Created deployment scripts
- Updated environment configuration

‚úÖ **What Stayed the Same:**
- Qdrant vector database (no changes)
- Google Gemini embeddings (no changes)
- RAG pipeline logic (no changes)
- Frontend UI (no changes)
- Collection data (no migration needed)

‚úÖ **Benefits:**
- Free API usage via 1minAI
- Multiple model providers
- Easy model switching
- Maintained compatibility
- Graceful fallbacks

üöÄ **You're ready to use RAG Superbot with LiteLLM and 1minAI!**
</file>

<file path="next.config.ts">
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  // Optimize for Vercel deployment
  serverExternalPackages: ['chromadb', '@qdrant/js-client-rest'],
  
  // Environment variables configuration
  env: {
    CUSTOM_KEY: process.env.CUSTOM_KEY,
  },
  
  // Headers for security and CORS
  async headers() {
    return [
      {
        source: '/api/:path*',
        headers: [
          { key: 'Access-Control-Allow-Credentials', value: 'true' },
          { key: 'Access-Control-Allow-Origin', value: '*' },
          { key: 'Access-Control-Allow-Methods', value: 'GET,OPTIONS,PATCH,DELETE,POST,PUT' },
          { key: 'Access-Control-Allow-Headers', value: 'X-CSRF-Token, X-Requested-With, Accept, Accept-Version, Content-Length, Content-MD5, Content-Type, Date, X-Api-Version' },
        ],
      },
    ];
  },
  
  // Redirects for better SEO
  async redirects() {
    return [
      {
        source: '/home',
        destination: '/',
        permanent: true,
      },
    ];
  },
};

export default nextConfig;
</file>

<file path="package-cloudflare.json">
{
    "name": "psychiatry-therapy-superbot-cloudflare-api",
    "version": "1.0.0",
    "description": "Cloudflare Worker for Psychiatry Therapy SuperBot LiteLLM Proxy",
    "main": "src/worker.js",
    "scripts": {
        "dev": "wrangler dev",
        "deploy": "wrangler deploy",
        "deploy:staging": "wrangler deploy --env staging",
        "deploy:production": "wrangler deploy --env production",
        "tail": "wrangler tail",
        "test": "wrangler dev --local"
    },
    "devDependencies": {
        "wrangler": "^3.0.0"
    },
    "engines": {
        "node": ">=18.0.0"
    }
}
</file>

<file path="package.json">
{
  "name": "psychiatry-therapy-superbot-frontend",
  "version": "1.0.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint",
    "test": "node test-gemini-qdrant.mjs",
    "test-integration": "node test-gemini-qdrant.mjs",
    "test-embedding": "node test-google-embedding.mjs",
    "postinstall": "next telemetry disable"
  },
  "dependencies": {
    "@google/generative-ai": "^0.21.0",
    "@qdrant/js-client-rest": "^1.7.0",
    "chromadb": "^3.0.14",
    "lucide-react": "^0.543.0",
    "next": "15.5.2",
    "axios": "^1.6.0",
    "react": "19.1.0",
    "react-dom": "19.1.0"
  },
  "devDependencies": {
    "@eslint/eslintrc": "^3",
    "@tailwindcss/postcss": "^4",
    "@types/node": "^20",
    "@types/react": "^19",
    "@types/react-dom": "^19",
    "eslint": "^9",
    "eslint-config-next": "15.5.2",
    "tailwindcss": "^4",
    "typescript": "^5.6.0"
  },
  "engines": {
    "node": ">=18.0.0",
    "npm": ">=8.0.0"
  }
}
</file>

<file path="postcss.config.mjs">
const config = {
  plugins: ["@tailwindcss/postcss"],
};

export default config;
</file>

<file path="public/file.svg">
<svg fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><path d="M14.5 13.5V5.41a1 1 0 0 0-.3-.7L9.8.29A1 1 0 0 0 9.08 0H1.5v13.5A2.5 2.5 0 0 0 4 16h8a2.5 2.5 0 0 0 2.5-2.5m-1.5 0v-7H8v-5H3v12a1 1 0 0 0 1 1h8a1 1 0 0 0 1-1M9.5 5V2.12L12.38 5zM5.13 5h-.62v1.25h2.12V5zm-.62 3h7.12v1.25H4.5zm.62 3h-.62v1.25h7.12V11z" clip-rule="evenodd" fill="#666" fill-rule="evenodd"/></svg>
</file>

<file path="public/globe.svg">
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><g clip-path="url(#a)"><path fill-rule="evenodd" clip-rule="evenodd" d="M10.27 14.1a6.5 6.5 0 0 0 3.67-3.45q-1.24.21-2.7.34-.31 1.83-.97 3.1M8 16A8 8 0 1 0 8 0a8 8 0 0 0 0 16m.48-1.52a7 7 0 0 1-.96 0H7.5a4 4 0 0 1-.84-1.32q-.38-.89-.63-2.08a40 40 0 0 0 3.92 0q-.25 1.2-.63 2.08a4 4 0 0 1-.84 1.31zm2.94-4.76q1.66-.15 2.95-.43a7 7 0 0 0 0-2.58q-1.3-.27-2.95-.43a18 18 0 0 1 0 3.44m-1.27-3.54a17 17 0 0 1 0 3.64 39 39 0 0 1-4.3 0 17 17 0 0 1 0-3.64 39 39 0 0 1 4.3 0m1.1-1.17q1.45.13 2.69.34a6.5 6.5 0 0 0-3.67-3.44q.65 1.26.98 3.1M8.48 1.5l.01.02q.41.37.84 1.31.38.89.63 2.08a40 40 0 0 0-3.92 0q.25-1.2.63-2.08a4 4 0 0 1 .85-1.32 7 7 0 0 1 .96 0m-2.75.4a6.5 6.5 0 0 0-3.67 3.44 29 29 0 0 1 2.7-.34q.31-1.83.97-3.1M4.58 6.28q-1.66.16-2.95.43a7 7 0 0 0 0 2.58q1.3.27 2.95.43a18 18 0 0 1 0-3.44m.17 4.71q-1.45-.12-2.69-.34a6.5 6.5 0 0 0 3.67 3.44q-.65-1.27-.98-3.1" fill="#666"/></g><defs><clipPath id="a"><path fill="#fff" d="M0 0h16v16H0z"/></clipPath></defs></svg>
</file>

<file path="public/next.svg">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 394 80"><path fill="#000" d="M262 0h68.5v12.7h-27.2v66.6h-13.6V12.7H262V0ZM149 0v12.7H94v20.4h44.3v12.6H94v21h55v12.6H80.5V0h68.7zm34.3 0h-17.8l63.8 79.4h17.9l-32-39.7 32-39.6h-17.9l-23 28.6-23-28.6zm18.3 56.7-9-11-27.1 33.7h17.8l18.3-22.7z"/><path fill="#000" d="M81 79.3 17 0H0v79.3h13.6V17l50.2 62.3H81Zm252.6-.4c-1 0-1.8-.4-2.5-1s-1.1-1.6-1.1-2.6.3-1.8 1-2.5 1.6-1 2.6-1 1.8.3 2.5 1a3.4 3.4 0 0 1 .6 4.3 3.7 3.7 0 0 1-3 1.8zm23.2-33.5h6v23.3c0 2.1-.4 4-1.3 5.5a9.1 9.1 0 0 1-3.8 3.5c-1.6.8-3.5 1.3-5.7 1.3-2 0-3.7-.4-5.3-1s-2.8-1.8-3.7-3.2c-.9-1.3-1.4-3-1.4-5h6c.1.8.3 1.6.7 2.2s1 1.2 1.6 1.5c.7.4 1.5.5 2.4.5 1 0 1.8-.2 2.4-.6a4 4 0 0 0 1.6-1.8c.3-.8.5-1.8.5-3V45.5zm30.9 9.1a4.4 4.4 0 0 0-2-3.3 7.5 7.5 0 0 0-4.3-1.1c-1.3 0-2.4.2-3.3.5-.9.4-1.6 1-2 1.6a3.5 3.5 0 0 0-.3 4c.3.5.7.9 1.3 1.2l1.8 1 2 .5 3.2.8c1.3.3 2.5.7 3.7 1.2a13 13 0 0 1 3.2 1.8 8.1 8.1 0 0 1 3 6.5c0 2-.5 3.7-1.5 5.1a10 10 0 0 1-4.4 3.5c-1.8.8-4.1 1.2-6.8 1.2-2.6 0-4.9-.4-6.8-1.2-2-.8-3.4-2-4.5-3.5a10 10 0 0 1-1.7-5.6h6a5 5 0 0 0 3.5 4.6c1 .4 2.2.6 3.4.6 1.3 0 2.5-.2 3.5-.6 1-.4 1.8-1 2.4-1.7a4 4 0 0 0 .8-2.4c0-.9-.2-1.6-.7-2.2a11 11 0 0 0-2.1-1.4l-3.2-1-3.8-1c-2.8-.7-5-1.7-6.6-3.2a7.2 7.2 0 0 1-2.4-5.7 8 8 0 0 1 1.7-5 10 10 0 0 1 4.3-3.5c2-.8 4-1.2 6.4-1.2 2.3 0 4.4.4 6.2 1.2 1.8.8 3.2 2 4.3 3.4 1 1.4 1.5 3 1.5 5h-5.8z"/></svg>
</file>

<file path="public/vercel.svg">
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1155 1000"><path d="m577.3 0 577.4 1000H0z" fill="#fff"/></svg>
</file>

<file path="public/window.svg">
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.5 2.5h13v10a1 1 0 0 1-1 1h-11a1 1 0 0 1-1-1zM0 1h16v11.5a2.5 2.5 0 0 1-2.5 2.5h-11A2.5 2.5 0 0 1 0 12.5zm3.75 4.5a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5M7 4.75a.75.75 0 1 1-1.5 0 .75.75 0 0 1 1.5 0m1.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5" fill="#666"/></svg>
</file>

<file path="QUICKSTART.md">
# Quick Start - RAG Superbot with LiteLLM

Get up and running in 5 minutes!

## Prerequisites

‚úÖ Docker Desktop installed
‚úÖ Node.js 18+ installed  
‚úÖ 1minAI API key ([Get one here](https://1min.ai))

## Step 1: Get Your API Keys

1. **1minAI API Key**: https://1min.ai
2. **Google API Key** (for embeddings): https://makersuite.google.com/app/apikey
3. **Qdrant Cloud**: Already configured ‚úÖ

## Step 2: Configure Environment

```bash
# Copy template
cp config.env .env.local

# Edit .env.local with your API keys
notepad .env.local  # or use your favorite editor
```

**Update these values:**
```bash
ONEMINAI_API_KEY=your_1minai_key_here
NEXT_PUBLIC_GOOGLE_API_KEY=your_gemini_key_here
```

## Step 3: Install Dependencies

```bash
npm install
```

## Step 4: Start Services

```powershell
# Start LiteLLM proxy
.\start-services.ps1

# In another terminal, start Next.js
npm run dev
```

## Step 5: Test It!

Open http://localhost:3000 and start chatting!

## Verify Everything Works

### Check LiteLLM Proxy
```bash
curl http://localhost:8000/health
```

### Check Models
```bash
curl http://localhost:8000/v1/models
```

### Test Chat
Ask a question in the web interface at http://localhost:3000

## View Logs

```powershell
.\logs.ps1
```

## Stop Services

```powershell
.\stop-services.ps1
```

## Troubleshooting

**Problem:** Docker not starting
- **Solution:** Ensure Docker Desktop is running

**Problem:** Port 8000 already in use
- **Solution:** Stop other services using port 8000

**Problem:** 1minAI API errors
- **Solution:** Check your ONEMINAI_API_KEY in .env.local

## Next Steps

- Read [LITELLM_INTEGRATION.md](./LITELLM_INTEGRATION.md) for detailed documentation
- Try different models by changing `NEXT_PUBLIC_LITELLM_MODEL`
- Adjust temperature and max tokens for better responses

## Quick Commands Reference

| Action | Command |
|--------|---------|
| Start services | `.\start-services.ps1` |
| Start Next.js | `npm run dev` |
| View logs | `.\logs.ps1` |
| Stop services | `.\stop-services.ps1` |
| Health check | `curl http://localhost:8000/health` |
| List models | `curl http://localhost:8000/v1/models` |

üöÄ **Happy chatting with your RAG Superbot!**
</file>

<file path="railway-env-template.txt">
# Railway Environment Variables Template
# Set these in your Railway dashboard or via CLI

# 1minAI API Configuration
ONEMINAI_API_KEY=your_1minai_api_key_here

# FastAPI Server Configuration
FASTAPI_HOST=0.0.0.0
FASTAPI_PORT=8000
FASTAPI_RELOAD=false
FASTAPI_LOG_LEVEL=info

# LiteLLM Configuration
LITELLM_BASE_URL=https://api.1min.ai
DEFAULT_MODEL=gemini-2.0-flash-lite
MAX_TOKENS=4096
TEMPERATURE=0.7

# CORS Configuration
CORS_ORIGINS=*
CORS_ALLOW_CREDENTIALS=true

# Health Check Configuration
HEALTH_CHECK_INTERVAL=30

# Set these via Railway CLI:
# railway variables set ONEMINAI_API_KEY=your_key_here
# railway variables set FASTAPI_HOST=0.0.0.0
# railway variables set FASTAPI_PORT=8000
# railway variables set CORS_ORIGINS=*
</file>

<file path="railway.json">
{
    "$schema": "https://railway.app/railway.schema.json",
    "build": {
        "builder": "DOCKERFILE",
        "dockerfilePath": "Dockerfile.fastapi"
    },
    "deploy": {
        "startCommand": "python fastapi_server.py",
        "healthcheckPath": "/health",
        "healthcheckTimeout": 100,
        "restartPolicyType": "ON_FAILURE",
        "restartPolicyMaxRetries": 10
    }
}
</file>

<file path="README.md">
# RAG A2A Superbot - LiteLLM Edition

A Next.js RAG (Retrieval-Augmented Generation) application with intelligent agent-based responses powered by **LiteLLM proxy + 1minAI** for free multi-model access (Gemini 2.0 Flash Lite, GPT-4o, Claude, etc.) and Qdrant Cloud for vector search.

## üÜï What's New - LiteLLM Integration

**üéâ Now using FREE 1minAI API via LiteLLM Proxy!**

- ‚úÖ **Free API Access**: Use 1minAI's free tier for text generation
- ‚úÖ **Multi-Model Support**: Switch between Gemini, GPT-4, Claude, and more
- ‚úÖ **FastAPI Proxy**: OpenAI-compatible proxy server with Docker
- ‚úÖ **Intelligent Fallback**: Auto-fallback to Google Gemini if needed
- ‚úÖ **Same RAG Quality**: Unchanged vector search with Qdrant
- ‚úÖ **Easy Setup**: One-command Docker deployment

**üìö Quick Links:**
- **[Quick Start Guide](./QUICKSTART.md)** - Get running in 5 minutes
- **[Full Integration Docs](./LITELLM_INTEGRATION.md)** - Detailed technical guide
- **[Deployment Scripts](./start-services.ps1)** - Easy service management

## üöÄ Features

### ‚úÖ LiteLLM + 1minAI Integration (NEW!)
- **Default Model**: `gemini-2.0-flash-lite` via 1minAI (FREE!)
- **Available Models**: Gemini 2.0, GPT-4o, Claude 3.5, and more
- **OpenAI-Compatible**: Standard chat completion API
- **Docker-Based**: FastAPI proxy server with health checks
- **Multi-Tier Fallback**: LiteLLM ‚Üí Google Gemini ‚Üí Ollama
- **Error Handling**: Comprehensive error handling and logging

### ‚úÖ Google Gemini Integration (Fallback + Embeddings)
- **Embeddings**: `gemini-embedding-001` (3072 dimensions)
- **Fallback Model**: `gemini-1.5-flash` for text generation
- **Temperature**: 0.7 (configurable)
- **Max Tokens**: 2048 (configurable)

### ‚úÖ Qdrant Cloud Database Support
- **Cloud Support**: Full Qdrant Cloud integration with API key authentication
- **Local Support**: Maintains compatibility with local Qdrant instances
- **Auto-Detection**: Automatically detects and uses cloud vs local based on configuration
- **Security**: Secure API key management through environment variables

### ‚úÖ Advanced RAG Capabilities
- **Multiple Pipeline Modes**: Choose from 5 different processing pipelines
- **Real-time Thinking Process**: See how agents process your queries step-by-step
- **Document Management**: Load sample documents or upload your own
- **Modern UI**: Clean, responsive interface built with React and Tailwind CSS
- **Vector Store Support**: Chroma and Qdrant vector database integration
- **LLM Integration**: Google Gemini and Ollama support

## üèóÔ∏è Architecture

### API Routes

- `/api/chat` - Main chat endpoint with RAG pipeline
- `/api/documents` - Document upload and management
- `/api/sample-documents` - Load sample knowledge base
- `/api/status` - System health and configuration status

### Pipeline Modes

1. **Phase 1: Basic A2A** - Simple query ‚Üí retrieval ‚Üí answer
2. **Phase 2: Smart A2A** - Adds answer evaluation and refinement
3. **Phase 3: Self-Refinement** - Iterative improvement with multiple refinement cycles
4. **AUTO: AI Selects Optimal** - Automatically chooses the best pipeline
5. **META: Intelligent Selection** - Advanced AI-driven pipeline selection

### Agent Architecture

- **QueryAgent**: Processes and analyzes user queries
- **RetrievalAgent**: Searches vector database for relevant documents
- **AnswerAgent**: Generates responses using retrieved context
- **CriticAgent**: Evaluates answer quality and relevance
- **RefineAgent**: Improves answers based on critique

## üõ†Ô∏è Quick Start

### 1. Install Dependencies

```bash
cd frontend
npm install
```

### 2. Configure Environment

```bash
# Copy the configuration template
cp config.env .env.local

# Edit .env.local with your API keys
nano .env.local
```

### 3. Test Integration

```bash
# Run the integration test
npm test
```

### 4. Start Development Server

```bash
# Start the Next.js development server
npm run dev
```

Visit `http://localhost:3000` to see your RAG application.

## üîß Configuration

### Environment Variables

Create a `.env.local` file in the frontend directory with the following variables:

```bash
# Google Gemini Configuration
NEXT_PUBLIC_GOOGLE_API_KEY=your_gemini_api_key_here
NEXT_PUBLIC_GEMINI_MODEL=gemini-1.5-flash
NEXT_PUBLIC_GEMINI_TEMPERATURE=0.7
NEXT_PUBLIC_GEMINI_MAX_TOKENS=2048

# Qdrant Cloud Configuration
NEXT_PUBLIC_QDRANT_CLOUD_URL=https://your-cluster-id.eu-central.aws.cloud.qdrant.io
NEXT_PUBLIC_QDRANT_CLOUD_API_KEY=your_qdrant_cloud_api_key_here

# Vector Store Configuration
NEXT_PUBLIC_VECTOR_STORE=qdrant
NEXT_PUBLIC_COLLECTION_NAME=rag_a2a_collection

# Fallback Configuration (for local development)
NEXT_PUBLIC_QDRANT_HOST=localhost
NEXT_PUBLIC_QDRANT_PORT=6333
NEXT_PUBLIC_OLLAMA_HOST=http://localhost:11434

# Chroma Configuration (if using Chroma)
NEXT_PUBLIC_CHROMA_PATH=./chroma_db
```

### Getting API Keys

#### Google Gemini API Key
1. Go to [Google AI Studio](https://makersuite.google.com/app/apikey)
2. Sign in with your Google account
3. Click "Create API Key"
4. Copy the generated API key
5. Add it to your `.env.local` file as `NEXT_PUBLIC_GOOGLE_API_KEY`

#### Qdrant Cloud API Key
1. Go to [Qdrant Cloud](https://cloud.qdrant.io/)
2. Sign up or sign in to your account
3. Create a new cluster
4. Go to your cluster dashboard
5. Copy the cluster URL and API key
6. Add them to your `.env.local` file as `NEXT_PUBLIC_QDRANT_CLOUD_URL` and `NEXT_PUBLIC_QDRANT_CLOUD_API_KEY`

## üß™ Testing

### Run Integration Tests

```bash
# Test all components
npm test

# Test specific components
node test-gemini-qdrant.mjs
```

### Test Results

The test script will verify:
- ‚úÖ Google Gemini API connectivity
- ‚úÖ Qdrant Cloud/Local connectivity
- ‚úÖ Ollama fallback functionality
- ‚úÖ Embedding generation
- ‚úÖ Environment variable configuration
- ‚úÖ End-to-end integration

### Manual Testing

1. **Load Sample Documents**: Click the "Load Sample Docs" button to populate the knowledge base
2. **Test Different Pipelines**: Try different pipeline modes with various queries
3. **Observe Thinking Process**: Watch the real-time agent thinking steps

## üìö Usage

### Basic Chat

1. Select a pipeline mode from the dropdown
2. Type your question in the input field
3. Press Enter or click Send
4. Watch the agents process your query in real-time
5. Review the generated response and thinking steps

### Sample Queries

Try these sample queries to test different capabilities:

- **Simple**: "What is AI?"
- **Medium**: "How does machine learning work?"
- **Complex**: "Compare the advantages and disadvantages of different vector databases for RAG systems"
- **Technical**: "Explain the agent-to-agent architecture pattern and its benefits"

### Pipeline Selection Guide

- **Phase 1**: Use for simple, straightforward questions
- **Phase 2**: Use for questions requiring some analysis
- **Phase 3**: Use for complex queries needing thorough processing
- **AUTO**: Let the system choose based on query complexity
- **META**: Use for advanced queries requiring intelligent processing

## üèóÔ∏è Architecture Details

### Vector Store Selection Logic

```typescript
// Automatic detection based on environment variables
if (QDRANT_CLOUD_URL && QDRANT_CLOUD_API_KEY) {
  // Use Qdrant Cloud
  client = new QdrantClient({
    url: QDRANT_CLOUD_URL,
    apiKey: QDRANT_CLOUD_API_KEY
  });
} else {
  // Use local Qdrant
  client = new QdrantClient({
    host: QDRANT_HOST,
    port: QDRANT_PORT
  });
}
```

### LLM Selection Logic

```typescript
// Try Google Gemini first
if (genAI) {
  try {
    const model = genAI.getGenerativeModel({ 
      model: GEMINI_MODEL,
      generationConfig: {
        temperature: GEMINI_TEMPERATURE,
        maxOutputTokens: GEMINI_MAX_TOKENS,
      }
    });
    return await model.generateContent(prompt);
  } catch (error) {
    // Fallback to Ollama
    return await ollamaGenerate(prompt);
  }
}
```

### Vector Store Options

**Qdrant Cloud (Recommended)**:
- High-performance vector search
- Scalable cloud infrastructure
- Enterprise-grade security
- Automatic backups and monitoring

**Qdrant Local**:
- Local processing
- Good for development
- Requires separate Qdrant server

**Chroma (Fallback)**:
- Lightweight and easy to use
- Good for development and testing
- Stores data locally

### LLM Configuration

**Google Gemini 1.5 Flash** (Primary):
- High-quality responses
- Fast processing
- Configurable parameters
- Automatic fallback to Ollama

**Ollama** (Fallback):
- Local processing
- No API key required
- Requires Ollama server running

## üìÅ Project Structure

```
frontend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat/route.ts          # Main chat API
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ documents/route.ts     # Document management
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sample-documents/route.ts # Sample data loading
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ status/route.ts        # System status
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ globals.css                # Global styles
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx                 # App layout
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx                   # Main chat interface
‚îÇ   ‚îî‚îÄ‚îÄ lib/
‚îÇ       ‚îú‚îÄ‚îÄ agents.ts                  # AI agents with Gemini integration
‚îÇ       ‚îú‚îÄ‚îÄ pipelines.ts               # RAG pipelines
‚îÇ       ‚îî‚îÄ‚îÄ vectorstore.ts             # Vector store with Qdrant Cloud
‚îú‚îÄ‚îÄ config.env                         # Configuration template
‚îú‚îÄ‚îÄ test-gemini-qdrant.mjs            # Integration test script
‚îú‚îÄ‚îÄ test-integration.js                # Legacy integration test
‚îî‚îÄ‚îÄ README.md                          # This file
```

## üö® Troubleshooting

### Common Issues

#### 1. Environment Variables Not Loading
```
Error: NEXT_PUBLIC_GOOGLE_API_KEY not found
```
**Solution**: Ensure variables start with `NEXT_PUBLIC_` and are in `.env.local`

#### 2. CORS Issues
```
Error: CORS policy blocked
```
**Solution**: Check that Ollama is running and accessible

#### 3. Qdrant Connection Issues
```
Error: Qdrant connection failed
```
**Solution**: Verify your Qdrant Cloud credentials or local Qdrant instance

#### 4. Gemini API Issues
```
Error: Google Gemini API failed
```
**Solution**: Check your API key and quota limits

#### 5. Ollama Connection Failed
```
Error: Ollama connection failed
```
**Solution**: Ensure Ollama is running: `ollama serve`

#### 6. Vector Store Initialization Failed
```
Error: Vector store initialization failed
```
**Solution**: Check vector store configuration and permissions

### Debug Mode

Enable debug logging by setting:
```bash
NODE_ENV=development
```

### Logs

Check the browser console and terminal for detailed error messages.

## üîÑ Development

### Adding New Agents

1. Create a new agent class in `src/lib/agents.ts`
2. Implement the `Agent` interface
3. Add the agent to your pipeline in `src/lib/pipelines.ts`

### Adding New Pipelines

1. Create a new pipeline class in `src/lib/pipelines.ts`
2. Implement the `Pipeline` interface
3. Add the pipeline to the factory function
4. Update the frontend dropdown options

### Customizing UI

The main UI is in `src/app/page.tsx`. Key components:
- Message display and formatting
- Thinking steps visualization
- Pipeline mode selector
- Document loading interface

## üìà Performance

### Optimization Tips

1. **Caching**: Implement response caching for repeated queries
2. **Streaming**: Use streaming for long responses
3. **CDN**: Use a CDN for static assets
4. **Database**: Optimize vector database queries

### Monitoring

Monitor key metrics:
- API response times
- Error rates
- Token usage (Gemini)
- Vector search performance

Check system status at `/api/status` to monitor:
- Vector store connectivity
- Ollama service status
- Google AI configuration
- Overall system health

## üöÄ Vercel Deployment (Recommended)

### Quick Deploy to Vercel

1. **Push to GitHub**:
   ```bash
   git add .
   git commit -m "Ready for Vercel deployment"
   git push origin main
   ```

2. **Deploy via Vercel Dashboard**:
   - Go to [Vercel Dashboard](https://vercel.com/dashboard)
   - Click "New Project"
   - Import your GitHub repository
   - Set root directory to `frontend` (if needed)

3. **Configure Environment Variables**:
   ```bash
   # Required for Vercel
   NEXT_PUBLIC_GOOGLE_API_KEY=your_gemini_api_key
   NEXT_PUBLIC_QDRANT_CLOUD_URL=your_qdrant_cloud_url
   NEXT_PUBLIC_QDRANT_CLOUD_API_KEY=your_qdrant_cloud_key
   NEXT_PUBLIC_VECTOR_STORE=qdrant
   NEXT_PUBLIC_COLLECTION_NAME=rag_a2a_collection
   ```

4. **Deploy**: Click "Deploy" and wait for completion

### Vercel CLI Deployment

```bash
# Install Vercel CLI
npm i -g vercel

# Login and deploy
vercel login
vercel

# Deploy to production
vercel --prod
```

### Environment Variables for Vercel

For production, ensure all environment variables are set in Vercel dashboard:

```bash
# Google Gemini Configuration
NEXT_PUBLIC_GOOGLE_API_KEY=your_production_key
NEXT_PUBLIC_GEMINI_MODEL=gemini-1.5-flash
NEXT_PUBLIC_GEMINI_TEMPERATURE=0.7
NEXT_PUBLIC_GEMINI_MAX_TOKENS=2048

# Qdrant Cloud Configuration (Required for Vercel)
NEXT_PUBLIC_QDRANT_CLOUD_URL=your_cloud_url
NEXT_PUBLIC_QDRANT_CLOUD_API_KEY=your_cloud_key

# Vector Store Configuration
NEXT_PUBLIC_VECTOR_STORE=qdrant
NEXT_PUBLIC_COLLECTION_NAME=your_collection

# App Configuration
NEXT_PUBLIC_APP_NAME=RAG A2A Superbot
NEXT_PUBLIC_APP_VERSION=1.0.0
```

### Build and Deploy Locally

```bash
# Build the application
npm run build

# Start production server
npm run start
```

### Other Deployment Platforms

#### Netlify
1. Connect your GitHub repository
2. Set environment variables in Netlify dashboard
3. Deploy automatically

#### Docker
```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production
COPY . .
RUN npm run build
EXPOSE 3000
CMD ["npm", "start"]
```

### üìã Vercel Deployment Checklist

- [ ] Google Gemini API key configured
- [ ] Qdrant Cloud account and cluster created
- [ ] All environment variables set in Vercel dashboard
- [ ] Repository pushed to GitHub
- [ ] Vercel project connected to GitHub repository
- [ ] Build completed successfully
- [ ] Application tested on deployed URL

## üîí Security

### API Key Management
- Never commit API keys to version control
- Use environment variables for all sensitive data
- Rotate API keys regularly
- Monitor API usage

### Network Security
- Use HTTPS in production
- Implement rate limiting
- Validate all inputs
- Monitor for unusual activity

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Submit a pull request

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

## üìû Support

### Getting Help

1. Check the troubleshooting section
2. Review the test results
3. Check browser console for errors
4. Verify environment configuration

### Common Solutions

- **API Key Issues**: Verify keys are correct and have proper permissions
- **Connection Issues**: Check network connectivity and service availability
- **Configuration Issues**: Ensure all required environment variables are set

---

This setup provides a robust, scalable frontend for your RAG A2A Superbot with enterprise-grade LLM and vector database capabilities.

**Happy chatting with your RAG A2A Superbot!** ü§ñ‚ú®
</file>

<file path="render-python311.yaml">
# Alternative render.yaml for forcing Python 3.11
# Use this if you specifically need Python 3.11

services:
  - type: web
    name: psychiatry-therapy-superbot-api
    env: python3
    plan: free
    region: oregon
    branch: main
    
    # Force Python 3.11 installation
    buildCommand: |
      echo "Current Python version:" && python --version &&
      echo "Installing Python 3.11..." &&
      apt-get update && apt-get install -y python3.11 python3.11-venv &&
      python3.11 --version &&
      echo "Python 3.11 installation complete"
    
    # Use Python 3.11 explicitly
    startCommand: "python3.11 simple_server.py"
    healthCheckPath: /health
    
    envVars:
      - key: FASTAPI_HOST
        value: "0.0.0.0"
      - key: FASTAPI_RELOAD
        value: "false"
      - key: FASTAPI_LOG_LEVEL
        value: "info"
      - key: LITELLM_BASE_URL
        value: "https://api.1min.ai"
      - key: DEFAULT_MODEL
        value: "gemini-2.0-flash-lite"
      - key: MAX_TOKENS
        value: "4096"
      - key: TEMPERATURE
        value: "0.7"
      - key: CORS_ORIGINS
        value: "*"
      - key: CORS_ALLOW_CREDENTIALS
        value: "true"
      - key: HEALTH_CHECK_INTERVAL
        value: "30"
      - key: ONEMINAI_API_KEY
        sync: false

    autoDeploy: true

# Note: This approach installs Python 3.11 manually
# It may increase build time but gives you exact version control
</file>

<file path="requirements-minimal.txt">
# Minimal requirements for Render Free Tier (Python 3.13 compatible)
# Uses only built-in Python modules to avoid dependency conflicts

# No external dependencies needed!
# The simple_server.py uses only Python standard library
</file>

<file path="requirements-render-minimal.txt">
# Minimal dependencies for Render Free Tier
# Uses only packages that don't require compilation

# No dependencies - using only Python standard library
# This ensures maximum compatibility with Render's free tier

# If you need to add dependencies, use these compilation-free versions:
# fastapi==0.68.0  # Older version without pydantic v2
# uvicorn==0.15.0  # Older stable version
# python-dotenv==0.19.0  # For .env file support
</file>

<file path="setup-qdrant-collection.mjs">
#!/usr/bin/env node

/**
 * Qdrant Collection Setup Script
 * Run with: node setup-qdrant-collection.mjs
 */

import { QdrantClient } from '@qdrant/js-client-rest';
import { GoogleGenerativeAI } from '@google/generative-ai';
import dotenv from 'dotenv';

// Load environment variables
dotenv.config({ path: './.env.local' });

const QDRANT_CLOUD_URL = process.env.NEXT_PUBLIC_QDRANT_CLOUD_URL;
const QDRANT_CLOUD_API_KEY = process.env.NEXT_PUBLIC_QDRANT_CLOUD_API_KEY;
const COLLECTION_NAME = process.env.NEXT_PUBLIC_COLLECTION_NAME || 'psychiatry_therapy_v1_google-001';
const GOOGLE_API_KEY = process.env.NEXT_PUBLIC_GOOGLE_API_KEY;
const EMBEDDING_MODEL = process.env.NEXT_PUBLIC_EMBEDDING_MODEL || 'embedding-001';
const EMBEDDING_DIM = parseInt(process.env.NEXT_PUBLIC_EMBEDDING_DIM || '3072');

async function main() {
  console.log("üîß Qdrant Collection Setup\n");

  // Validate environment variables
  if (!QDRANT_CLOUD_URL || !QDRANT_CLOUD_API_KEY) {
    console.error("‚ùå Qdrant Cloud credentials not found in .env.local");
    console.log("Please ensure NEXT_PUBLIC_QDRANT_CLOUD_URL and NEXT_PUBLIC_QDRANT_CLOUD_API_KEY are set");
    process.exit(1);
  }

  if (!GOOGLE_API_KEY) {
    console.error("‚ùå Google API key not found in .env.local");
    console.log("Please ensure NEXT_PUBLIC_GOOGLE_API_KEY is set");
    process.exit(1);
  }

  console.log(`üîç Configuration:`);
  console.log(`  Qdrant URL: ${QDRANT_CLOUD_URL}`);
  console.log(`  Collection: ${COLLECTION_NAME}`);
  console.log(`  Embedding Model: ${EMBEDDING_MODEL}`);
  console.log(`  Vector Dimensions: ${EMBEDDING_DIM}\n`);

  try {
    // Initialize Qdrant client
    const client = new QdrantClient({
      url: QDRANT_CLOUD_URL,
      apiKey: QDRANT_CLOUD_API_KEY
    });

    // Check existing collections
    console.log("üîç Checking existing collections...");
    const collections = await client.getCollections();

    console.log(`üìã Found ${collections.collections.length} collections:`);
    collections.collections.forEach(col => {
      console.log(`  - ${col.name}`);
    });
    console.log();

    // Check if target collection exists
    const existingCollection = collections.collections.find(col => col.name === COLLECTION_NAME);

    if (existingCollection) {
      console.log(`‚úÖ Collection '${COLLECTION_NAME}' already exists!`);

      // Get collection info
      const info = await client.getCollection(COLLECTION_NAME);
      const vectorSize = info.config?.params?.vectors?.size;
      console.log(`üìä Vector dimensions: ${vectorSize}`);

      if (vectorSize !== EMBEDDING_DIM) {
        console.log(`‚ö†Ô∏è  Warning: Collection vector size (${vectorSize}) doesn't match configured embedding dimensions (${EMBEDDING_DIM})`);
      }

      // Get collection stats
      const collectionInfo = await client.getCollection(COLLECTION_NAME);
      console.log(`üìà Collection stats:`);
      console.log(`  - Points count: ${collectionInfo.points_count || 0}`);
      console.log(`  - Status: ${collectionInfo.status}`);

    } else {
      console.log(`‚ùå Collection '${COLLECTION_NAME}' not found`);
      console.log(`üöÄ Creating collection with ${EMBEDDING_DIM} dimensions...`);

      // Create the collection
      await client.createCollection(COLLECTION_NAME, {
        vectors: {
          size: EMBEDDING_DIM,
          distance: 'Cosine'
        }
      });

      console.log(`‚úÖ Collection '${COLLECTION_NAME}' created successfully!`);

      // Verify creation
      const newInfo = await client.getCollection(COLLECTION_NAME);
      console.log(`üìä New collection vector dimensions: ${newInfo.config?.params?.vectors?.size}`);
    }

    // Test embedding generation
    console.log("\nüß™ Testing embedding generation...");
    const genAI = new GoogleGenerativeAI(GOOGLE_API_KEY);
    const model = genAI.getGenerativeModel({ model: EMBEDDING_MODEL });

    const testText = "This is a test for the RAG system.";
    const result = await model.embedContent(testText);
    const embedding = result.embedding.values;

    console.log(`‚úÖ Successfully generated ${embedding.length}-dimensional embedding`);

    if (embedding.length !== EMBEDDING_DIM) {
      console.log(`‚ö†Ô∏è  Warning: Generated embedding size (${embedding.length}) doesn't match configured size (${EMBEDDING_DIM})`);
      console.log(`üí° Consider updating NEXT_PUBLIC_EMBEDDING_DIM to ${embedding.length}`);
    }

    console.log("\nüéâ Setup completed successfully!");
    console.log("\nüìã Next steps:");
    console.log("1. Load sample documents: npm run dev (then use 'Load Sample Docs' button)");
    console.log("2. Or upload your own documents via the /api/documents endpoint");
    console.log("3. Start chatting with your RAG system!");

  } catch (error) {
    console.error("\n‚ùå Setup failed:", error.message);

    if (error.message.includes('401') || error.message.includes('403')) {
      console.log("üí° Check your Qdrant Cloud API key permissions");
    } else if (error.message.includes('API_KEY_INVALID')) {
      console.log("üí° Check your Google API key");
    } else {
      console.log("üí° Full error details:", error);
    }

    process.exit(1);
  }
}

// Run the setup
main().catch(console.error);
</file>

<file path="src/app/api/chat/route.ts">
import { NextRequest, NextResponse } from 'next/server';
import { createPipeline } from '@/lib/pipelines';

export async function POST(request: NextRequest) {
  try {
    const { message, pipelineMode = 'meta' } = await request.json();
    
    if (!message || typeof message !== 'string') {
      return NextResponse.json(
        { success: false, error: 'Message is required and must be a string' },
        { status: 400 }
      );
    }

    // Create and run the selected pipeline
    const pipeline = createPipeline(pipelineMode);
    const result = await pipeline.process(message);

    const response = {
      success: true,
      answer: result.answer,
      thinkingSteps: result.thinkingSteps,
      pipelineInfo: result.pipelineInfo,
      sources: result.sources || [],
      timestamp: new Date().toISOString()
    };

    return NextResponse.json(response);
  } catch (error) {
    console.error('Chat API error:', error);
    return NextResponse.json(
      { 
        success: false, 
        error: error instanceof Error ? error.message : 'Unknown error',
        timestamp: new Date().toISOString()
      },
      { status: 500 }
    );
  }
}
</file>

<file path="src/app/api/documents/route.ts">
import { NextRequest, NextResponse } from 'next/server';
import { getVectorStore } from '@/lib/vectorstore';

export async function POST(request: NextRequest) {
  try {
    const { documents } = await request.json();
    
    if (!documents || !Array.isArray(documents) || documents.length === 0) {
      return NextResponse.json(
        { success: false, error: 'Documents array is required and must not be empty' },
        { status: 400 }
      );
    }

    // Validate document format
    for (const doc of documents) {
      if (!doc.content || typeof doc.content !== 'string') {
        return NextResponse.json(
          { success: false, error: 'Each document must have a content field' },
          { status: 400 }
        );
      }
    }

    // Add documents to vector store
    const vectorStore = await getVectorStore();
    await vectorStore.addDocuments(documents);

    return NextResponse.json({
      success: true,
      message: `Successfully added ${documents.length} documents to the knowledge base`,
      documentCount: documents.length,
      timestamp: new Date().toISOString()
    });
  } catch (error) {
    console.error('Document upload error:', error);
    return NextResponse.json(
      { 
        success: false, 
        error: error instanceof Error ? error.message : 'Unknown error',
        timestamp: new Date().toISOString()
      },
      { status: 500 }
    );
  }
}

export async function DELETE(request: NextRequest) {
  try {
    const vectorStore = await getVectorStore();
    await vectorStore.deleteCollection();

    return NextResponse.json({
      success: true,
      message: 'Knowledge base cleared successfully',
      timestamp: new Date().toISOString()
    });
  } catch (error) {
    console.error('Document deletion error:', error);
    return NextResponse.json(
      { 
        success: false, 
        error: error instanceof Error ? error.message : 'Unknown error',
        timestamp: new Date().toISOString()
      },
      { status: 500 }
    );
  }
}
</file>

<file path="src/app/api/sample-documents/route.ts">
import { NextRequest, NextResponse } from 'next/server';
import { getVectorStore } from '@/lib/vectorstore';

const sampleDocuments = [
  {
    content: "Artificial Intelligence (AI) is a branch of computer science that aims to create machines capable of intelligent behavior. AI systems can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation.",
    metadata: { 
      title: "Introduction to Artificial Intelligence",
      category: "AI Basics",
      source: "Educational Material"
    }
  },
  {
    content: "Machine Learning is a subset of artificial intelligence that focuses on algorithms and statistical models that enable computers to improve their performance on a specific task through experience. It includes supervised learning, unsupervised learning, and reinforcement learning approaches.",
    metadata: { 
      title: "Machine Learning Fundamentals",
      category: "AI Basics",
      source: "Educational Material"
    }
  },
  {
    content: "Retrieval-Augmented Generation (RAG) is an AI technique that combines information retrieval with text generation. It retrieves relevant documents from a knowledge base and uses them as context to generate more accurate and informative responses. This approach helps reduce hallucinations and provides more factual answers.",
    metadata: { 
      title: "Retrieval-Augmented Generation",
      category: "AI Techniques",
      source: "Research Paper"
    }
  },
  {
    content: "Agent-to-Agent (A2A) architecture is a design pattern where multiple specialized AI agents work together to solve complex problems. Each agent has a specific role and can communicate with other agents to share information and coordinate their actions. This approach enables more modular, scalable, and maintainable AI systems.",
    metadata: { 
      title: "Agent-to-Agent Architecture",
      category: "AI Architecture",
      source: "Technical Documentation"
    }
  },
  {
    content: "Vector databases are specialized databases designed to store and search high-dimensional vectors efficiently. They use techniques like approximate nearest neighbor search to find similar vectors quickly. Popular vector databases include Chroma, Qdrant, Pinecone, and Weaviate. They are essential for AI applications that use embeddings for semantic search.",
    metadata: { 
      title: "Vector Databases",
      category: "AI Infrastructure",
      source: "Technical Documentation"
    }
  },
  {
    content: "Embeddings are dense vector representations of text, images, or other data that capture semantic meaning. They enable AI systems to understand relationships between different pieces of information. Popular embedding models include OpenAI's text-embedding-ada-002, Google's Universal Sentence Encoder, and various open-source models like sentence-transformers.",
    metadata: { 
      title: "Embeddings in AI",
      category: "AI Techniques",
      source: "Research Paper"
    }
  },
  {
    content: "Large Language Models (LLMs) are AI models trained on vast amounts of text data to understand and generate human-like text. They can perform various natural language processing tasks including text generation, translation, summarization, and question answering. Examples include GPT, BERT, T5, and more recent models like ChatGPT and Claude.",
    metadata: { 
      title: "Large Language Models",
      category: "AI Models",
      source: "Research Paper"
    }
  },
  {
    content: "Prompt engineering is the practice of designing effective prompts to get desired outputs from AI models. It involves crafting clear, specific instructions and providing relevant context. Good prompt engineering can significantly improve the quality and reliability of AI responses. Techniques include few-shot learning, chain-of-thought prompting, and role-based prompting.",
    metadata: { 
      title: "Prompt Engineering",
      category: "AI Techniques",
      source: "Best Practices Guide"
    }
  }
];

export async function POST(request: NextRequest) {
  try {
    const vectorStore = await getVectorStore();
    await vectorStore.addDocuments(sampleDocuments);

    return NextResponse.json({
      success: true,
      message: `Successfully loaded ${sampleDocuments.length} sample documents into the knowledge base`,
      documentCount: sampleDocuments.length,
      documents: sampleDocuments.map(doc => ({
        title: doc.metadata.title,
        category: doc.metadata.category,
        source: doc.metadata.source
      })),
      timestamp: new Date().toISOString()
    });
  } catch (error) {
    console.error('Sample documents loading error:', error);
    return NextResponse.json(
      { 
        success: false, 
        error: error instanceof Error ? error.message : 'Unknown error',
        timestamp: new Date().toISOString()
      },
      { status: 500 }
    );
  }
}
</file>

<file path="src/app/api/status/route.ts">
import { NextRequest, NextResponse } from 'next/server';
import { getVectorStore } from '@/lib/vectorstore';
import axios from 'axios';

export async function GET(request: NextRequest) {
  try {
    const status = {
      system: 'online',
      timestamp: new Date().toISOString(),
      services: {
        vectorStore: 'unknown',
        ollama: 'unknown',
        googleAI: 'unknown'
      },
      configuration: {
        vectorStoreType: process.env.VECTOR_STORE || 'chroma',
        ollamaHost: process.env.OLLAMA_HOST || 'http://localhost:11434',
        hasGoogleAPIKey: !!process.env.GOOGLE_API_KEY
      }
    };

    // Check vector store
    try {
      const vectorStore = await getVectorStore();
      status.services.vectorStore = 'online';
    } catch (error) {
      status.services.vectorStore = 'offline';
      console.warn('Vector store check failed:', error);
    }

    // Check Ollama
    try {
      const ollamaHost = process.env.OLLAMA_HOST || 'http://localhost:11434';
      await axios.get(`${ollamaHost}/api/tags`, { timeout: 5000 });
      status.services.ollama = 'online';
    } catch (error) {
      status.services.ollama = 'offline';
      console.warn('Ollama check failed:', error);
    }

    // Check Google AI (if API key is provided)
    if (process.env.GOOGLE_API_KEY) {
      status.services.googleAI = 'configured';
    } else {
      status.services.googleAI = 'not_configured';
    }

    return NextResponse.json({
      success: true,
      status,
      timestamp: new Date().toISOString()
    });
  } catch (error) {
    console.error('Status check error:', error);
    return NextResponse.json(
      { 
        success: false, 
        error: error instanceof Error ? error.message : 'Unknown error',
        timestamp: new Date().toISOString()
      },
      { status: 500 }
    );
  }
}
</file>

<file path="src/app/globals.css">
@import "tailwindcss";

:root {
  --background: #ffffff;
  --foreground: #171717;
  --card: #ffffff;
  --card-foreground: #171717;
  --border: #e5e7eb;
  --input: #ffffff;
  --primary: #7c3aed;
  --primary-foreground: #ffffff;
  --secondary: #f3f4f6;
  --secondary-foreground: #374151;
  --muted: #f9fafb;
  --muted-foreground: #6b7280;
  --accent: #f3f4f6;
  --accent-foreground: #374151;
  --destructive: #ef4444;
  --destructive-foreground: #ffffff;
  --ring: #7c3aed;
}

@theme inline {
  --color-background: var(--background);
  --color-foreground: var(--foreground);
  --color-card: var(--card);
  --color-card-foreground: var(--card-foreground);
  --color-border: var(--border);
  --color-input: var(--input);
  --color-primary: var(--primary);
  --color-primary-foreground: var(--primary-foreground);
  --color-secondary: var(--secondary);
  --color-secondary-foreground: var(--secondary-foreground);
  --color-muted: var(--muted);
  --color-muted-foreground: var(--muted-foreground);
  --color-accent: var(--accent);
  --color-accent-foreground: var(--accent-foreground);
  --color-destructive: var(--destructive);
  --color-destructive-foreground: var(--destructive-foreground);
  --color-ring: var(--ring);
  --font-sans: var(--font-geist-sans);
  --font-mono: var(--font-geist-mono);
}

@media (prefers-color-scheme: dark) {
  :root {
    --background: #0a0a0a;
    --foreground: #ededed;
    --card: #1a1a1a;
    --card-foreground: #ededed;
    --border: #374151;
    --input: #1a1a1a;
    --primary: #8b5cf6;
    --primary-foreground: #ffffff;
    --secondary: #374151;
    --secondary-foreground: #d1d5db;
    --muted: #1f2937;
    --muted-foreground: #9ca3af;
    --accent: #374151;
    --accent-foreground: #d1d5db;
    --destructive: #ef4444;
    --destructive-foreground: #ffffff;
    --ring: #8b5cf6;
  }
}

body {
  background: var(--background);
  color: var(--foreground);
  font-family: var(--font-geist-sans), Arial, Helvetica, sans-serif;
}

/* Custom scrollbar for dark mode */
::-webkit-scrollbar {
  width: 8px;
}

::-webkit-scrollbar-track {
  background: var(--muted);
}

::-webkit-scrollbar-thumb {
  background: var(--muted-foreground);
  border-radius: 4px;
}

::-webkit-scrollbar-thumb:hover {
  background: var(--foreground);
}
</file>

<file path="src/app/layout.tsx">
import type { Metadata } from "next";
import { Geist, Geist_Mono } from "next/font/google";
import "./globals.css";

const geistSans = Geist({
  variable: "--font-geist-sans",
  subsets: ["latin"],
});

const geistMono = Geist_Mono({
  variable: "--font-geist-mono",
  subsets: ["latin"],
});

export const metadata: Metadata = {
  title: "Psychiatry Therapy SuperBot",
  description: "A RAG SuperBot for Psychiatry and Therapy that uses multi-agentic architecture",
  viewport: "width=device-width, initial-scale=1",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en">
      <body
        className={`${geistSans.variable} ${geistMono.variable} antialiased`}
      >
        {children}
      </body>
    </html>
  );
}
</file>

<file path="src/app/page.tsx">
'use client';

import { useState, useRef, useEffect } from 'react';
import { Send, Bot, User, Brain, Search, FileText, MessageSquare, CheckCircle, Clock, AlertCircle, ChevronDown, ChevronRight } from 'lucide-react';

interface ThinkingStep {
  agent: string;
  step: string;
  status: 'processing' | 'completed' | 'error';
  message: string;
  details?: any;
}

interface Message {
  id: string;
  type: 'user' | 'bot';
  content: string;
  timestamp: Date;
  thinkingSteps?: ThinkingStep[];
  pipelineInfo?: string;
  sources?: any[];
}

const agentIcons: Record<string, React.ReactNode> = {
  'QueryAgent': <Search className="w-4 h-4" />,
  'RetrievalAgent': <FileText className="w-4 h-4" />,
  'AnswerAgent': <MessageSquare className="w-4 h-4" />,
  'CriticAgent': <AlertCircle className="w-4 h-4" />,
  'RefineAgent': <CheckCircle className="w-4 h-4" />,
  'ContextOptimizerAgent': <Brain className="w-4 h-4" />,
  'SelfEvaluationAgent': <CheckCircle className="w-4 h-4" />,
  'DynamicRetrievalAgent': <FileText className="w-4 h-4" />,
  'QueryPreprocessorAgent': <Search className="w-4 h-4" />,
  'Pipeline': <Brain className="w-4 h-4" />
};

export default function Home() {
  const [messages, setMessages] = useState<Message[]>([
    {
      id: '1',
      type: 'bot',
      content: 'Hello! I\'m your Psychiatry Therapy SuperBot. I can help you with mental health questions using my advanced agent-based architecture. What would you like to know?',
      timestamp: new Date()
    }
  ]);
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [pipelineMode, setPipelineMode] = useState('meta');
  const [expandedThinkingSteps, setExpandedThinkingSteps] = useState<Record<string, boolean>>({});
  const [isClient, setIsClient] = useState(false);
  const messagesEndRef = useRef<HTMLDivElement>(null);

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  };

  useEffect(() => {
    scrollToBottom();
  }, [messages]);

  useEffect(() => {
    setIsClient(true);
  }, []);

  const sendMessage = async () => {
    if (!input.trim() || isLoading) return;

    const userMessage: Message = {
      id: Date.now().toString(),
      type: 'user',
      content: input.trim(),
      timestamp: new Date()
    };

    setMessages(prev => [...prev, userMessage]);
    setInput('');
    setIsLoading(true);

    try {
      const response = await fetch('/api/chat', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          message: input.trim(),
          pipelineMode: pipelineMode
        }),
      });

      const data = await response.json();

      if (data.success) {
        const botMessage: Message = {
          id: (Date.now() + 1).toString(),
          type: 'bot',
          content: data.answer,
          timestamp: new Date(),
          thinkingSteps: data.thinkingSteps || [],
          pipelineInfo: data.pipelineInfo,
          sources: data.sources || []
        };

        setMessages(prev => [...prev, botMessage]);
      } else {
        throw new Error(data.error || 'Failed to get response');
      }
    } catch (error) {
      const errorMessage: Message = {
        id: (Date.now() + 1).toString(),
        type: 'bot',
        content: `Sorry, I encountered an error: ${error instanceof Error ? error.message : 'Unknown error'}`,
        timestamp: new Date()
      };
      setMessages(prev => [...prev, errorMessage]);
    } finally {
      setIsLoading(false);
    }
  };

  const handleKeyPress = (e: React.KeyboardEvent) => {
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault();
      sendMessage();
    }
  };


  const getStatusColor = (status: string) => {
    switch (status) {
      case 'processing': return 'text-yellow-600 dark:text-yellow-400 bg-yellow-100 dark:bg-yellow-900/30';
      case 'completed': return 'text-green-600 dark:text-green-400 bg-green-100 dark:bg-green-900/30';
      case 'error': return 'text-red-600 dark:text-red-400 bg-red-100 dark:bg-red-900/30';
      default: return 'text-gray-600 dark:text-gray-400 bg-gray-100 dark:bg-gray-700';
    }
  };

  const getStatusIcon = (status: string) => {
    switch (status) {
      case 'processing': return <Clock className="w-3 h-3" />;
      case 'completed': return <CheckCircle className="w-3 h-3" />;
      case 'error': return <AlertCircle className="w-3 h-3" />;
      default: return <Clock className="w-3 h-3" />;
    }
  };

  const toggleThinkingSteps = (messageId: string) => {
    setExpandedThinkingSteps(prev => ({
      ...prev,
      [messageId]: !prev[messageId]
    }));
  };

  // Consistent timestamp formatting to prevent hydration errors
  const formatTimestamp = (date: Date) => {
    const hours = date.getHours();
    const minutes = date.getMinutes();
    const seconds = date.getSeconds();
    const ampm = hours >= 12 ? 'PM' : 'AM';
    const displayHours = hours % 12 || 12;
    const displayMinutes = minutes.toString().padStart(2, '0');
    const displaySeconds = seconds.toString().padStart(2, '0');
    return `${displayHours}:${displayMinutes}:${displaySeconds} ${ampm}`;
  };

  return (
    <div className="min-h-screen bg-gradient-to-br from-purple-50 via-blue-50 to-indigo-100 dark:from-gray-900 dark:via-gray-800 dark:to-gray-900">
      <div className="container mx-auto px-2 sm:px-4 py-4 sm:py-8">
        {/* Header */}
        <div className="text-center mb-4 sm:mb-8">
          <div className="flex items-center justify-center mb-2 sm:mb-4">
            <Brain className="w-6 h-6 sm:w-8 sm:h-8 text-purple-600 dark:text-purple-400 mr-2 sm:mr-3" />
            <h1 className="text-2xl sm:text-3xl md:text-4xl font-bold text-gray-800 dark:text-gray-100">Psychiatry Therapy SuperBot</h1>
          </div>
          <p className="text-sm sm:text-base lg:text-lg text-gray-600 dark:text-gray-300 mb-4 sm:mb-6 px-2">
            Intelligent Agent-to-Agent Architecture with Real-time Thinking Process
          </p>

          {/* Controls */}
          <div className="flex items-center justify-center gap-2 sm:gap-4 flex-wrap">
            <div className="flex items-center gap-1 sm:gap-2">
              <label className="text-xs sm:text-sm font-medium text-gray-700 dark:text-gray-300">Pipeline Mode:</label>
              <select
                value={pipelineMode}
                onChange={(e) => setPipelineMode(e.target.value)}
                className="px-2 sm:px-4 py-1 sm:py-2 text-xs sm:text-sm border border-gray-300 dark:border-gray-600 rounded-lg bg-white dark:bg-gray-800 text-gray-900 dark:text-gray-100 shadow-sm focus:ring-2 focus:ring-purple-500 focus:border-transparent"
              >
                <option value="phase1">Phase 1: Basic A2A</option>
                <option value="phase2">Phase 2: Smart A2A</option>
                <option value="phase3">Phase 3: Self-Refinement</option>
                <option value="auto">AUTO: AI Selects Optimal</option>
                <option value="meta">META: Intelligent Selection</option>
              </select>
            </div>
          </div>
        </div>

        {/* Chat Container */}
        <div className="max-w-4xl mx-auto bg-white dark:bg-gray-800 rounded-xl sm:rounded-2xl shadow-xl overflow-hidden">
          {/* Messages */}
          <div className="h-[70vh] sm:h-[600px] md:h-[700px] lg:h-[800px] xl:h-[900px] overflow-y-auto p-3 sm:p-4 md:p-6 space-y-3 sm:space-y-4">
            {messages.map((message) => (
              <div key={message.id} className="space-y-3">
                {/* Message */}
                <div className={`flex ${message.type === 'user' ? 'justify-end' : 'justify-start'}`}>
                  <div className={`flex items-start space-x-2 sm:space-x-3 max-w-[85%] sm:max-w-3xl ${message.type === 'user' ? 'flex-row-reverse space-x-reverse' : ''
                    }`}>
                    <div className={`flex-shrink-0 w-6 h-6 sm:w-8 sm:h-8 rounded-full flex items-center justify-center ${message.type === 'user'
                      ? 'bg-blue-500 dark:bg-blue-600 text-white'
                      : 'bg-purple-500 dark:bg-purple-600 text-white'
                      }`}>
                      {message.type === 'user' ? <User className="w-3 h-3 sm:w-4 sm:h-4" /> : <Bot className="w-3 h-3 sm:w-4 sm:h-4" />}
                    </div>
                    <div className={`px-3 py-2 sm:px-4 sm:py-3 rounded-xl sm:rounded-2xl ${message.type === 'user'
                      ? 'bg-blue-500 dark:bg-blue-600 text-white'
                      : 'bg-gray-100 dark:bg-gray-700 text-gray-800 dark:text-gray-100'
                      }`}>
                      <p className="whitespace-pre-wrap text-sm sm:text-base">{message.content}</p>
                      {isClient && (
                        <p className="text-xs opacity-70 mt-1">
                          {formatTimestamp(message.timestamp)}
                        </p>
                      )}
                    </div>
                  </div>
                </div>

                {/* Thinking Steps */}
                {message.thinkingSteps && message.thinkingSteps.length > 0 && (
                  <div className="ml-6 sm:ml-11 bg-gradient-to-r from-purple-50 to-blue-50 dark:from-purple-900/20 dark:to-blue-900/20 rounded-xl p-3 sm:p-4 border border-purple-200 dark:border-purple-700">
                    <div
                      className="flex items-center justify-between mb-2 sm:mb-3 cursor-pointer hover:bg-purple-100 dark:hover:bg-purple-800/30 rounded-lg p-2 -m-2 transition-colors"
                      onClick={() => toggleThinkingSteps(message.id)}
                    >
                      <div className="flex items-center">
                        <Brain className="w-4 h-4 sm:w-5 sm:h-5 text-purple-600 dark:text-purple-400 mr-1 sm:mr-2" />
                        <h3 className="font-semibold text-gray-800 dark:text-gray-200 text-sm sm:text-base">Agent Thinking Process</h3>
                        <span className="ml-1 sm:ml-2 text-xs sm:text-sm text-gray-500 dark:text-gray-400">({message.thinkingSteps.length} steps)</span>
                      </div>
                      {expandedThinkingSteps[message.id] ? (
                        <ChevronDown className="w-4 h-4 sm:w-5 sm:h-5 text-gray-500 dark:text-gray-400" />
                      ) : (
                        <ChevronRight className="w-4 h-4 sm:w-5 sm:h-5 text-gray-500 dark:text-gray-400" />
                      )}
                    </div>
                    {expandedThinkingSteps[message.id] && (
                      <div className="space-y-2 sm:space-y-3">
                        {message.thinkingSteps.map((step, index) => (
                          <div key={index} className="bg-white dark:bg-gray-800 rounded-lg p-2 sm:p-3 border border-gray-200 dark:border-gray-600 shadow-sm">
                            <div className="flex items-center justify-between mb-1 sm:mb-2">
                              <div className="flex items-center space-x-1 sm:space-x-2">
                                {agentIcons[step.agent] || <Brain className="w-3 h-3 sm:w-4 sm:h-4" />}
                                <span className="font-medium text-xs sm:text-sm text-gray-700 dark:text-gray-300">{step.agent}</span>
                                <span className="text-xs text-gray-500 dark:text-gray-400">{step.step}</span>
                              </div>
                              <div className={`flex items-center space-x-1 px-1 sm:px-2 py-1 rounded-full text-xs font-medium ${getStatusColor(step.status)}`}>
                                {getStatusIcon(step.status)}
                                <span className="capitalize hidden sm:inline">{step.status}</span>
                              </div>
                            </div>
                            <p className="text-xs sm:text-sm text-gray-600 dark:text-gray-300 mb-1 sm:mb-2">{step.message}</p>
                            {step.details && (
                              <details className="text-xs text-gray-500 dark:text-gray-400">
                                <summary className="cursor-pointer hover:text-gray-700 dark:hover:text-gray-300">Show Details</summary>
                                <pre className="mt-2 p-2 bg-gray-50 dark:bg-gray-700 rounded text-xs overflow-x-auto">
                                  {JSON.stringify(step.details, null, 2)}
                                </pre>
                              </details>
                            )}
                          </div>
                        ))}
                      </div>
                    )}
                  </div>
                )}

                {/* Pipeline Info */}
                {message.pipelineInfo && (
                  <div className="ml-6 sm:ml-11">
                    <div className="inline-flex items-center px-2 sm:px-3 py-1 bg-blue-100 dark:bg-blue-900/30 text-blue-800 dark:text-blue-300 text-xs sm:text-sm rounded-full">
                      <Brain className="w-3 h-3 sm:w-4 sm:h-4 mr-1" />
                      {message.pipelineInfo}
                    </div>
                  </div>
                )}

                {/* Sources */}
                {message.sources && message.sources.length > 0 && (
                  <div className="ml-6 sm:ml-11 mt-2 sm:mt-3">
                    <div className="bg-gradient-to-r from-green-50 to-emerald-50 dark:from-green-900/20 dark:to-emerald-900/20 rounded-xl p-3 sm:p-4 border border-green-200 dark:border-green-700">
                      <div className="flex items-center mb-2 sm:mb-3">
                        <FileText className="w-4 h-4 sm:w-5 sm:h-5 text-green-600 dark:text-green-400 mr-1 sm:mr-2" />
                        <h3 className="font-semibold text-gray-800 dark:text-gray-200 text-sm sm:text-base">Sources ({message.sources.length})</h3>
                      </div>
                      <div className="space-y-2">
                        {message.sources.map((source, index) => (
                          <div key={index} className="bg-white dark:bg-gray-800 rounded-lg p-2 sm:p-3 border border-gray-200 dark:border-gray-600 shadow-sm">
                            <div className="flex items-start justify-between">
                              <div className="flex-1">
                                <p className="text-xs sm:text-sm text-gray-700 dark:text-gray-300 mb-1 sm:mb-2">
                                  {source.content ? source.content.substring(0, 150) + (source.content.length > 150 ? '...' : '') : 'No content available'}
                                </p>
                                {source.metadata && (
                                  <div className="text-xs text-gray-500 dark:text-gray-400 flex flex-wrap gap-1">
                                    {source.metadata.source && (
                                      <span className="inline-block bg-gray-100 dark:bg-gray-700 px-1 sm:px-2 py-1 rounded text-xs">
                                        Source: {source.metadata.source}
                                      </span>
                                    )}
                                    {source.metadata.page && (
                                      <span className="inline-block bg-gray-100 dark:bg-gray-700 px-1 sm:px-2 py-1 rounded text-xs">
                                        Page: {source.metadata.page}
                                      </span>
                                    )}
                                    {source.metadata.chunk_id && (
                                      <span className="inline-block bg-gray-100 dark:bg-gray-700 px-1 sm:px-2 py-1 rounded text-xs">
                                        Chunk: {source.metadata.chunk_id}
                                      </span>
                                    )}
                                  </div>
                                )}
                              </div>
                            </div>
                          </div>
                        ))}
                      </div>
                    </div>
                  </div>
                )}
              </div>
            ))}

            {/* Loading Indicator */}
            {isLoading && (
              <div className="flex justify-start">
                <div className="flex items-start space-x-2 sm:space-x-3">
                  <div className="flex-shrink-0 w-6 h-6 sm:w-8 sm:h-8 rounded-full bg-purple-500 dark:bg-purple-600 text-white flex items-center justify-center">
                    <Bot className="w-3 h-3 sm:w-4 sm:h-4" />
                  </div>
                  <div className="bg-gray-100 dark:bg-gray-700 px-3 py-2 sm:px-4 sm:py-3 rounded-xl sm:rounded-2xl">
                    <div className="flex items-center space-x-2">
                      <div className="animate-spin rounded-full h-3 w-3 sm:h-4 sm:w-4 border-b-2 border-purple-500"></div>
                      <span className="text-gray-600 dark:text-gray-300 text-sm sm:text-base">Thinking...</span>
                    </div>
                  </div>
                </div>
              </div>
            )}

            <div ref={messagesEndRef} />
          </div>

          {/* Input */}
          <div className="border-t border-gray-200 dark:border-gray-600 p-3 sm:p-4">
            <div className="flex space-x-2 sm:space-x-3">
              <input
                type="text"
                value={input}
                onChange={(e) => setInput(e.target.value)}
                onKeyPress={handleKeyPress}
                placeholder="Ask me anything..."
                className="flex-1 px-3 py-2 sm:px-4 sm:py-3 text-sm sm:text-base border border-gray-300 dark:border-gray-600 rounded-lg bg-white dark:bg-gray-800 text-gray-900 dark:text-gray-100 placeholder-gray-500 dark:placeholder-gray-400 focus:ring-2 focus:ring-purple-500 focus:border-transparent outline-none"
                disabled={isLoading}
              />
              <button
                onClick={sendMessage}
                disabled={isLoading || !input.trim()}
                className="px-3 py-2 sm:px-6 sm:py-3 bg-purple-600 dark:bg-purple-700 text-white rounded-lg hover:bg-purple-700 dark:hover:bg-purple-800 disabled:opacity-50 disabled:cursor-not-allowed transition-colors flex items-center space-x-1 sm:space-x-2"
              >
                <Send className="w-3 h-3 sm:w-4 sm:h-4" />
                <span className="hidden sm:inline">Send</span>
              </button>
            </div>
          </div>
        </div>
      </div>
    </div>
  );
}
</file>

<file path="src/lib/agents.ts">
import { GoogleGenerativeAI } from "@google/generative-ai";
import axios from 'axios';
import { getVectorStore } from './vectorstore';
import { generateTextCompletion, checkHealth as checkLiteLLMHealth } from './litellm-client';

// Configuration - Using Next.js public environment variables
const GOOGLE_API_KEY = process.env.NEXT_PUBLIC_GOOGLE_API_KEY;
const OLLAMA_HOST = process.env.NEXT_PUBLIC_OLLAMA_HOST || 'http://localhost:11434';
const GEMINI_MODEL = process.env.NEXT_PUBLIC_GEMINI_MODEL || 'gemini-1.5-flash';
const GEMINI_TEMPERATURE = parseFloat(process.env.NEXT_PUBLIC_GEMINI_TEMPERATURE || '0.7');
const GEMINI_MAX_TOKENS = parseInt(process.env.NEXT_PUBLIC_GEMINI_MAX_TOKENS || '2048');

// LiteLLM Configuration
const USE_LITELLM = process.env.NEXT_PUBLIC_USE_LITELLM === 'true';
const LITELLM_MODEL = process.env.NEXT_PUBLIC_LITELLM_MODEL || 'gemini-2.0-flash-lite';

// Initialize Google AI (fallback only)
const genAI = GOOGLE_API_KEY ? new GoogleGenerativeAI(GOOGLE_API_KEY) : null;

// Base Agent interface
export interface Agent {
  name: string;
  process(input: any): Promise<any>;
}

// Thinking step interface
export interface ThinkingStep {
  agent: string;
  step: string;
  status: 'processing' | 'completed' | 'error';
  message: string;
  details?: any;
}

// Query Agent
export class QueryAgent implements Agent {
  name = 'QueryAgent';

  async process(input: { query: string }): Promise<{ processedQuery: string; needsRetrieval: boolean; thinkingSteps: ThinkingStep[] }> {
    const thinkingSteps: ThinkingStep[] = [];
    
    try {
      thinkingSteps.push({
        agent: this.name,
        step: 'Query Analysis',
        status: 'processing',
        message: 'Analyzing query structure and intent...'
      });

      // Simple query analysis
      const processedQuery = input.query.trim();
      const needsRetrieval = this.shouldRetrieve(processedQuery);

      thinkingSteps.push({
        agent: this.name,
        step: 'Query Analysis',
        status: 'completed',
        message: `Query processed. Needs retrieval: ${needsRetrieval}`,
        details: { originalQuery: input.query, processedQuery, needsRetrieval }
      });

      return { processedQuery, needsRetrieval, thinkingSteps };
    } catch (error) {
      thinkingSteps.push({
        agent: this.name,
        step: 'Query Analysis',
        status: 'error',
        message: `Error processing query: ${error}`,
        details: { error: error instanceof Error ? error.message : 'Unknown error' }
      });
      throw error;
    }
  }

  private shouldRetrieve(query: string): boolean {
    // Simple heuristic - retrieve for questions and specific queries
    const questionWords = ['what', 'how', 'why', 'when', 'where', 'who', 'which'];
    const lowerQuery = query.toLowerCase();
    return questionWords.some(word => lowerQuery.includes(word)) || 
           query.length > 20 || 
           lowerQuery.includes('?');
  }
}

// Retrieval Agent
export class RetrievalAgent implements Agent {
  name = 'RetrievalAgent';

  async process(input: { query: string; k?: number }): Promise<{ documents: any[]; thinkingSteps: ThinkingStep[] }> {
    const thinkingSteps: ThinkingStep[] = [];
    
    try {
      thinkingSteps.push({
        agent: this.name,
        step: 'Vector Search',
        status: 'processing',
        message: 'Searching for relevant documents...'
      });

      const vectorStore = await getVectorStore();
      const documents = await vectorStore.similaritySearch(input.query, input.k || 5);

      thinkingSteps.push({
        agent: this.name,
        step: 'Vector Search',
        status: 'completed',
        message: `Found ${documents.length} relevant documents`,
        details: { query: input.query, documentCount: documents.length }
      });

      return { documents, thinkingSteps };
    } catch (error) {
      thinkingSteps.push({
        agent: this.name,
        step: 'Vector Search',
        status: 'error',
        message: `Error retrieving documents: ${error}`,
        details: { error: error instanceof Error ? error.message : 'Unknown error' }
      });
      throw error;
    }
  }
}

// Answer Agent
export class AnswerAgent implements Agent {
  name = 'AnswerAgent';

  async process(input: { query: string; documents: any[] }): Promise<{ answer: string; thinkingSteps: ThinkingStep[] }> {
    const thinkingSteps: ThinkingStep[] = [];
    
    try {
      thinkingSteps.push({
        agent: this.name,
        step: 'Response Generation',
        status: 'processing',
        message: 'Generating response using retrieved context...'
      });

      const context = input.documents.map(doc => doc.content).join('\n\n');
      const answer = await this.generateAnswer(input.query, context);

      thinkingSteps.push({
        agent: this.name,
        step: 'Response Generation',
        status: 'completed',
        message: 'Response generated successfully',
        details: { 
          query: input.query, 
          contextLength: context.length,
          documentCount: input.documents.length 
        }
      });

      return { answer, thinkingSteps };
    } catch (error) {
      thinkingSteps.push({
        agent: this.name,
        step: 'Response Generation',
        status: 'error',
        message: `Error generating answer: ${error}`,
        details: { error: error instanceof Error ? error.message : 'Unknown error' }
      });
      throw error;
    }
  }

  private async generateAnswer(query: string, context: string): Promise<string> {
    const prompt = `Based on the following context, please answer the question. If the context doesn't contain enough information to answer the question, please say so.

Context:
${context}

Question: ${query}

Answer:`;

    // Check if we're in a Vercel environment
    const isVercel = process.env.VERCEL === '1';

    // Try LiteLLM proxy first if enabled
    if (USE_LITELLM) {
      try {
        console.log("üöÄ Using LiteLLM proxy for answer generation...");
        const response = await generateTextCompletion(prompt, {
          model: LITELLM_MODEL,
          temperature: GEMINI_TEMPERATURE,
          maxTokens: GEMINI_MAX_TOKENS,
        });
        console.log("‚úÖ LiteLLM response received successfully");
        return response;
      } catch (error) {
        console.warn("‚ö†Ô∏è LiteLLM failed, falling back to Google Gemini:", error);
      }
    }

    // Fallback to Google Gemini
    if (genAI) {
      try {
        console.log("üöÄ Using Google Gemini for answer generation...");
        const model = genAI.getGenerativeModel({ 
          model: GEMINI_MODEL,
          generationConfig: {
            temperature: GEMINI_TEMPERATURE,
            maxOutputTokens: GEMINI_MAX_TOKENS,
          }
        });
        const result = await model.generateContent(prompt);
        const response = await result.response;
        return response.text();
      } catch (error) {
        console.warn("Google Gemini failed:", error);
        if (isVercel) {
          // On Vercel, we can't use Ollama, so return a fallback message
          return "I apologize, but I'm unable to generate a response at the moment. Please ensure your API keys are properly configured.";
        }
      }
    }

    // Fallback to Ollama (only for local development)
    if (!isVercel) {
      try {
        const response = await axios.post(`${OLLAMA_HOST}/api/generate`, {
          model: "gemma3:1b",
          prompt: prompt,
          stream: false
        });
        return response.data.response;
      } catch (error) {
        console.error("Ollama failed:", error);
      }
    }

    // Final fallback
    return "I apologize, but I'm unable to generate a response at the moment. Please try again later.";
  }
}

// Critic Agent
export class CriticAgent implements Agent {
  name = 'CriticAgent';

  async process(input: { query: string; answer: string; documents: any[] }): Promise<{ critique: string; score: number; thinkingSteps: ThinkingStep[] }> {
    const thinkingSteps: ThinkingStep[] = [];
    
    try {
      thinkingSteps.push({
        agent: this.name,
        step: 'Answer Evaluation',
        status: 'processing',
        message: 'Evaluating answer quality and relevance...'
      });

      const critique = this.evaluateAnswer(input.query, input.answer, input.documents);
      const score = this.calculateScore(input.query, input.answer, input.documents);

      thinkingSteps.push({
        agent: this.name,
        step: 'Answer Evaluation',
        status: 'completed',
        message: `Answer evaluated with score: ${score}/10`,
        details: { critique, score, query: input.query }
      });

      return { critique, score, thinkingSteps };
    } catch (error) {
      thinkingSteps.push({
        agent: this.name,
        step: 'Answer Evaluation',
        status: 'error',
        message: `Error evaluating answer: ${error}`,
        details: { error: error instanceof Error ? error.message : 'Unknown error' }
      });
      throw error;
    }
  }

  private evaluateAnswer(query: string, answer: string, documents: any[]): string {
    const critiques = [];
    
    if (answer.length < 50) {
      critiques.push("Answer is too brief");
    }
    
    if (answer.includes("I don't know") || answer.includes("I can't")) {
      critiques.push("Answer indicates uncertainty");
    }
    
    if (documents.length === 0) {
      critiques.push("No supporting documents found");
    }
    
    if (critiques.length === 0) {
      return "Answer appears comprehensive and well-supported";
    }
    
    return critiques.join("; ");
  }

  private calculateScore(query: string, answer: string, documents: any[]): number {
    let score = 5; // Base score
    
    if (answer.length > 100) score += 1;
    if (documents.length > 0) score += 2;
    if (!answer.includes("I don't know")) score += 1;
    if (answer.includes(query.toLowerCase())) score += 1;
    
    return Math.min(10, score);
  }
}

// Refine Agent
export class RefineAgent implements Agent {
  name = 'RefineAgent';

  async process(input: { query: string; answer: string; critique: string; documents: any[] }): Promise<{ refinedAnswer: string; thinkingSteps: ThinkingStep[] }> {
    const thinkingSteps: ThinkingStep[] = [];
    
    try {
      thinkingSteps.push({
        agent: this.name,
        step: 'Answer Refinement',
        status: 'processing',
        message: 'Refining answer based on critique...'
      });

      const refinedAnswer = await this.refineAnswer(input.query, input.answer, input.critique, input.documents);

      thinkingSteps.push({
        agent: this.name,
        step: 'Answer Refinement',
        status: 'completed',
        message: 'Answer refined successfully',
        details: { 
          originalLength: input.answer.length,
          refinedLength: refinedAnswer.length,
          critique: input.critique
        }
      });

      return { refinedAnswer, thinkingSteps };
    } catch (error) {
      thinkingSteps.push({
        agent: this.name,
        step: 'Answer Refinement',
        status: 'error',
        message: `Error refining answer: ${error}`,
        details: { error: error instanceof Error ? error.message : 'Unknown error' }
      });
      throw error;
    }
  }

  private async refineAnswer(query: string, answer: string, critique: string, documents: any[]): Promise<string> {
    const refinementPrompt = `Please refine the following answer based on the critique provided. Make it more comprehensive and accurate.

Original Query: ${query}
Original Answer: ${answer}
Critique: ${critique}
Supporting Documents: ${documents.map(doc => doc.content).join('\n\n')}

Refined Answer:`;

    // Check if we're in a Vercel environment
    const isVercel = process.env.VERCEL === '1';

    // Try LiteLLM proxy first if enabled
    if (USE_LITELLM) {
      try {
        console.log("üöÄ Using LiteLLM proxy for answer refinement...");
        const response = await generateTextCompletion(refinementPrompt, {
          model: LITELLM_MODEL,
          temperature: GEMINI_TEMPERATURE,
          maxTokens: GEMINI_MAX_TOKENS,
        });
        console.log("‚úÖ LiteLLM refinement response received successfully");
        return response;
      } catch (error) {
        console.warn("‚ö†Ô∏è LiteLLM failed for refinement, falling back to Google Gemini:", error);
      }
    }

    // Fallback to Google Gemini
    if (genAI) {
      try {
        console.log("üöÄ Using Google Gemini for answer refinement...");
        const model = genAI.getGenerativeModel({ 
          model: GEMINI_MODEL,
          generationConfig: {
            temperature: GEMINI_TEMPERATURE,
            maxOutputTokens: GEMINI_MAX_TOKENS,
          }
        });
        const result = await model.generateContent(refinementPrompt);
        const response = await result.response;
        return response.text();
      } catch (error) {
        console.warn("Google Gemini failed for refinement:", error);
        if (isVercel) {
          // On Vercel, we can't use Ollama, so return original answer
          return answer;
        }
      }
    }

    // Fallback to Ollama (only for local development)
    if (!isVercel) {
      try {
        const response = await axios.post(`${OLLAMA_HOST}/api/generate`, {
          model: "gemma3:1b",
          prompt: refinementPrompt,
          stream: false
        });
        return response.data.response;
      } catch (error) {
        console.error("Ollama failed for refinement:", error);
      }
    }

    // Return original answer if refinement fails
    return answer;
  }
}
</file>

<file path="src/lib/litellm-client.ts">
import axios, { AxiosError } from 'axios';

// Configuration - Using Next.js public environment variables
const LITELLM_API_URL = process.env.NEXT_PUBLIC_LITELLM_API_URL || 'http://localhost:8000';
const LITELLM_MODEL = process.env.NEXT_PUBLIC_LITELLM_MODEL || 'gemini-2.0-flash-lite';
const LITELLM_TEMPERATURE = parseFloat(process.env.NEXT_PUBLIC_LITELLM_TEMPERATURE || '0.7');
const LITELLM_MAX_TOKENS = parseInt(process.env.NEXT_PUBLIC_LITELLM_MAX_TOKENS || '2048');

// Message interface
export interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

// Response interface
export interface ChatCompletionResponse {
  id: string;
  object: string;
  created: number;
  model: string;
  choices: Array<{
    index: number;
    message: {
      role: string;
      content: string;
    };
    finish_reason: string;
  }>;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

// Models list response interface
export interface ModelsListResponse {
  object: string;
  data: Array<{
    id: string;
    object: string;
    created: number;
    owned_by: string;
  }>;
}

/**
 * Generate a chat completion using the LiteLLM proxy
 */
export async function generateChatCompletion(
  messages: ChatMessage[],
  options?: {
    model?: string;
    temperature?: number;
    maxTokens?: number;
  }
): Promise<string> {
  try {
    const model = options?.model || LITELLM_MODEL;
    const temperature = options?.temperature ?? LITELLM_TEMPERATURE;
    const maxTokens = options?.maxTokens || LITELLM_MAX_TOKENS;

    console.log(`üöÄ LiteLLM: Generating completion with model: ${model}`);
    console.log(`üìä LiteLLM: Temperature: ${temperature}, Max tokens: ${maxTokens}`);
    console.log(`üìù LiteLLM: Messages count: ${messages.length}`);

    const response = await axios.post<ChatCompletionResponse>(
      `${LITELLM_API_URL}/v1/chat/completions`,
      {
        model,
        messages,
        temperature,
        max_tokens: maxTokens,
        stream: false,
      },
      {
        headers: {
          'Content-Type': 'application/json',
        },
        timeout: 60000, // 60 second timeout
      }
    );

    const content = response.data.choices[0]?.message?.content || '';
    
    console.log(`‚úÖ LiteLLM: Successfully generated response (${content.length} chars)`);
    console.log(`üìä LiteLLM: Token usage - Prompt: ${response.data.usage.prompt_tokens}, Completion: ${response.data.usage.completion_tokens}`);

    return content;
  } catch (error) {
    if (axios.isAxiosError(error)) {
      const axiosError = error as AxiosError;
      console.error('‚ùå LiteLLM API Error:', {
        status: axiosError.response?.status,
        statusText: axiosError.response?.statusText,
        data: axiosError.response?.data,
        message: axiosError.message,
      });
      
      // Provide more helpful error messages
      if (axiosError.code === 'ECONNREFUSED') {
        throw new Error(
          `LiteLLM proxy server is not running. Please start it with: docker-compose up -d fastapi-litellm`
        );
      }
      
      if (axiosError.response?.status === 500) {
        const errorData = axiosError.response.data as any;
        throw new Error(
          `LiteLLM proxy error: ${errorData?.detail || 'Internal server error'}. Check if ONEMINAI_API_KEY is configured.`
        );
      }
      
      throw new Error(
        `LiteLLM request failed: ${axiosError.message}. Status: ${axiosError.response?.status || 'N/A'}`
      );
    }
    
    console.error('‚ùå Unexpected error in LiteLLM client:', error);
    throw error;
  }
}

/**
 * Generate a simple text completion from a prompt
 */
export async function generateTextCompletion(
  prompt: string,
  options?: {
    model?: string;
    temperature?: number;
    maxTokens?: number;
    systemPrompt?: string;
  }
): Promise<string> {
  const messages: ChatMessage[] = [];
  
  // Add system prompt if provided
  if (options?.systemPrompt) {
    messages.push({
      role: 'system',
      content: options.systemPrompt,
    });
  }
  
  // Add user prompt
  messages.push({
    role: 'user',
    content: prompt,
  });
  
  return generateChatCompletion(messages, options);
}

/**
 * Check if the LiteLLM proxy is healthy
 */
export async function checkHealth(): Promise<boolean> {
  try {
    console.log('üè• LiteLLM: Checking health...');
    const response = await axios.get(`${LITELLM_API_URL}/health`, {
      timeout: 5000,
    });
    
    const isHealthy = response.status === 200 && response.data.status === 'healthy';
    console.log(`${isHealthy ? '‚úÖ' : '‚ùå'} LiteLLM: Health check ${isHealthy ? 'passed' : 'failed'}`);
    
    return isHealthy;
  } catch (error) {
    console.error('‚ùå LiteLLM: Health check failed:', error);
    return false;
  }
}

/**
 * Get list of available models from the LiteLLM proxy
 */
export async function getAvailableModels(): Promise<string[]> {
  try {
    console.log('üìã LiteLLM: Fetching available models...');
    const response = await axios.get<ModelsListResponse>(`${LITELLM_API_URL}/v1/models`, {
      timeout: 5000,
    });
    
    const models = response.data.data.map((model) => model.id);
    console.log(`‚úÖ LiteLLM: Found ${models.length} models:`, models);
    
    return models;
  } catch (error) {
    console.error('‚ùå LiteLLM: Failed to fetch models:', error);
    return [LITELLM_MODEL]; // Return default model as fallback
  }
}

/**
 * Generate embeddings using the proxy (if supported in future)
 * Currently returns placeholder - implement when 1minAI adds embedding support
 */
export async function generateEmbedding(text: string): Promise<number[]> {
  // For now, this is a placeholder since 1minAI may not support embeddings directly
  // You would still use Google Gemini embeddings for this
  throw new Error('Embeddings are not supported through LiteLLM proxy. Use Google Gemini embeddings instead.');
}

/**
 * Get the current configuration
 */
export function getLiteLLMConfig() {
  return {
    apiUrl: LITELLM_API_URL,
    model: LITELLM_MODEL,
    temperature: LITELLM_TEMPERATURE,
    maxTokens: LITELLM_MAX_TOKENS,
  };
}

/**
 * Test the LiteLLM connection with a simple query
 */
export async function testConnection(): Promise<{
  success: boolean;
  message: string;
  models?: string[];
}> {
  try {
    // Check health
    const isHealthy = await checkHealth();
    if (!isHealthy) {
      return {
        success: false,
        message: 'LiteLLM proxy is not healthy or not reachable',
      };
    }

    // Get models
    const models = await getAvailableModels();

    // Test a simple completion
    const testResponse = await generateTextCompletion('Say "Hello from LiteLLM!"', {
      maxTokens: 50,
    });

    return {
      success: true,
      message: `LiteLLM connection successful! Response: ${testResponse}`,
      models,
    };
  } catch (error) {
    return {
      success: false,
      message: `LiteLLM connection failed: ${error instanceof Error ? error.message : 'Unknown error'}`,
    };
  }
}
</file>

<file path="src/lib/pipelines.ts">
import { QueryAgent, RetrievalAgent, AnswerAgent, CriticAgent, RefineAgent, ThinkingStep } from './agents';
import { getVectorStore } from './vectorstore';

// Pipeline interface
export interface Pipeline {
  name: string;
  process(query: string): Promise<{
    answer: string;
    thinkingSteps: ThinkingStep[];
    pipelineInfo: string;
    sources?: any[];
  }>;
}

// Phase 1: Basic A2A Pipeline
export class Phase1Pipeline implements Pipeline {
  name = 'Phase 1: Basic A2A';

  async process(query: string): Promise<{
    answer: string;
    thinkingSteps: ThinkingStep[];
    pipelineInfo: string;
    sources?: any[];
  }> {
    const allThinkingSteps: ThinkingStep[] = [];
    
    try {
      // Step 1: Query Processing
      const queryAgent = new QueryAgent();
      const queryResult = await queryAgent.process({ query });
      allThinkingSteps.push(...queryResult.thinkingSteps);

      let answer = "I'm not sure how to answer that.";
      let documents: any[] = [];

      // Step 2: Retrieval (if needed)
      if (queryResult.needsRetrieval) {
        const retrievalAgent = new RetrievalAgent();
        const retrievalResult = await retrievalAgent.process({ 
          query: queryResult.processedQuery, 
          k: 3 
        });
        allThinkingSteps.push(...retrievalResult.thinkingSteps);
        documents = retrievalResult.documents;
      }

      // Step 3: Answer Generation
      const answerAgent = new AnswerAgent();
      const answerResult = await answerAgent.process({ 
        query: queryResult.processedQuery, 
        documents 
      });
      allThinkingSteps.push(...answerResult.thinkingSteps);
      answer = answerResult.answer;

      return {
        answer,
        thinkingSteps: allThinkingSteps,
        pipelineInfo: this.name,
        sources: documents
      };
    } catch (error) {
      allThinkingSteps.push({
        agent: 'Pipeline',
        step: 'Error Handling',
        status: 'error',
        message: `Pipeline error: ${error}`,
        details: { error: error instanceof Error ? error.message : 'Unknown error' }
      });
      
      return {
        answer: "I encountered an error while processing your request. Please try again.",
        thinkingSteps: allThinkingSteps,
        pipelineInfo: this.name,
        sources: []
      };
    }
  }
}

// Phase 2: Smart A2A Pipeline
export class Phase2Pipeline implements Pipeline {
  name = 'Phase 2: Smart A2A';

  async process(query: string): Promise<{
    answer: string;
    thinkingSteps: ThinkingStep[];
    pipelineInfo: string;
    sources?: any[];
  }> {
    const allThinkingSteps: ThinkingStep[] = [];
    
    try {
      // Step 1: Query Processing
      const queryAgent = new QueryAgent();
      const queryResult = await queryAgent.process({ query });
      allThinkingSteps.push(...queryResult.thinkingSteps);

      let answer = "I'm not sure how to answer that.";
      let documents: any[] = [];

      // Step 2: Retrieval (if needed)
      if (queryResult.needsRetrieval) {
        const retrievalAgent = new RetrievalAgent();
        const retrievalResult = await retrievalAgent.process({ 
          query: queryResult.processedQuery, 
          k: 5 
        });
        allThinkingSteps.push(...retrievalResult.thinkingSteps);
        documents = retrievalResult.documents;
      }

      // Step 3: Answer Generation
      const answerAgent = new AnswerAgent();
      const answerResult = await answerAgent.process({ 
        query: queryResult.processedQuery, 
        documents 
      });
      allThinkingSteps.push(...answerResult.thinkingSteps);
      answer = answerResult.answer;

      // Step 4: Answer Evaluation
      const criticAgent = new CriticAgent();
      const critiqueResult = await criticAgent.process({ 
        query: queryResult.processedQuery, 
        answer, 
        documents 
      });
      allThinkingSteps.push(...critiqueResult.thinkingSteps);

      // If score is low, try to improve
      if (critiqueResult.score < 7) {
        const refineAgent = new RefineAgent();
        const refineResult = await refineAgent.process({ 
          query: queryResult.processedQuery, 
          answer, 
          critique: critiqueResult.critique, 
          documents 
        });
        allThinkingSteps.push(...refineResult.thinkingSteps);
        answer = refineResult.refinedAnswer;
      }

      return {
        answer,
        thinkingSteps: allThinkingSteps,
        pipelineInfo: this.name,
        sources: documents
      };
    } catch (error) {
      allThinkingSteps.push({
        agent: 'Pipeline',
        step: 'Error Handling',
        status: 'error',
        message: `Pipeline error: ${error}`,
        details: { error: error instanceof Error ? error.message : 'Unknown error' }
      });
      
      return {
        answer: "I encountered an error while processing your request. Please try again.",
        thinkingSteps: allThinkingSteps,
        pipelineInfo: this.name,
        sources: []
      };
    }
  }
}

// Phase 3: Self-Refinement Pipeline
export class Phase3Pipeline implements Pipeline {
  name = 'Phase 3: Self-Refinement';

  async process(query: string): Promise<{
    answer: string;
    thinkingSteps: ThinkingStep[];
    pipelineInfo: string;
    sources?: any[];
  }> {
    const allThinkingSteps: ThinkingStep[] = [];
    
    try {
      // Step 1: Query Processing
      const queryAgent = new QueryAgent();
      const queryResult = await queryAgent.process({ query });
      allThinkingSteps.push(...queryResult.thinkingSteps);

      let answer = "I'm not sure how to answer that.";
      let documents: any[] = [];

      // Step 2: Retrieval (if needed)
      if (queryResult.needsRetrieval) {
        const retrievalAgent = new RetrievalAgent();
        const retrievalResult = await retrievalAgent.process({ 
          query: queryResult.processedQuery, 
          k: 7 
        });
        allThinkingSteps.push(...retrievalResult.thinkingSteps);
        documents = retrievalResult.documents;
      }

      // Step 3: Answer Generation
      const answerAgent = new AnswerAgent();
      const answerResult = await answerAgent.process({ 
        query: queryResult.processedQuery, 
        documents 
      });
      allThinkingSteps.push(...answerResult.thinkingSteps);
      answer = answerResult.answer;

      // Step 4: Iterative Refinement (up to 3 iterations)
      let currentAnswer = answer;
      let iteration = 0;
      const maxIterations = 3;

      while (iteration < maxIterations) {
        const criticAgent = new CriticAgent();
        const critiqueResult = await criticAgent.process({ 
          query: queryResult.processedQuery, 
          answer: currentAnswer, 
          documents 
        });
        allThinkingSteps.push(...critiqueResult.thinkingSteps);

        if (critiqueResult.score >= 8) {
          break; // Good enough score, stop refining
        }

        const refineAgent = new RefineAgent();
        const refineResult = await refineAgent.process({ 
          query: queryResult.processedQuery, 
          answer: currentAnswer, 
          critique: critiqueResult.critique, 
          documents 
        });
        allThinkingSteps.push(...refineResult.thinkingSteps);
        currentAnswer = refineResult.refinedAnswer;
        iteration++;
      }

      answer = currentAnswer;

      return {
        answer,
        thinkingSteps: allThinkingSteps,
        pipelineInfo: this.name,
        sources: documents
      };
    } catch (error) {
      allThinkingSteps.push({
        agent: 'Pipeline',
        step: 'Error Handling',
        status: 'error',
        message: `Pipeline error: ${error}`,
        details: { error: error instanceof Error ? error.message : 'Unknown error' }
      });
      
      return {
        answer: "I encountered an error while processing your request. Please try again.",
        thinkingSteps: allThinkingSteps,
        pipelineInfo: this.name,
        sources: []
      };
    }
  }
}

// Auto Pipeline (AI selects optimal)
export class AutoPipeline implements Pipeline {
  name = 'AUTO: AI Selects Optimal';

  async process(query: string): Promise<{
    answer: string;
    thinkingSteps: ThinkingStep[];
    pipelineInfo: string;
    sources?: any[];
  }> {
    const allThinkingSteps: ThinkingStep[] = [];
    
    try {
      // Analyze query complexity to select pipeline
      const queryComplexity = this.analyzeQueryComplexity(query);
      
      allThinkingSteps.push({
        agent: 'Pipeline',
        step: 'Pipeline Selection',
        status: 'processing',
        message: `Analyzing query complexity: ${queryComplexity}`,
        details: { query, complexity: queryComplexity }
      });

      let selectedPipeline: Pipeline;
      
      if (queryComplexity === 'simple') {
        selectedPipeline = new Phase1Pipeline();
      } else if (queryComplexity === 'medium') {
        selectedPipeline = new Phase2Pipeline();
      } else {
        selectedPipeline = new Phase3Pipeline();
      }

      allThinkingSteps.push({
        agent: 'Pipeline',
        step: 'Pipeline Selection',
        status: 'completed',
        message: `Selected pipeline: ${selectedPipeline.name}`,
        details: { selectedPipeline: selectedPipeline.name }
      });

      // Add a step showing we're executing the selected pipeline
      allThinkingSteps.push({
        agent: 'Pipeline',
        step: 'Pipeline Execution',
        status: 'processing',
        message: `Executing ${selectedPipeline.name}...`,
        details: { selectedPipeline: selectedPipeline.name }
      });

      // Execute selected pipeline
      const result = await selectedPipeline.process(query);
      allThinkingSteps.push(...result.thinkingSteps);

      // Mark pipeline execution as completed
      allThinkingSteps.push({
        agent: 'Pipeline',
        step: 'Pipeline Execution',
        status: 'completed',
        message: `${selectedPipeline.name} completed successfully`,
        details: { 
          selectedPipeline: selectedPipeline.name,
          answerLength: result.answer.length,
          sourcesCount: result.sources?.length || 0
        }
      });

      return {
        answer: result.answer,
        thinkingSteps: allThinkingSteps,
        pipelineInfo: `${this.name} ‚Üí ${selectedPipeline.name}`,
        sources: result.sources || []
      };
    } catch (error) {
      allThinkingSteps.push({
        agent: 'Pipeline',
        step: 'Error Handling',
        status: 'error',
        message: `Pipeline error: ${error}`,
        details: { error: error instanceof Error ? error.message : 'Unknown error' }
      });
      
      return {
        answer: "I encountered an error while processing your request. Please try again.",
        thinkingSteps: allThinkingSteps,
        pipelineInfo: this.name,
        sources: []
      };
    }
  }

  private analyzeQueryComplexity(query: string): 'simple' | 'medium' | 'complex' {
    const lowerQuery = query.toLowerCase();
    
    // Simple queries
    if (query.length < 30 && !lowerQuery.includes('?') && !lowerQuery.includes('explain')) {
      return 'simple';
    }
    
    // Complex queries
    if (query.length > 100 || 
        lowerQuery.includes('compare') || 
        lowerQuery.includes('analyze') || 
        lowerQuery.includes('explain') ||
        lowerQuery.includes('multiple') ||
        lowerQuery.split(' ').length > 15) {
      return 'complex';
    }
    
    return 'medium';
  }
}

// Meta Pipeline (Intelligent Selection)
export class MetaPipeline implements Pipeline {
  name = 'META: Intelligent Selection';

  async process(query: string): Promise<{
    answer: string;
    thinkingSteps: ThinkingStep[];
    pipelineInfo: string;
    sources?: any[];
  }> {
    const allThinkingSteps: ThinkingStep[] = [];
    
    try {
      // Use AI to analyze query and select best approach
      const analysis = await this.analyzeQuery(query);
      
      allThinkingSteps.push({
        agent: 'Pipeline',
        step: 'Meta Analysis',
        status: 'processing',
        message: 'Performing intelligent query analysis...',
        details: { query, analysis }
      });

      let selectedPipeline: Pipeline;
      
      if (analysis.recommendedPipeline === 'phase1') {
        selectedPipeline = new Phase1Pipeline();
      } else if (analysis.recommendedPipeline === 'phase2') {
        selectedPipeline = new Phase2Pipeline();
      } else {
        selectedPipeline = new Phase3Pipeline();
      }

      allThinkingSteps.push({
        agent: 'Pipeline',
        step: 'Meta Analysis',
        status: 'completed',
        message: `Intelligent selection: ${selectedPipeline.name}`,
        details: { 
          analysis, 
          selectedPipeline: selectedPipeline.name,
          confidence: analysis.confidence 
        }
      });

      // Add a step showing we're executing the selected pipeline
      allThinkingSteps.push({
        agent: 'Pipeline',
        step: 'Pipeline Execution',
        status: 'processing',
        message: `Executing ${selectedPipeline.name}...`,
        details: { selectedPipeline: selectedPipeline.name }
      });

      // Execute selected pipeline
      const result = await selectedPipeline.process(query);
      allThinkingSteps.push(...result.thinkingSteps);

      // Mark pipeline execution as completed
      allThinkingSteps.push({
        agent: 'Pipeline',
        step: 'Pipeline Execution',
        status: 'completed',
        message: `${selectedPipeline.name} completed successfully`,
        details: { 
          selectedPipeline: selectedPipeline.name,
          answerLength: result.answer.length,
          sourcesCount: result.sources?.length || 0
        }
      });

      return {
        answer: result.answer,
        thinkingSteps: allThinkingSteps,
        pipelineInfo: `${this.name} ‚Üí ${selectedPipeline.name} (${analysis.confidence}% confidence)`,
        sources: result.sources || []
      };
    } catch (error) {
      allThinkingSteps.push({
        agent: 'Pipeline',
        step: 'Error Handling',
        status: 'error',
        message: `Pipeline error: ${error}`,
        details: { error: error instanceof Error ? error.message : 'Unknown error' }
      });
      
      return {
        answer: "I encountered an error while processing your request. Please try again.",
        thinkingSteps: allThinkingSteps,
        pipelineInfo: this.name,
        sources: []
      };
    }
  }

  private async analyzeQuery(query: string): Promise<{
    recommendedPipeline: 'phase1' | 'phase2' | 'phase3';
    confidence: number;
    reasoning: string;
  }> {
    // Simple heuristic analysis (in a real implementation, this could use AI)
    const lowerQuery = query.toLowerCase();
    
    if (query.length < 50 && !lowerQuery.includes('explain') && !lowerQuery.includes('analyze')) {
      return {
        recommendedPipeline: 'phase1',
        confidence: 85,
        reasoning: 'Simple query, basic pipeline sufficient'
      };
    }
    
    if (lowerQuery.includes('compare') || lowerQuery.includes('analyze') || query.length > 100) {
      return {
        recommendedPipeline: 'phase3',
        confidence: 90,
        reasoning: 'Complex query requiring self-refinement'
      };
    }
    
    return {
      recommendedPipeline: 'phase2',
      confidence: 80,
      reasoning: 'Medium complexity query, smart pipeline recommended'
    };
  }
}

// Pipeline factory
export function createPipeline(mode: string): Pipeline {
  switch (mode) {
    case 'phase1':
      return new Phase1Pipeline();
    case 'phase2':
      return new Phase2Pipeline();
    case 'phase3':
      return new Phase3Pipeline();
    case 'auto':
      return new AutoPipeline();
    case 'meta':
      return new MetaPipeline();
    default:
      return new MetaPipeline(); // Default to meta
  }
}
</file>

<file path="src/lib/vectorstore.ts">
import { ChromaClient } from 'chromadb';
import { QdrantClient } from '@qdrant/js-client-rest';
import { GoogleGenerativeAI } from '@google/generative-ai';

// Configuration - Using Next.js public environment variables
const VECTOR_STORE_TYPE = process.env.NEXT_PUBLIC_VECTOR_STORE || 'qdrant';
const CHROMA_PATH = process.env.NEXT_PUBLIC_CHROMA_PATH || './chroma_db';
const QDRANT_HOST = process.env.NEXT_PUBLIC_QDRANT_HOST || 'localhost';
const QDRANT_PORT = parseInt(process.env.NEXT_PUBLIC_QDRANT_PORT || '6333');
const QDRANT_CLOUD_URL = process.env.NEXT_PUBLIC_QDRANT_CLOUD_URL;
const QDRANT_CLOUD_API_KEY = process.env.NEXT_PUBLIC_QDRANT_CLOUD_API_KEY;
const COLLECTION_NAME = process.env.NEXT_PUBLIC_COLLECTION_NAME || 'rag_a2a_collection';
const GOOGLE_API_KEY = process.env.NEXT_PUBLIC_GOOGLE_API_KEY;
const EMBEDDING_MODEL = process.env.NEXT_PUBLIC_EMBEDDING_MODEL || 'embedding-001';
const EMBEDDING_DIM = parseInt(process.env.NEXT_PUBLIC_EMBEDDING_DIM || '3072');

// Embeddings function - Gemini embedding-001 (3072 dims)
export async function getEmbedding(text: string): Promise<number[]> {
  try {
    if (!GOOGLE_API_KEY) {
      throw new Error("Google API key not found for embeddings");
    }
    return await getGoogleEmbedding(text);
  } catch (error) {
    console.error("‚ùå Embedding failed:", error);
    throw error;
  }
}

// Google Gemini Embeddings function
async function getGoogleEmbedding(text: string): Promise<number[]> {
  try {
    if (!GOOGLE_API_KEY) {
      throw new Error("Google API key not found");
    }

    const genAI = new GoogleGenerativeAI(GOOGLE_API_KEY);
    const model = genAI.getGenerativeModel({ model: EMBEDDING_MODEL });
    
    const result = await model.embedContent(text);
    const embedding = result.embedding.values;
    
    console.log(`‚úÖ Generated Google ${EMBEDDING_MODEL} (${embedding.length} dimensions)`);
    if (embedding.length !== EMBEDDING_DIM) {
      console.warn(`‚ö†Ô∏è Embedding dimension (${embedding.length}) does not match configured EMBEDDING_DIM (${EMBEDDING_DIM}).`);
    }
    return embedding;
  } catch (error) {
    console.error("‚ùå Google embeddings failed:", error);
    throw error;
  }
}

// No local fallback supported to enforce strict 3072-d embeddings

// Vector store interface
export interface VectorStore {
  init(): Promise<void>;
  addDocuments(documents: Array<{ content: string; metadata?: any }>): Promise<void>;
  similaritySearch(query: string, k: number): Promise<Array<{ content: string; metadata: any; distance: number }>>;
  deleteCollection(): Promise<void>;
}

// Chroma implementation
class ChromaVectorStore implements VectorStore {
  private client: ChromaClient;
  private collection: any;

  constructor() {
    this.client = new ChromaClient({ path: process.cwd() + '/' + CHROMA_PATH });
  }

  async init(): Promise<void> {
    try {
      this.collection = await this.client.getOrCreateCollection({
        name: COLLECTION_NAME,
        metadata: { "hnsw:space": "cosine" }
      });
      console.log("‚úÖ Chroma collection initialized:", COLLECTION_NAME);
    } catch (err) {
      console.error("‚ùå Chroma init error:", err);
      throw err;
    }
  }

  async addDocuments(documents: Array<{ content: string; metadata?: any }>): Promise<void> {
    try {
      const embeddings = await Promise.all(
        documents.map(doc => getEmbedding(doc.content))
      );
      // For Qdrant, validate vector size against collection's configured size
      // Note: Chroma path does not require this validation
      
      const ids = documents.map((_, i) => `doc_${Date.now()}_${i}`);
      const metadatas = documents.map(doc => doc.metadata || {});
      
      await this.collection.add({
        ids,
        embeddings,
        documents: documents.map(doc => doc.content),
        metadatas
      });
      
      console.log(`‚úÖ Added ${documents.length} documents to Chroma`);
    } catch (err) {
      console.error("‚ùå Chroma add documents error:", err);
      throw err;
    }
  }

  async similaritySearch(query: string, k: number): Promise<Array<{ content: string; metadata: any; distance: number }>> {
    try {
      const queryEmbedding = await getEmbedding(query);
      const results = await this.collection.query({
        queryEmbeddings: [queryEmbedding],
        nResults: k
      });
      
      return results.documents[0].map((doc: string, i: number) => ({
        content: doc,
        metadata: results.metadatas[0][i] || {},
        distance: results.distances[0][i]
      }));
    } catch (err) {
      console.error("‚ùå Chroma similarity search error:", err);
      throw err;
    }
  }

  async deleteCollection(): Promise<void> {
    try {
      await this.client.deleteCollection({ name: COLLECTION_NAME });
      console.log("‚úÖ Chroma collection deleted");
    } catch (err) {
      console.error("‚ùå Chroma delete error:", err);
      throw err;
    }
  }
}

// Qdrant implementation
class QdrantVectorStore implements VectorStore {
  private client: QdrantClient;
  private collectionName: string;

  constructor() {
    // Debug environment variables
    console.log("üîç Qdrant Configuration Debug:");
    console.log(`  QDRANT_CLOUD_URL: ${QDRANT_CLOUD_URL ? 'Set' : 'Not set'}`);
    console.log(`  QDRANT_CLOUD_API_KEY: ${QDRANT_CLOUD_API_KEY ? 'Set' : 'Not set'}`);
    console.log(`  QDRANT_HOST: ${QDRANT_HOST}`);
    console.log(`  QDRANT_PORT: ${QDRANT_PORT}`);
    console.log(`  VECTOR_STORE_TYPE: ${VECTOR_STORE_TYPE}`);
    
    // Support both local and cloud Qdrant instances
    if (QDRANT_CLOUD_URL && QDRANT_CLOUD_API_KEY) {
      this.client = new QdrantClient({
        url: QDRANT_CLOUD_URL,
        apiKey: QDRANT_CLOUD_API_KEY
      });
      console.log("‚úÖ Using Qdrant Cloud instance");
    } else {
      this.client = new QdrantClient({ 
        host: QDRANT_HOST, 
        port: QDRANT_PORT 
      });
      console.log("‚úÖ Using local Qdrant instance");
    }
    this.collectionName = COLLECTION_NAME;
  }

  async init(): Promise<void> {
    try {
      const collections = await this.client.getCollections();
      const collection = collections.collections.find((col: any) => col.name === this.collectionName);
      if (!collection) {
        throw new Error(`Qdrant collection '${this.collectionName}' not found. Please create it in Qdrant Cloud and set the correct NEXT_PUBLIC_COLLECTION_NAME.`);
      }
      console.log("‚úÖ Qdrant collection found:", this.collectionName);
    } catch (err) {
      console.error("‚ùå Qdrant init error:", err);
      throw err;
    }
  }

  async addDocuments(documents: Array<{ content: string; metadata?: any }>): Promise<void> {
    try {
      const embeddings = await Promise.all(
        documents.map(doc => getEmbedding(doc.content))
      );
      
      const points = documents.map((doc, i) => ({
        id: `doc_${Date.now()}_${i}`,
        vector: embeddings[i],
        payload: {
          content: doc.content,
          ...doc.metadata
        }
      }));
      
      // Validate vector size matches collection configuration to avoid 400s
      try {
        const info = await this.client.getCollection(this.collectionName);
        const expectedSize = (info as any)?.config?.params?.vectors?.size as number | undefined;
        if (expectedSize && points[0].vector.length !== expectedSize) {
          throw new Error(`Embedding dimension ${points[0].vector.length} does not match collection vector size ${expectedSize} for '${this.collectionName}'.`);
        }
      } catch (e) {
        console.warn("‚ö†Ô∏è Could not verify Qdrant collection vector size before upsert:", e);
      }

      await this.client.upsert(this.collectionName, {
        wait: true,
        points
      });
      
      console.log(`‚úÖ Added ${documents.length} documents to Qdrant`);
    } catch (err) {
      console.error("‚ùå Qdrant add documents error:", err);
      throw err;
    }
  }

  async similaritySearch(query: string, k: number): Promise<Array<{ content: string; metadata: any; distance: number }>> {
    try {
      const queryEmbedding = await getEmbedding(query);
      // Validate embedding size against collection's configured size
      const infoAny: any = await this.client.getCollection(this.collectionName);
      const expectedSize: number | undefined = infoAny?.result?.config?.params?.vectors?.size ?? infoAny?.config?.params?.vectors?.size;
      if (typeof expectedSize === 'number' && queryEmbedding.length !== expectedSize) {
        throw new Error(`Query embedding dimension ${queryEmbedding.length} does not match collection vector size ${expectedSize} for '${this.collectionName}'.`);
      }
      const results = await this.client.search(this.collectionName, {
        vector: queryEmbedding,
        limit: k,
        with_payload: true
      });
      
      return results.map((result: any) => ({
        content: result.payload.content,
        metadata: result.payload,
        distance: result.score
      }));
    } catch (err) {
      console.error("‚ùå Qdrant similarity search error:", err);
      throw err;
    }
  }

  async deleteCollection(): Promise<void> {
    try {
      await this.client.deleteCollection(this.collectionName);
      console.log("‚úÖ Qdrant collection deleted");
    } catch (err) {
      console.error("‚ùå Qdrant delete error:", err);
      throw err;
    }
  }
}

// Factory function
export function createVectorStore(): VectorStore {
  if (VECTOR_STORE_TYPE === 'chroma') {
    return new ChromaVectorStore();
  } else if (VECTOR_STORE_TYPE === 'qdrant') {
    return new QdrantVectorStore();
  } else {
    throw new Error(`Unsupported vector store type: ${VECTOR_STORE_TYPE}`);
  }
}

// Singleton instance
let vectorStoreInstance: VectorStore | null = null;

export async function getVectorStore(): Promise<VectorStore> {
  if (!vectorStoreInstance) {
    vectorStoreInstance = createVectorStore();
    await vectorStoreInstance.init();
  }
  return vectorStoreInstance;
}
</file>

<file path="src/worker.js">
/**
 * Cloudflare Worker for Psychiatry Therapy SuperBot LiteLLM Proxy
 * Provides OpenAI-compatible endpoints for 1minAI integration
 */

// CORS headers configuration
function getCorsHeaders(env) {
    return {
        'Access-Control-Allow-Origin': env.CORS_ORIGINS || '*',
        'Access-Control-Allow-Methods': 'GET, POST, PUT, DELETE, OPTIONS',
        'Access-Control-Allow-Headers': 'Content-Type, Authorization, X-Requested-With',
        'Access-Control-Allow-Credentials': env.CORS_ALLOW_CREDENTIALS || 'true',
    };
}

// Handle CORS preflight requests
function handleCORS(request, env) {
    if (request.method === 'OPTIONS') {
        return new Response(null, {
            status: 200,
            headers: getCorsHeaders(env),
        });
    }
    return null;
}

// Add CORS headers to response
function addCORSHeaders(response, env) {
    const corsHeaders = getCorsHeaders(env);
    const newResponse = new Response(response.body, response);
    Object.entries(corsHeaders).forEach(([key, value]) => {
        newResponse.headers.set(key, value);
    });
    return newResponse;
}

// Make request to 1minAI API
async function make1minAIRequest(messages, model, temperature = 0.7, maxTokens = null, env) {
    const ONEMINAI_API_KEY = env.ONEMINAI_API_KEY;

    if (!ONEMINAI_API_KEY) {
        throw new Error('ONEMINAI_API_KEY not configured');
    }

    // Transform messages to prompt format
    const promptParts = [];
    for (const msg of messages) {
        const { role, content } = msg;
        if (role === 'system') {
            promptParts.push(`System: ${content}`);
        } else if (role === 'assistant') {
            promptParts.push(`Assistant: ${content}`);
        } else {
            promptParts.push(`User: ${content}`);
        }
    }

    const prompt = promptParts.join('\n\n');

    // Map model names to 1minAI supported format
    const modelMapping = {
        '1minai-gpt-4o-mini': 'gpt-4o-mini',
        '1minai-gpt-4o': 'gpt-4o',
        '1minai-claude-3-5-sonnet': 'claude-3-5-sonnet',
        '1minai-claude-3-haiku': 'claude-3-haiku',
        'gpt-4o-mini': 'gpt-4o-mini',
        'gpt-4o': 'gpt-4o',
        'claude-3-5-sonnet': 'claude-3-5-sonnet',
        'claude-3-haiku': 'claude-3-haiku',
        'gemini-2.0-flash-lite': 'gemini-2.0-flash-lite',
        'gemini-2.0-flash': 'gemini-2.0-flash',
        'gemini-1.5-flash': 'gemini-1.5-flash',
        'gemini-1.5-pro': 'gemini-1.5-pro',
    };

    const mappedModel = modelMapping[model] || 'gemini-2.0-flash-lite';

    // Create 1minAI payload
    const payload = {
        type: 'CHAT_WITH_AI',
        model: mappedModel,
        promptObject: {
            prompt: prompt,
            isMixed: false,
            webSearch: false,
        },
    };

    const headers = {
        'API-KEY': ONEMINAI_API_KEY,
        'Content-Type': 'application/json',
    };

    try {
        console.log(`Making request to 1minAI API for model: ${mappedModel}`);

        const response = await fetch('https://api.1min.ai/api/features', {
            method: 'POST',
            headers: headers,
            body: JSON.stringify(payload),
        });

        if (response.ok) {
            const result = await response.json();
            console.log('1minAI API request successful');

            // Parse 1minAI response format
            const aiRecord = result.aiRecord || {};
            const aiRecordDetail = aiRecord.aiRecordDetail || {};
            const resultObject = aiRecordDetail.resultObject || [];

            // Extract response text
            let responseText = '';
            if (Array.isArray(resultObject) && resultObject.length > 0) {
                responseText = String(resultObject[0]);
            } else {
                responseText = 'No response generated';
            }

            // Convert to OpenAI format
            const openaiResponse = {
                id: `chatcmpl-${Date.now()}`,
                object: 'chat.completion',
                created: Math.floor(Date.now() / 1000),
                model: model,
                choices: [
                    {
                        index: 0,
                        message: {
                            role: 'assistant',
                            content: responseText,
                        },
                        finish_reason: 'stop',
                    },
                ],
                usage: {
                    prompt_tokens: prompt.split(' ').length,
                    completion_tokens: responseText.split(' ').length,
                    total_tokens: prompt.split(' ').length + responseText.split(' ').length,
                },
            };

            return openaiResponse;
        } else {
            const errorText = await response.text();
            console.error(`1minAI API error: ${response.status} - ${errorText}`);
            throw new Error(`1minAI API error: ${errorText}`);
        }
    } catch (error) {
        console.error(`1minAI API connection error: ${error.message}`);
        throw new Error(`1minAI API connection failed: ${error.message}`);
    }
}

// Get available models
function getAvailableModels() {
    const models = [
        {
            id: 'gemini-2.0-flash-lite',
            object: 'model',
            created: Math.floor(Date.now() / 1000),
            owned_by: '1minai',
        },
        {
            id: 'gemini-2.0-flash',
            object: 'model',
            created: Math.floor(Date.now() / 1000),
            owned_by: '1minai',
        },
        {
            id: 'gemini-1.5-flash',
            object: 'model',
            created: Math.floor(Date.now() / 1000),
            owned_by: '1minai',
        },
        {
            id: 'gemini-1.5-pro',
            object: 'model',
            created: Math.floor(Date.now() / 1000),
            owned_by: '1minai',
        },
        {
            id: 'gpt-4o-mini',
            object: 'model',
            created: Math.floor(Date.now() / 1000),
            owned_by: '1minai',
        },
        {
            id: 'gpt-4o',
            object: 'model',
            created: Math.floor(Date.now() / 1000),
            owned_by: '1minai',
        },
        {
            id: 'claude-3-5-sonnet',
            object: 'model',
            created: Math.floor(Date.now() / 1000),
            owned_by: '1minai',
        },
        {
            id: 'claude-3-haiku',
            object: 'model',
            created: Math.floor(Date.now() / 1000),
            owned_by: '1minai',
        },
    ];

    return {
        object: 'list',
        data: models,
    };
}

// Main request handler
export default {
    async fetch(request, env, ctx) {
        // Handle CORS preflight
        const corsResponse = handleCORS(request, env);
        if (corsResponse) return corsResponse;

        const url = new URL(request.url);
        const path = url.pathname;

        try {
            // Health check endpoint
            if (path === '/health') {
                const response = new Response(
                    JSON.stringify({
                        status: 'healthy',
                        timestamp: new Date().toISOString(),
                        service: 'psychiatry-therapy-superbot-api',
                        version: '1.0.0',
                    }),
                    {
                        status: 200,
                        headers: { 'Content-Type': 'application/json' },
                    }
                );
                return addCORSHeaders(response, env);
            }

            // Root endpoint
            if (path === '/') {
                const response = new Response(
                    JSON.stringify({
                        service: 'Psychiatry Therapy SuperBot LiteLLM Proxy',
                        version: '1.0.0',
                        status: 'running',
                        endpoints: {
                            health: '/health',
                            chat_completions: '/v1/chat/completions',
                            models: '/v1/models',
                        },
                    }),
                    {
                        status: 200,
                        headers: { 'Content-Type': 'application/json' },
                    }
                );
                return addCORSHeaders(response, env);
            }

            // Models endpoint
            if (path === '/v1/models' && request.method === 'GET') {
                const models = getAvailableModels();
                const response = new Response(JSON.stringify(models), {
                    status: 200,
                    headers: { 'Content-Type': 'application/json' },
                });
                return addCORSHeaders(response, env);
            }

            // Chat completions endpoint
            if ((path === '/v1/chat/completions' || path === '/chat/completions') && request.method === 'POST') {
                try {
                    const requestData = await request.json();
                    const { model = 'gemini-2.0-flash-lite', messages, temperature = 0.7, max_tokens } = requestData;

                    if (!messages || !Array.isArray(messages)) {
                        throw new Error('Messages array is required');
                    }

                    console.log(`Chat completion request for model: ${model}`);
                    console.log(`Request messages: ${messages.length} messages`);

                    // Make request to 1minAI API
                    const result = await make1minAIRequest(messages, model, temperature, max_tokens, env);

                    const response = new Response(JSON.stringify(result), {
                        status: 200,
                        headers: { 'Content-Type': 'application/json' },
                    });
                    return addCORSHeaders(response, env);
                } catch (error) {
                    console.error(`Error in chat completions: ${error.message}`);

                    // Return fallback response
                    const fallbackResponse = {
                        id: `chatcmpl-${Date.now()}`,
                        object: 'chat.completion',
                        created: Math.floor(Date.now() / 1000),
                        model: requestData?.model || 'gemini-2.0-flash-lite',
                        choices: [
                            {
                                index: 0,
                                message: {
                                    role: 'assistant',
                                    content: `I apologize, but I'm currently experiencing technical difficulties. Error: ${error.message}. Please try again later.`,
                                },
                                finish_reason: 'stop',
                            },
                        ],
                        usage: {
                            prompt_tokens: 10,
                            completion_tokens: 20,
                            total_tokens: 30,
                        },
                    };

                    const response = new Response(JSON.stringify(fallbackResponse), {
                        status: 200,
                        headers: { 'Content-Type': 'application/json' },
                    });
                    return addCORSHeaders(response, env);
                }
            }

            // 404 for unknown endpoints
            const response = new Response(
                JSON.stringify({ error: 'Not Found', message: `Endpoint ${path} not found` }),
                {
                    status: 404,
                    headers: { 'Content-Type': 'application/json' },
                }
            );
            return addCORSHeaders(response, env);
        } catch (error) {
            console.error(`Worker error: ${error.message}`);
            const response = new Response(
                JSON.stringify({ error: 'Internal Server Error', message: error.message }),
                {
                    status: 500,
                    headers: { 'Content-Type': 'application/json' },
                }
            );
            return addCORSHeaders(response, env);
        }
    },
};
</file>

<file path="start-services.ps1">
#!/usr/bin/env pwsh
# Start all RAG Superbot services (LiteLLM proxy + Next.js)

Write-Host "üöÄ Starting RAG Superbot with LiteLLM Integration..." -ForegroundColor Cyan
Write-Host ""

# Check if Docker is running
Write-Host "üîç Checking Docker status..." -ForegroundColor Yellow
try {
    docker info | Out-Null
    Write-Host "‚úÖ Docker is running" -ForegroundColor Green
} catch {
    Write-Host "‚ùå Docker is not running. Please start Docker Desktop." -ForegroundColor Red
    exit 1
}

# Check if .env.local exists
if (-not (Test-Path ".env.local")) {
    Write-Host "‚ö†Ô∏è  .env.local not found. Copying from config.env..." -ForegroundColor Yellow
    Copy-Item "config.env" ".env.local"
    Write-Host "‚ö†Ô∏è  Please update .env.local with your API keys!" -ForegroundColor Yellow
    Write-Host ""
}

# Start Docker Compose services (FastAPI LiteLLM proxy)
Write-Host "üê≥ Starting LiteLLM proxy server..." -ForegroundColor Cyan
docker-compose up -d fastapi-litellm

if ($LASTEXITCODE -ne 0) {
    Write-Host "‚ùå Failed to start Docker services" -ForegroundColor Red
    exit 1
}

# Wait for service to be healthy
Write-Host "‚è≥ Waiting for LiteLLM proxy to be healthy..." -ForegroundColor Yellow
$maxAttempts = 30
$attempt = 0

while ($attempt -lt $maxAttempts) {
    try {
        $response = Invoke-WebRequest -Uri "http://localhost:8000/health" -TimeoutSec 2 -UseBasicParsing 2>$null
        if ($response.StatusCode -eq 200) {
            Write-Host "‚úÖ LiteLLM proxy is healthy!" -ForegroundColor Green
            break
        }
    } catch {
        # Service not ready yet
    }
    
    $attempt++
    if ($attempt -eq $maxAttempts) {
        Write-Host "‚ùå LiteLLM proxy failed to become healthy" -ForegroundColor Red
        Write-Host "   Check logs with: docker-compose logs fastapi-litellm" -ForegroundColor Yellow
        exit 1
    }
    
    Write-Host "   Attempt $attempt/$maxAttempts..." -ForegroundColor Gray
    Start-Sleep -Seconds 2
}

Write-Host ""
Write-Host "üåê Starting Next.js development server..." -ForegroundColor Cyan
Write-Host "   Navigate to http://localhost:3000 when ready" -ForegroundColor Gray
Write-Host ""

# Start Next.js in a new terminal window (optional - user can run npm run dev manually)
Write-Host "üìù To start the frontend, run:" -ForegroundColor Yellow
Write-Host "   npm run dev" -ForegroundColor White
Write-Host ""

Write-Host "‚úÖ Services started successfully!" -ForegroundColor Green
Write-Host ""
Write-Host "üìä Service Status:" -ForegroundColor Cyan
Write-Host "   - LiteLLM Proxy: http://localhost:8000" -ForegroundColor White
Write-Host "   - Health Check: http://localhost:8000/health" -ForegroundColor White
Write-Host "   - Models List: http://localhost:8000/v1/models" -ForegroundColor White
Write-Host "   - Next.js App: http://localhost:3000 (after running npm run dev)" -ForegroundColor White
Write-Host ""
Write-Host "üõë To stop services, run:" -ForegroundColor Yellow
Write-Host "   .\stop-services.ps1" -ForegroundColor White
Write-Host ""
</file>

<file path="stop-services.ps1">
#!/usr/bin/env pwsh
# Stop all RAG Superbot services

Write-Host "üõë Stopping RAG Superbot services..." -ForegroundColor Cyan
Write-Host ""

# Stop Docker Compose services
Write-Host "üê≥ Stopping Docker containers..." -ForegroundColor Yellow
docker-compose down

if ($LASTEXITCODE -eq 0) {
    Write-Host "‚úÖ All services stopped successfully!" -ForegroundColor Green
} else {
    Write-Host "‚ö†Ô∏è  Some services may not have stopped cleanly" -ForegroundColor Yellow
}

Write-Host ""
Write-Host "üìù Note: If Next.js is running in another terminal, stop it with Ctrl+C" -ForegroundColor Gray
Write-Host ""
</file>

<file path="test_1minai.py">
#!/usr/bin/env python3
"""
Test script to verify 1minAI API key and endpoint
"""

import json
import urllib.request
import urllib.error

# Test the 1minAI API
ONEMINAI_API_KEY = "5736fcb4bcf61ad04bd7637a0fed935401de06bf64f79b4822bb64fbaab8c1d3"

def test_1minai_api():
    url = "https://api.1min.ai/api/features"
    
    payload = {
        "type": "CHAT_WITH_AI",
        "model": "gemini-2.0-flash-lite",
        "promptObject": {
            "prompt": "Hello, how are you?",
            "isMixed": False,
            "webSearch": False
        }
    }
    
    headers = {
        "API-KEY": ONEMINAI_API_KEY,
        "Content-Type": "application/json"
    }
    
    try:
        data = json.dumps(payload).encode('utf-8')
        req = urllib.request.Request(url, data=data, headers=headers)
        
        print(f"Testing 1minAI API...")
        print(f"URL: {url}")
        print(f"API Key: {ONEMINAI_API_KEY[:10]}...")
        print(f"Payload: {json.dumps(payload, indent=2)}")
        
        with urllib.request.urlopen(req, timeout=30) as response:
            print(f"Response status: {response.status}")
            result = response.read().decode('utf-8')
            print(f"Response: {result}")
            
            # Try to parse JSON
            try:
                parsed = json.loads(result)
                print(f"Parsed response: {json.dumps(parsed, indent=2)}")
            except:
                print("Response is not valid JSON")
                
    except urllib.error.HTTPError as e:
        print(f"HTTP Error: {e.code}")
        error_body = e.read().decode('utf-8')
        print(f"Error body: {error_body}")
    except Exception as e:
        print(f"Error: {e}")

if __name__ == "__main__":
    test_1minai_api()
</file>

<file path="test_fastapi_1minai.py">
#!/usr/bin/env python3
"""
Test the 1minAI API call from the original FastAPI server
"""

import os
import asyncio
import json
import httpx
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

ONEMINAI_API_KEY = os.getenv("ONEMINAI_API_KEY")

async def test_1minai_fastapi_style():
    """Test using the exact same method as fastapi_server.py"""
    
    # Transform messages to prompt format (same as FastAPI server)
    messages = [{"role": "user", "content": "Hello, how are you?"}]
    prompt_parts = []
    for msg in messages:
        role = msg["role"]
        content = msg["content"]
        if role == "system":
            prompt_parts.append(f"System: {content}")
        elif role == "assistant":
            prompt_parts.append(f"Assistant: {content}")
        else:
            prompt_parts.append(f"User: {content}")
    
    prompt = "\n\n".join(prompt_parts)
    
    # Create 1minAI payload (exact same as FastAPI server)
    payload = {
        "type": "CHAT_WITH_AI",
        "model": "gemini-2.0-flash-lite",
        "promptObject": {
            "prompt": prompt,
            "isMixed": False,
            "webSearch": False
        }
    }
    
    headers = {
        "API-KEY": ONEMINAI_API_KEY,
        "Content-Type": "application/json"
    }
    
    print(f"Testing 1minAI API with FastAPI server method...")
    print(f"API Key: {ONEMINAI_API_KEY[:10] if ONEMINAI_API_KEY else 'None'}...")
    print(f"Payload: {json.dumps(payload, indent=2)}")
    
    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                "https://api.1min.ai/api/features",
                json=payload,
                headers=headers
            )
            print(f"Response status: {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                print(f"Success! Response: {json.dumps(result, indent=2)}")
                
                # Parse response like FastAPI server
                ai_record = result.get("aiRecord", {})
                ai_record_detail = ai_record.get("aiRecordDetail", {})
                result_object = ai_record_detail.get("resultObject", [])
                
                if isinstance(result_object, list) and result_object:
                    response_text = str(result_object[0])
                    print(f"Extracted response: {response_text}")
                else:
                    print("No response text found in result")
            else:
                error_text = response.text
                print(f"Error: {response.status_code} - {error_text}")
                
    except Exception as e:
        print(f"Exception: {e}")

if __name__ == "__main__":
    asyncio.run(test_1minai_fastapi_style())
</file>

<file path="test-google-embedding.mjs">
#!/usr/bin/env node

/**
 * Test Google embedding-001 implementation
 * Run with: node test-google-embedding.mjs
 */

import { GoogleGenerativeAI } from '@google/generative-ai';
import dotenv from 'dotenv';

// Load environment variables
dotenv.config({ path: './.env' });

const GOOGLE_API_KEY = process.env.NEXT_PUBLIC_GOOGLE_API_KEY;

async function testGoogleEmbedding() {
  console.log("üß™ Testing Google text-embedding-004 implementation...\n");
  
  if (!GOOGLE_API_KEY) {
    console.error("‚ùå NEXT_PUBLIC_GOOGLE_API_KEY not found in environment variables");
    console.log("Please set your Google API key in the .env file");
    process.exit(1);
  }

  try {
    const genAI = new GoogleGenerativeAI(GOOGLE_API_KEY);
    const model = genAI.getGenerativeModel({ model: "text-embedding-004" });
    
    const testText = "This is a test of Google's text-embedding-004 model for RAG applications.";
    
    console.log(`üìù Test text: "${testText}"`);
    console.log("üîÑ Generating embedding...\n");
    
    const result = await model.embedContent(testText);
    const embedding = result.embedding.values;
    
    console.log("‚úÖ Embedding generated successfully!");
    console.log(`üìä Embedding dimensions: ${embedding.length}`);
    console.log(`üìä First 5 values: [${embedding.slice(0, 5).map(v => v.toFixed(4)).join(', ')}...]`);
    console.log(`üìä Last 5 values: [...${embedding.slice(-5).map(v => v.toFixed(4)).join(', ')}]`);
    
    // Test similarity between similar texts
    console.log("\nüîÑ Testing similarity between similar texts...");
    
    const text1 = "The patient has pleural effusion";
    const text2 = "Pleural effusion is a medical condition";
    const text3 = "The weather is sunny today";
    
    const [embedding1, embedding2, embedding3] = await Promise.all([
      model.embedContent(text1).then(r => r.embedding.values),
      model.embedContent(text2).then(r => r.embedding.values),
      model.embedContent(text3).then(r => r.embedding.values)
    ]);
    
    // Calculate cosine similarity
    function cosineSimilarity(a, b) {
      const dotProduct = a.reduce((sum, val, i) => sum + val * b[i], 0);
      const magnitudeA = Math.sqrt(a.reduce((sum, val) => sum + val * val, 0));
      const magnitudeB = Math.sqrt(b.reduce((sum, val) => sum + val * val, 0));
      return dotProduct / (magnitudeA * magnitudeB);
    }
    
    const similarity1_2 = cosineSimilarity(embedding1, embedding2);
    const similarity1_3 = cosineSimilarity(embedding1, embedding3);
    
    console.log(`üìä Similarity between medical texts: ${similarity1_2.toFixed(4)}`);
    console.log(`üìä Similarity between medical and weather: ${similarity1_3.toFixed(4)}`);
    
    if (similarity1_2 > similarity1_3) {
      console.log("‚úÖ Embeddings correctly show higher similarity for related content!");
    } else {
      console.log("‚ö†Ô∏è Unexpected similarity results - embeddings may need tuning");
    }
    
    console.log("\nüéâ Google text-embedding-004 test completed successfully!");
    
  } catch (error) {
    console.error("‚ùå Test failed:", error.message);
    if (error.message.includes('API_KEY_INVALID')) {
      console.log("üí° Please check your Google API key in the .env file");
    }
    process.exit(1);
  }
}

// Run the test
testGoogleEmbedding();
</file>

<file path="tsconfig.json">
{
  "compilerOptions": {
    "target": "ES2017",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": ["./src/*"]
    }
  },
  "include": ["next-env.d.ts", "**/*.ts", "**/*.tsx", ".next/types/**/*.ts"],
  "exclude": ["node_modules"]
}
</file>

<file path="VERCEL_DEPLOYMENT.md">
# Vercel Deployment Guide

This guide will help you deploy the RAG A2A Superbot frontend to Vercel.

## Prerequisites

1. **Google Gemini API Key**: Get your API key from [Google AI Studio](https://makersuite.google.com/app/apikey)
2. **Qdrant Cloud Account**: Sign up at [Qdrant Cloud](https://cloud.qdrant.io/) and create a cluster
3. **GitHub Repository**: Push your code to GitHub

## Step 1: Prepare Your Repository

1. **Push to GitHub**:
   ```bash
   git add .
   git commit -m "Prepare for Vercel deployment"
   git push origin main
   ```

2. **Verify Files**: Ensure these files are in your repository:
   - `vercel.json`
   - `next.config.ts`
   - `package.json`
   - `src/` directory with all source code

## Step 2: Deploy to Vercel

### Option A: Deploy via Vercel Dashboard

1. Go to [Vercel Dashboard](https://vercel.com/dashboard)
2. Click "New Project"
3. Import your GitHub repository
4. Configure the project:
   - **Framework Preset**: Next.js
   - **Root Directory**: `frontend` (if your frontend is in a subdirectory)
   - **Build Command**: `npm run build`
   - **Output Directory**: `.next`

### Option B: Deploy via Vercel CLI

1. **Install Vercel CLI**:
   ```bash
   npm i -g vercel
   ```

2. **Login to Vercel**:
   ```bash
   vercel login
   ```

3. **Deploy**:
   ```bash
   cd frontend
   vercel
   ```

4. **Follow the prompts**:
   - Link to existing project or create new
   - Set root directory (if needed)
   - Configure environment variables

## Step 3: Configure Environment Variables

In your Vercel dashboard, go to **Settings > Environment Variables** and add:

### Required Variables

```bash
# Google Gemini Configuration
NEXT_PUBLIC_GOOGLE_API_KEY=your_gemini_api_key_here
NEXT_PUBLIC_GEMINI_MODEL=gemini-1.5-flash
NEXT_PUBLIC_GEMINI_TEMPERATURE=0.7
NEXT_PUBLIC_GEMINI_MAX_TOKENS=2048

# Qdrant Cloud Configuration
NEXT_PUBLIC_QDRANT_CLOUD_URL=https://your-cluster-id.eu-central.aws.cloud.qdrant.io
NEXT_PUBLIC_QDRANT_CLOUD_API_KEY=your_qdrant_cloud_api_key_here

# Vector Store Configuration
NEXT_PUBLIC_VECTOR_STORE=qdrant
NEXT_PUBLIC_COLLECTION_NAME=rag_a2a_collection

# App Configuration
NEXT_PUBLIC_APP_NAME=RAG A2A Superbot
NEXT_PUBLIC_APP_VERSION=1.0.0
```

### Environment-Specific Settings

- **Production**: Set all variables for production environment
- **Preview**: Set all variables for preview environment
- **Development**: Set all variables for development environment

## Step 4: Deploy and Test

1. **Trigger Deployment**:
   - If using dashboard: Click "Deploy"
   - If using CLI: `vercel --prod`

2. **Monitor Build**:
   - Check the build logs for any errors
   - Ensure all dependencies are installed correctly

3. **Test Your Application**:
   - Visit your deployed URL
   - Test the chat functionality
   - Verify environment variables are loaded

## Step 5: Post-Deployment Setup

### 1. Load Sample Documents

1. Visit your deployed application
2. Click "Load Sample Docs" to populate the knowledge base
3. Verify documents are loaded successfully

### 2. Test Different Pipeline Modes

1. Try different pipeline modes:
   - Phase 1: Basic A2A
   - Phase 2: Smart A2A
   - Phase 3: Self-Refinement
   - AUTO: AI Selects Optimal
   - META: Intelligent Selection

### 3. Monitor Performance

1. Check Vercel Analytics for performance metrics
2. Monitor API response times
3. Check for any errors in the logs

## Troubleshooting

### Common Issues

#### 1. Build Failures

**Error**: `Module not found` or `Build failed`
**Solution**: 
- Check that all dependencies are in `package.json`
- Ensure TypeScript compilation passes locally
- Check for any missing imports

#### 2. Environment Variables Not Loading

**Error**: `NEXT_PUBLIC_GOOGLE_API_KEY not found`
**Solution**:
- Ensure variables start with `NEXT_PUBLIC_`
- Check that variables are set in Vercel dashboard
- Redeploy after adding variables

#### 3. API Route Errors

**Error**: `500 Internal Server Error`
**Solution**:
- Check Vercel function logs
- Verify API key permissions
- Ensure Qdrant Cloud credentials are correct

#### 4. Vector Store Issues

**Error**: `Qdrant connection failed`
**Solution**:
- Verify Qdrant Cloud URL and API key
- Check collection name is correct
- Ensure Qdrant cluster is active

### Debug Steps

1. **Check Build Logs**:
   ```bash
   vercel logs [deployment-url]
   ```

2. **Test Locally**:
   ```bash
   npm run build
   npm run start
   ```

3. **Verify Environment Variables**:
   - Check Vercel dashboard
   - Test with a simple API endpoint

## Performance Optimization

### 1. Enable Vercel Analytics

1. Go to your project dashboard
2. Navigate to Analytics
3. Enable Web Analytics and Speed Insights

### 2. Optimize Bundle Size

1. Use dynamic imports for heavy components
2. Optimize images with Next.js Image component
3. Remove unused dependencies

### 3. Configure Caching

1. Set appropriate cache headers
2. Use Vercel's Edge Network
3. Implement response caching where appropriate

## Security Considerations

### 1. API Key Security

- Never expose API keys in client-side code
- Use environment variables for all sensitive data
- Rotate API keys regularly

### 2. CORS Configuration

- Configure CORS headers appropriately
- Restrict origins in production
- Use HTTPS for all communications

### 3. Rate Limiting

- Implement rate limiting for API routes
- Monitor for unusual traffic patterns
- Set appropriate limits

## Monitoring and Maintenance

### 1. Set Up Monitoring

1. **Vercel Analytics**: Monitor performance and errors
2. **External Monitoring**: Use services like Sentry for error tracking
3. **Uptime Monitoring**: Monitor application availability

### 2. Regular Maintenance

1. **Update Dependencies**: Keep packages up to date
2. **Monitor API Usage**: Track Google Gemini and Qdrant usage
3. **Review Logs**: Regularly check for errors or issues

### 3. Scaling Considerations

1. **Function Limits**: Be aware of Vercel function limits
2. **Database Scaling**: Consider Qdrant Cloud scaling options
3. **CDN Usage**: Optimize static asset delivery

## Support

If you encounter issues:

1. Check the troubleshooting section above
2. Review Vercel documentation
3. Check the application logs
4. Verify all environment variables are set correctly

---

Your RAG A2A Superbot should now be successfully deployed on Vercel! üöÄ
</file>

<file path="vercel-env-template.txt">
# Vercel Environment Variables Template
# Copy these variables to your Vercel dashboard under Settings > Environment Variables

# Google Gemini Configuration
NEXT_PUBLIC_GOOGLE_API_KEY=your_gemini_api_key_here
NEXT_PUBLIC_GEMINI_MODEL=gemini-1.5-flash
NEXT_PUBLIC_GEMINI_TEMPERATURE=0.7
NEXT_PUBLIC_GEMINI_MAX_TOKENS=2048

# Google Embedding Configuration (Gemini embedding-001)
# 3072-dimensional embeddings. Uses the same API key as Gemini above.
NEXT_PUBLIC_EMBEDDING_MODEL=embedding-001
NEXT_PUBLIC_EMBEDDING_DIM=3072

# Qdrant Cloud Configuration (Required for Vercel)
NEXT_PUBLIC_QDRANT_CLOUD_URL=https://your-cluster-id.eu-central.aws.cloud.qdrant.io
NEXT_PUBLIC_QDRANT_CLOUD_API_KEY=your_qdrant_cloud_api_key_here

# Vector Store Configuration
NEXT_PUBLIC_VECTOR_STORE=qdrant
NEXT_PUBLIC_COLLECTION_NAME=rag_a2a_collection

# App Configuration
NEXT_PUBLIC_APP_NAME=Psychiatry Therapy SuperBot
NEXT_PUBLIC_APP_VERSION=1.0.0

# Note: Ollama and local Qdrant are not supported on Vercel
# Use Qdrant Cloud for production deployment
# Gemini embedding-001 is used for embeddings (3072 dimensions)
</file>

<file path="VPS-DEPLOYMENT-GUIDE.md">
# VPS Deployment Guide for RAG Superbot

This guide provides step-by-step instructions for deploying the RAG Superbot application on a Virtual Private Server (VPS).

## Overview

The RAG Superbot is a full-stack application consisting of:
- **Frontend**: Next.js 15 with React 19 (TypeScript)
- **Backend**: FastAPI server for LiteLLM 1minAI proxy
- **Vector Database**: Qdrant Cloud (external service)
- **AI Models**: 1minAI API integration

## Prerequisites

### VPS Requirements
- **OS**: Ubuntu 20.04 LTS or newer (recommended)
- **RAM**: Minimum 2GB, recommended 4GB+
- **Storage**: Minimum 20GB SSD
- **CPU**: 2+ cores recommended
- **Network**: Public IP with ports 80, 443, 3000, 8000 accessible

### Required Accounts & API Keys
- 1minAI API key (for AI models)
- Qdrant Cloud account and API key
- Domain name (optional but recommended)
- SSL certificate (Let's Encrypt recommended)

## Step 1: Server Setup

### 1.1 Initial Server Configuration

```bash
# Update system packages
sudo apt update && sudo apt upgrade -y

# Install essential packages
sudo apt install -y curl wget git unzip software-properties-common

# Install Node.js 18+ (required for Next.js)
curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
sudo apt install -y nodejs

# Verify Node.js installation
node --version  # Should be 18.0.0 or higher
npm --version   # Should be 8.0.0 or higher

# Install Python 3.9+ (required for FastAPI)
sudo apt install -y python3 python3-pip python3-venv

# Install Docker and Docker Compose (recommended)
sudo apt install -y docker.io docker-compose
sudo systemctl enable docker
sudo systemctl start docker
sudo usermod -aG docker $USER

# Install Nginx (for reverse proxy)
sudo apt install -y nginx

# Install Certbot (for SSL certificates)
sudo apt install -y certbot python3-certbot-nginx
```

### 1.2 Create Application User

```bash
# Create dedicated user for the application
sudo useradd -m -s /bin/bash ragbot
sudo usermod -aG docker ragbot

# Switch to application user
sudo su - ragbot
```

## Step 2: Application Deployment

### 2.1 Clone and Setup Repository

```bash
# Clone the repository
cd /home/ragbot
git clone <your-repository-url> rag-superbot
cd rag-superbot

# Create environment file
cp .env.example .env
```

### 2.2 Configure Environment Variables

Edit the `.env` file with your production values:

```bash
nano .env
```

```env
# Google Gemini Configuration (Fallback/Embeddings only)
NEXT_PUBLIC_GOOGLE_API_KEY=your_gemini_api_key_here
NEXT_PUBLIC_GEMINI_MODEL=gemini-1.5-flash
NEXT_PUBLIC_GEMINI_TEMPERATURE=0.7
NEXT_PUBLIC_GEMINI_MAX_TOKENS=2048
NEXT_PUBLIC_EMBEDDING_MODEL=gemini-embedding-001
NEXT_PUBLIC_EMBEDDING_DIM=3072

# LiteLLM / 1minAI Configuration (Primary)
NEXT_PUBLIC_USE_LITELLM=true
NEXT_PUBLIC_LITELLM_API_URL=https://yourdomain.com/api
NEXT_PUBLIC_LITELLM_MODEL=gemini-2.0-flash-lite
NEXT_PUBLIC_LITELLM_TEMPERATURE=0.7
NEXT_PUBLIC_LITELLM_MAX_TOKENS=2048
ONEMINAI_API_KEY=your_1minai_api_key_here

# Qdrant Cloud Configuration
NEXT_PUBLIC_QDRANT_CLOUD_URL=https://your-cluster-id.eu-central.aws.cloud.qdrant.io
NEXT_PUBLIC_QDRANT_CLOUD_API_KEY=your_qdrant_cloud_api_key_here

# Vector Store Configuration
NEXT_PUBLIC_VECTOR_STORE=qdrant
NEXT_PUBLIC_COLLECTION_NAME=rag_a2a_collection

# Production Configuration
NODE_ENV=production
FASTAPI_HOST=0.0.0.0
FASTAPI_PORT=8000
FASTAPI_RELOAD=false
FASTAPI_LOG_LEVEL=info
```

## Step 3: Docker Deployment (Recommended)

### 3.1 Build and Run with Docker Compose

```bash
# Build and start services
docker-compose up -d

# Check service status
docker-compose ps

# View logs
docker-compose logs -f
```

### 3.2 Alternative: Manual Deployment

If you prefer not to use Docker:

#### Backend (FastAPI) Setup

```bash
# Create Python virtual environment
python3 -m venv venv
source venv/bin/activate

# Install Python dependencies
pip install -r requirements.txt

# Start FastAPI server
python fastapi_server.py
```

#### Frontend (Next.js) Setup

```bash
# Install Node.js dependencies
npm install

# Build the application
npm run build

# Start the production server
npm start
```

## Step 4: Nginx Configuration

### 4.1 Create Nginx Configuration

```bash
sudo nano /etc/nginx/sites-available/ragbot
```

```nginx
server {
    listen 80;
    server_name yourdomain.com www.yourdomain.com;

    # Redirect HTTP to HTTPS
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name yourdomain.com www.yourdomain.com;

    # SSL Configuration (will be configured by Certbot)
    ssl_certificate /etc/letsencrypt/live/yourdomain.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/yourdomain.com/privkey.pem;

    # Security headers
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header Referrer-Policy "no-referrer-when-downgrade" always;
    add_header Content-Security-Policy "default-src 'self' http: https: data: blob: 'unsafe-inline'" always;

    # Frontend (Next.js)
    location / {
        proxy_pass http://localhost:3000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_cache_bypass $http_upgrade;
    }

    # Backend API (FastAPI)
    location /api/ {
        proxy_pass http://localhost:8000/;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_cache_bypass $http_upgrade;
    }

    # Health check endpoint
    location /health {
        proxy_pass http://localhost:8000/health;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

### 4.2 Enable Site and Configure SSL

```bash
# Enable the site
sudo ln -s /etc/nginx/sites-available/ragbot /etc/nginx/sites-enabled/

# Test Nginx configuration
sudo nginx -t

# Restart Nginx
sudo systemctl restart nginx

# Obtain SSL certificate
sudo certbot --nginx -d yourdomain.com -d www.yourdomain.com

# Test SSL renewal
sudo certbot renew --dry-run
```

## Step 5: Process Management with Systemd

### 5.1 Create Systemd Service for FastAPI

```bash
sudo nano /etc/systemd/system/ragbot-api.service
```

```ini
[Unit]
Description=RAG Superbot FastAPI Server
After=network.target

[Service]
Type=simple
User=ragbot
WorkingDirectory=/home/ragbot/rag-superbot
Environment=PATH=/home/ragbot/rag-superbot/venv/bin
ExecStart=/home/ragbot/rag-superbot/venv/bin/python fastapi_server.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

### 5.2 Create Systemd Service for Next.js

```bash
sudo nano /etc/systemd/system/ragbot-frontend.service
```

```ini
[Unit]
Description=RAG Superbot Next.js Frontend
After=network.target

[Service]
Type=simple
User=ragbot
WorkingDirectory=/home/ragbot/rag-superbot
Environment=NODE_ENV=production
ExecStart=/usr/bin/npm start
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

### 5.3 Enable and Start Services

```bash
# Reload systemd
sudo systemctl daemon-reload

# Enable services to start on boot
sudo systemctl enable ragbot-api
sudo systemctl enable ragbot-frontend

# Start services
sudo systemctl start ragbot-api
sudo systemctl start ragbot-frontend

# Check service status
sudo systemctl status ragbot-api
sudo systemctl status ragbot-frontend
```

## Step 6: Firewall Configuration

```bash
# Enable UFW firewall
sudo ufw enable

# Allow SSH (adjust port if needed)
sudo ufw allow 22

# Allow HTTP and HTTPS
sudo ufw allow 80
sudo ufw allow 443

# Check firewall status
sudo ufw status
```

## Step 7: Monitoring and Logging

### 7.1 Setup Log Rotation

```bash
sudo nano /etc/logrotate.d/ragbot
```

```
/home/ragbot/rag-superbot/logs/*.log {
    daily
    missingok
    rotate 52
    compress
    delaycompress
    notifempty
    create 644 ragbot ragbot
    postrotate
        systemctl reload ragbot-api
        systemctl reload ragbot-frontend
    endscript
}
```

### 7.2 Create Log Directory

```bash
sudo mkdir -p /home/ragbot/rag-superbot/logs
sudo chown ragbot:ragbot /home/ragbot/rag-superbot/logs
```

## Step 8: Backup Strategy

### 8.1 Create Backup Script

```bash
nano /home/ragbot/backup.sh
```

```bash
#!/bin/bash
BACKUP_DIR="/home/ragbot/backups"
DATE=$(date +%Y%m%d_%H%M%S)
APP_DIR="/home/ragbot/rag-superbot"

# Create backup directory
mkdir -p $BACKUP_DIR

# Backup application files
tar -czf $BACKUP_DIR/ragbot_$DATE.tar.gz -C /home/ragbot rag-superbot

# Keep only last 7 backups
find $BACKUP_DIR -name "ragbot_*.tar.gz" -mtime +7 -delete

echo "Backup completed: ragbot_$DATE.tar.gz"
```

```bash
chmod +x /home/ragbot/backup.sh

# Add to crontab for daily backups
crontab -e
# Add: 0 2 * * * /home/ragbot/backup.sh
```

## Step 9: Security Hardening

### 9.1 Update Environment Variables for Production

Ensure your `.env` file has production-ready values:
- Use strong, unique API keys
- Set `NODE_ENV=production`
- Configure proper CORS origins
- Use HTTPS URLs for all external services

### 9.2 Regular Updates

```bash
# Create update script
nano /home/ragbot/update.sh
```

```bash
#!/bin/bash
cd /home/ragbot/rag-superbot

# Pull latest changes
git pull origin main

# Update dependencies
npm install
pip install -r requirements.txt

# Rebuild frontend
npm run build

# Restart services
sudo systemctl restart ragbot-api
sudo systemctl restart ragbot-frontend

echo "Update completed"
```

## Step 10: Testing and Verification

### 10.1 Health Checks

```bash
# Test FastAPI health endpoint
curl https://yourdomain.com/health

# Test frontend
curl https://yourdomain.com

# Check service logs
sudo journalctl -u ragbot-api -f
sudo journalctl -u ragbot-frontend -f
```

### 10.2 Performance Testing

```bash
# Install Apache Bench for load testing
sudo apt install apache2-utils

# Test API performance
ab -n 100 -c 10 https://yourdomain.com/health

# Test frontend performance
ab -n 100 -c 10 https://yourdomain.com/
```

## Troubleshooting

### Common Issues

1. **Port conflicts**: Ensure ports 3000 and 8000 are not used by other services
2. **Permission issues**: Check file ownership and permissions
3. **Environment variables**: Verify all required API keys are set
4. **SSL issues**: Check certificate validity and Nginx configuration
5. **Service failures**: Check systemd logs for error details

### Useful Commands

```bash
# Check service status
sudo systemctl status ragbot-api ragbot-frontend

# View logs
sudo journalctl -u ragbot-api -n 50
sudo journalctl -u ragbot-frontend -n 50

# Restart services
sudo systemctl restart ragbot-api ragbot-frontend

# Check port usage
sudo netstat -tlnp | grep :3000
sudo netstat -tlnp | grep :8000

# Test Nginx configuration
sudo nginx -t

# Reload Nginx
sudo systemctl reload nginx
```

## Maintenance

### Regular Tasks

1. **Weekly**: Check service status and logs
2. **Monthly**: Update system packages and dependencies
3. **Quarterly**: Review and rotate API keys
4. **As needed**: Update application code and restart services

### Monitoring Recommendations

Consider implementing:
- Uptime monitoring (UptimeRobot, Pingdom)
- Log aggregation (ELK stack, Grafana)
- Performance monitoring (New Relic, DataDog)
- Automated backups to cloud storage

## Alternative Deployment Methods

### Option A: Coolify Deployment

Coolify is a self-hosted alternative to Heroku/Netlify that simplifies application deployment with a web UI.

#### A.1 Install Coolify on Your VPS

```bash
# Install Coolify (requires Docker)
curl -fsSL https://cdn.coollabs.io/coolify/install.sh | bash

# Access Coolify web interface
# Navigate to http://your-vps-ip:8000
```

#### A.2 Prepare Application for Coolify

Create a `coolify.yaml` configuration file:

```bash
nano coolify.yaml
```

```yaml
# Coolify configuration for RAG Superbot
version: '3.8'

services:
  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_USE_LITELLM=true
      - NEXT_PUBLIC_LITELLM_API_URL=https://yourdomain.com/api
      - NEXT_PUBLIC_LITELLM_MODEL=gemini-2.0-flash-lite
      - NEXT_PUBLIC_QDRANT_CLOUD_URL=${QDRANT_CLOUD_URL}
      - NEXT_PUBLIC_QDRANT_CLOUD_API_KEY=${QDRANT_CLOUD_API_KEY}
      - NEXT_PUBLIC_VECTOR_STORE=qdrant
      - NEXT_PUBLIC_COLLECTION_NAME=rag_a2a_collection
    depends_on:
      - backend
    restart: unless-stopped

  backend:
    build:
      context: .
      dockerfile: Dockerfile.fastapi
    ports:
      - "8000:8000"
    environment:
      - ONEMINAI_API_KEY=${ONEMINAI_API_KEY}
      - FASTAPI_HOST=0.0.0.0
      - FASTAPI_PORT=8000
      - FASTAPI_RELOAD=false
      - FASTAPI_LOG_LEVEL=info
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

#### A.3 Create Frontend Dockerfile

```bash
nano Dockerfile.frontend
```

```dockerfile
FROM node:18-alpine AS base

# Install dependencies only when needed
FROM base AS deps
RUN apk add --no-cache libc6-compat
WORKDIR /app

# Install dependencies based on the preferred package manager
COPY package.json package-lock.json* ./
RUN npm ci --only=production

# Rebuild the source code only when needed
FROM base AS builder
WORKDIR /app
COPY --from=deps /app/node_modules ./node_modules
COPY . .

# Build the application
RUN npm run build

# Production image, copy all the files and run next
FROM base AS runner
WORKDIR /app

ENV NODE_ENV production

RUN addgroup --system --gid 1001 nodejs
RUN adduser --system --uid 1001 nextjs

COPY --from=builder /app/public ./public

# Set the correct permission for prerender cache
RUN mkdir .next
RUN chown nextjs:nodejs .next

# Automatically leverage output traces to reduce image size
COPY --from=builder --chown=nextjs:nodejs /app/.next/standalone ./
COPY --from=builder --chown=nextjs:nodejs /app/.next/static ./.next/static

USER nextjs

EXPOSE 3000

ENV PORT 3000
ENV HOSTNAME "0.0.0.0"

CMD ["node", "server.js"]
```

#### A.4 Deploy with Coolify

1. **Access Coolify Dashboard**: Navigate to `http://your-vps-ip:8000`
2. **Create New Project**: Click "New Project" and select "Docker Compose"
3. **Connect Repository**: Link your Git repository
4. **Configure Environment Variables**:
   ```
   ONEMINAI_API_KEY=your_1minai_api_key
   QDRANT_CLOUD_URL=your_qdrant_url
   QDRANT_CLOUD_API_KEY=your_qdrant_key
   ```
5. **Set Domain**: Configure your custom domain
6. **Deploy**: Click "Deploy" to start the deployment

#### A.5 Coolify Configuration Benefits

- **Automatic SSL**: Coolify handles Let's Encrypt certificates
- **Git Integration**: Auto-deploy on git push
- **Environment Management**: Easy environment variable management
- **Monitoring**: Built-in application monitoring
- **Backup**: Automated backup capabilities

### Option B: Dokploy Deployment

Dokploy is a modern, Docker-based PaaS that provides Heroku-like experience on your own infrastructure.

#### B.1 Install Dokploy on Your VPS

```bash
# Install Dokploy (requires Ubuntu 20.04+ with Docker)
curl -sSL https://dokploy.com/install.sh | sh

# Access Dokploy web interface
# Navigate to http://your-vps-ip:3000
```

#### B.2 Prepare Application for Dokploy

Create a `dokploy.json` configuration file:

```bash
nano dokploy.json
```

```json
{
  "name": "rag-superbot",
  "type": "docker-compose",
  "repository": {
    "url": "https://github.com/yourusername/rag-superbot.git",
    "branch": "main"
  },
  "buildPath": "./",
  "dockerCompose": {
    "file": "docker-compose.dokploy.yml"
  },
  "domains": [
    {
      "host": "yourdomain.com",
      "port": 3000,
      "https": true
    }
  ],
  "environment": {
    "NODE_ENV": "production",
    "ONEMINAI_API_KEY": "${ONEMINAI_API_KEY}",
    "NEXT_PUBLIC_LITELLM_API_URL": "https://yourdomain.com/api",
    "QDRANT_CLOUD_URL": "${QDRANT_CLOUD_URL}",
    "QDRANT_CLOUD_API_KEY": "${QDRANT_CLOUD_API_KEY}"
  }
}
```

#### B.3 Create Dokploy Docker Compose

```bash
nano docker-compose.dokploy.yml
```

```yaml
version: '3.8'

services:
  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_USE_LITELLM=true
      - NEXT_PUBLIC_LITELLM_API_URL=https://yourdomain.com/api
      - NEXT_PUBLIC_LITELLM_MODEL=gemini-2.0-flash-lite
      - NEXT_PUBLIC_QDRANT_CLOUD_URL=${QDRANT_CLOUD_URL}
      - NEXT_PUBLIC_QDRANT_CLOUD_API_KEY=${QDRANT_CLOUD_API_KEY}
      - NEXT_PUBLIC_VECTOR_STORE=qdrant
      - NEXT_PUBLIC_COLLECTION_NAME=rag_a2a_collection
    depends_on:
      - backend
    restart: unless-stopped
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.frontend.rule=Host(`yourdomain.com`)"
      - "traefik.http.routers.frontend.tls=true"
      - "traefik.http.routers.frontend.tls.certresolver=letsencrypt"

  backend:
    build:
      context: .
      dockerfile: Dockerfile.fastapi
    ports:
      - "8000:8000"
    environment:
      - ONEMINAI_API_KEY=${ONEMINAI_API_KEY}
      - FASTAPI_HOST=0.0.0.0
      - FASTAPI_PORT=8000
      - FASTAPI_RELOAD=false
      - FASTAPI_LOG_LEVEL=info
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.backend.rule=Host(`yourdomain.com`) && PathPrefix(`/api`)"
      - "traefik.http.routers.backend.tls=true"
      - "traefik.http.routers.backend.tls.certresolver=letsencrypt"

networks:
  default:
    external:
      name: dokploy
```

#### B.4 Deploy with Dokploy

1. **Access Dokploy Dashboard**: Navigate to `http://your-vps-ip:3000`
2. **Create New Application**: Click "New Application"
3. **Select Source**: Choose "Git Repository" and enter your repo URL
4. **Configure Build Settings**:
   - Build Type: Docker Compose
   - Compose File: `docker-compose.dokploy.yml`
5. **Set Environment Variables**:
   ```
   ONEMINAI_API_KEY=your_1minai_api_key
   QDRANT_CLOUD_URL=your_qdrant_url
   QDRANT_CLOUD_API_KEY=your_qdrant_key
   ```
6. **Configure Domain**: Add your custom domain
7. **Deploy**: Click "Deploy" to start the deployment

#### B.5 Dokploy Advanced Configuration

Create a `dokploy.config.js` for advanced settings:

```javascript
module.exports = {
  apps: [
    {
      name: 'rag-superbot-frontend',
      type: 'docker-compose',
      compose: './docker-compose.dokploy.yml',
      env: {
        NODE_ENV: 'production',
        PORT: 3000
      },
      domains: ['yourdomain.com'],
      ssl: {
        enabled: true,
        forceHttps: true
      },
      monitoring: {
        enabled: true,
        healthCheck: '/health'
      },
      scaling: {
        min: 1,
        max: 3,
        cpu: 80,
        memory: 80
      }
    }
  ],
  database: {
    // If you need a database
    postgres: {
      enabled: false
    },
    redis: {
      enabled: false
    }
  },
  monitoring: {
    enabled: true,
    retention: '30d'
  }
};
```

### Comparison: Manual vs Coolify vs Dokploy

| Feature | Manual Deployment | Coolify | Dokploy |
|---------|------------------|---------|---------|
| **Setup Complexity** | High | Medium | Low |
| **Web Interface** | None | Yes | Yes |
| **Auto SSL** | Manual (Certbot) | Automatic | Automatic |
| **Git Integration** | Manual | Yes | Yes |
| **Monitoring** | Manual setup | Built-in | Built-in |
| **Scaling** | Manual | UI-based | UI-based |
| **Backup** | Manual scripts | Built-in | Built-in |
| **Resource Usage** | Minimal | Medium | Medium |
| **Learning Curve** | Steep | Moderate | Easy |

### Recommended Deployment Strategy

**For Beginners**: Use Dokploy for its simplicity and modern interface
**For Advanced Users**: Use Coolify for more control and features
**For Maximum Control**: Use manual deployment for complete customization

### Post-Deployment Steps (All Methods)

1. **Verify Deployment**:
   ```bash
   curl https://yourdomain.com/health
   curl https://yourdomain.com
   ```

2. **Monitor Logs**:
   - Coolify: Check logs in the web interface
   - Dokploy: View logs in the dashboard
   - Manual: Use `journalctl` or Docker logs

3. **Set Up Monitoring**:
   - Configure uptime monitoring
   - Set up log aggregation
   - Monitor resource usage

4. **Configure Backups**:
   - Database backups (if applicable)
   - Application code backups
   - Environment configuration backups

## Scaling Considerations

For high-traffic deployments:
- Use a load balancer (HAProxy, AWS ALB)
- Implement horizontal scaling with multiple instances
- Use a managed database service
- Consider CDN for static assets
- Implement caching (Redis, Memcached)

---

This guide provides multiple deployment options for your RAG Superbot on a VPS. Choose the method that best fits your technical expertise and requirements:

- **Manual Deployment**: Maximum control and customization
- **Coolify**: Balance of features and simplicity
- **Dokploy**: Modern, user-friendly deployment platform

All methods will result in a production-ready deployment with proper SSL, monitoring, and scaling capabilities.
</file>

<file path="wrangler.toml">
# Cloudflare Workers configuration for Psychiatry Therapy SuperBot LiteLLM Proxy
name = "psychiatry-therapy-superbot-api"
main = "src/worker.js"
compatibility_date = "2024-11-05"
compatibility_flags = ["nodejs_compat"]

# Variables available to the worker
[vars]
CORS_ORIGINS = "*"
CORS_ALLOW_CREDENTIALS = "true"
DEFAULT_MODEL = "gemini-2.0-flash-lite"
MAX_TOKENS = "4096"
TEMPERATURE = "0.7"
LITELLM_BASE_URL = "https://api.1min.ai"

# Production environment
[env.production]
name = "psychiatry-therapy-superbot-api"
vars = { ENVIRONMENT = "production" }

# Staging environment  
[env.staging]
name = "psychiatry-therapy-superbot-api-staging"
vars = { ENVIRONMENT = "staging" }

# Secrets to be set via CLI:
# wrangler secret put ONEMINAI_API_KEY
</file>

<file path="deploy-vercel.sh">
#!/bin/bash

# Deployment script for Psychiatry Therapy SuperBot Frontend to Vercel

echo "üöÄ Deploying Psychiatry Therapy SuperBot Frontend to Vercel..."

# Check if vercel CLI is installed
if ! command -v vercel &> /dev/null; then
    echo "‚ùå Vercel CLI not found. Installing..."
    npm install -g vercel
fi

# Check if logged in to Vercel
echo "üîê Checking Vercel authentication..."
if ! vercel whoami &> /dev/null; then
    echo "‚ùå Not logged in to Vercel. Please run: vercel login"
    exit 1
fi

# Install dependencies
echo "üì¶ Installing dependencies..."
npm install --legacy-peer-deps

# Build the project
echo "üî® Building the project..."
npm run build

if [ $? -eq 0 ]; then
    echo "‚úÖ Build successful!"
    
    # Deploy to Vercel
    echo "üöÄ Deploying to Vercel..."
    vercel --prod
    
    if [ $? -eq 0 ]; then
        echo "‚úÖ Vercel deployment successful!"
        echo ""
        echo "üéâ Deployment complete!"
        echo ""
        echo "üìã Important reminders:"
        echo "1. Make sure your Cloudflare Worker API URL is set in Vercel environment variables"
        echo "2. Update NEXT_PUBLIC_LITELLM_API_URL to point to your Cloudflare Worker"
        echo "3. Test the full application flow"
        echo ""
        echo "üîß Environment variables to set in Vercel dashboard:"
        echo "  NEXT_PUBLIC_LITELLM_API_URL=https://your-project-name.up.railway.app"
        echo "  NEXT_PUBLIC_USE_LITELLM=true"
        echo "  (plus all other environment variables from .env.local)"
    else
        echo "‚ùå Vercel deployment failed!"
        exit 1
    fi
else
    echo "‚ùå Build failed!"
    exit 1
fi
</file>

<file path="RENDER-SOLUTION.md">
# Render Deployment Solution

## Problem
Render build was failing with Rust compilation errors:
```
error: failed to create directory `/usr/local/cargo/registry/cache/index.crates.io-1949cf8c6b5b557f`
Caused by: Read-only file system (os error 30)
üí• maturin failed
```

This happens because `pydantic-core` (required by newer FastAPI/Pydantic versions) needs Rust compilation, which isn't available on Render's free tier.

## Solution
Use a **dual-server approach**:

### Local Development
- **Server**: `fastapi_server.py` (full FastAPI with all features)
- **Port**: 8000 (via `FASTAPI_PORT=8000` in `.env.local`)
- **Dependencies**: Full requirements in `requirements.txt`

### Render Production  
- **Server**: `simple_server.py` (Python standard library only)
- **Port**: 10000 (automatic via Render's `PORT` environment variable)
- **Dependencies**: None (uses only built-in Python modules)

## Key Changes Made

### 1. Updated render.yaml
```yaml
# Before (caused compilation errors)
buildCommand: "pip install -r requirements-render.txt"
startCommand: "python fastapi_server.py"

# After (no compilation needed)
buildCommand: "echo 'Using Python standard library only'"
startCommand: "python simple_server.py"
```

### 2. Added runtime.txt
```
python-3.13.4
```
Specifies Python 3.13.4 to match Render's default (ensures consistency).

### 3. Server Comparison

| Feature | fastapi_server.py | simple_server.py |
|---------|-------------------|------------------|
| **Dependencies** | FastAPI, httpx, pydantic | Python standard library only |
| **Compilation** | Requires Rust/C compilation | No compilation needed |
| **Render Compatible** | ‚ùå (compilation issues) | ‚úÖ (works perfectly) |
| **Local Development** | ‚úÖ (full features) | ‚úÖ (basic features) |
| **1minAI Integration** | ‚úÖ (httpx) | ‚úÖ (urllib) |
| **OpenAI Compatibility** | ‚úÖ | ‚úÖ |
| **CORS Support** | ‚úÖ | ‚úÖ |

## Deployment Process

### 1. Local Development
```bash
# Use full FastAPI server locally
python fastapi_server.py
# Runs on http://localhost:8000
```

### 2. Render Deployment
```bash
# Push to GitHub
git add .
git commit -m "Fix Render compilation issues"
git push origin main

# Deploy via Render Dashboard
# Uses simple_server.py automatically
# Runs on https://your-service.onrender.com (port 10000)
```

### 3. Set Environment Variable
In Render Dashboard ‚Üí Environment:
- **Key**: `ONEMINAI_API_KEY`
- **Value**: Your actual 1minAI API key

## Benefits

‚úÖ **No Compilation Issues**: `simple_server.py` uses only Python standard library  
‚úÖ **Same Functionality**: Both servers provide identical API endpoints  
‚úÖ **Automatic Port Handling**: Works on both local (8000) and Render (10000)  
‚úÖ **Free Tier Compatible**: No build timeouts or memory issues  
‚úÖ **Fast Deployments**: No dependency installation needed  

## API Endpoints (Both Servers)

- `GET /health` - Health check
- `GET /v1/models` - List available models  
- `POST /v1/chat/completions` - Chat completions (OpenAI compatible)
- `POST /chat/completions` - Alternative chat endpoint

## Testing

### Local
```bash
curl http://localhost:8000/health
```

### Render
```bash
curl https://your-service.onrender.com/health
```

Both should return the same response format.

## Result
‚úÖ **Local Development**: Full FastAPI server with all features (port 8000)  
‚úÖ **Render Production**: Lightweight server with same API (port 10000)  
‚úÖ **No Compilation**: Zero build issues on Render's free tier  
‚úÖ **Same Functionality**: Identical API behavior in both environments
</file>

<file path="requirements.txt">
# FastAPI server dependencies for LiteLLM 1minAI proxy (Render Free Tier Compatible)
fastapi==0.100.1
uvicorn[standard]==0.23.2
pydantic==2.3.0
python-multipart==0.0.6
httpx==0.24.1
aiohttp==3.8.5
python-dotenv==1.0.0
</file>

<file path="setup-deployment.sh">
#!/bin/bash

# Quick setup script for Psychiatry Therapy SuperBot deployment

echo "üß† Psychiatry Therapy SuperBot - Deployment Setup"
echo "=================================================="
echo ""

# Check prerequisites
echo "üîç Checking prerequisites..."

# Check Node.js
if ! command -v node &> /dev/null; then
    echo "‚ùå Node.js not found. Please install Node.js 18+ first."
    exit 1
fi

# Check npm
if ! command -v npm &> /dev/null; then
    echo "‚ùå npm not found. Please install npm first."
    exit 1
fi

echo "‚úÖ Node.js and npm found"

# Install CLI tools
echo ""
echo "üì¶ Installing deployment tools..."

# Install Render CLI (optional - can use web dashboard)
if ! command -v render &> /dev/null; then
    echo "Installing Render CLI..."
    if [[ "$OSTYPE" == "darwin"* ]]; then
        # macOS
        if command -v brew &> /dev/null; then
            brew install render
        else
            echo "‚ö†Ô∏è  Homebrew not found. You can install Render CLI manually or use the web dashboard."
        fi
    elif [[ "$OSTYPE" == "linux-gnu"* ]]; then
        # Linux
        curl -fsSL https://cli.render.com/install | sh
    else
        echo "‚ö†Ô∏è  Please install Render CLI manually from: https://render.com/docs/cli"
        echo "    Or use the web dashboard at: https://dashboard.render.com"
    fi
else
    echo "‚úÖ Render CLI already installed"
fi

# Install Vercel CLI
if ! command -v vercel &> /dev/null; then
    echo "Installing Vercel CLI..."
    npm install -g vercel
else
    echo "‚úÖ Vercel CLI already installed"
fi

# Make deployment scripts executable
echo ""
echo "üîß Setting up deployment scripts..."
chmod +x deploy-render.sh
chmod +x deploy-vercel.sh
echo "‚úÖ Deployment scripts are now executable"

# Create directories if they don't exist
mkdir -p src

echo ""
echo "üéâ Setup complete!"
echo ""
echo "üìã Next steps:"
echo "1. Make sure your code is in a Git repository"
echo "2. Login to Render: render auth login (or use web dashboard)"
echo "3. Login to Vercel: vercel login"
echo "4. Deploy backend: ./deploy-render.sh"
echo "5. Set ONEMINAI_API_KEY in Render dashboard"
echo "6. Update Vercel environment variables with your Render URL"
echo "7. Deploy frontend: ./deploy-vercel.sh"
echo ""
echo "üìñ For detailed instructions, see DEPLOYMENT.md"
</file>

<file path="vercel-env-production.txt">
# Production Environment Variables for Vercel Deployment
# Copy these to your Vercel dashboard under Settings > Environment Variables

# LiteLLM / Render Configuration (Primary)
NEXT_PUBLIC_USE_LITELLM=true
NEXT_PUBLIC_LITELLM_API_URL=https://rag-superbot-litellm-v3-psych.onrender.com
NEXT_PUBLIC_LITELLM_MODEL=gemini-2.0-flash-lite
NEXT_PUBLIC_LITELLM_TEMPERATURE=0.7
NEXT_PUBLIC_LITELLM_MAX_TOKENS=2048

# Google Gemini Configuration (Fallback/Embeddings only)
NEXT_PUBLIC_GOOGLE_API_KEY=your_gemini_api_key_here
NEXT_PUBLIC_GEMINI_MODEL=gemini-2.5-flash
NEXT_PUBLIC_GEMINI_TEMPERATURE=0.7
NEXT_PUBLIC_GEMINI_MAX_TOKENS=2048
NEXT_PUBLIC_EMBEDDING_MODEL=gemini-embedding-001
NEXT_PUBLIC_EMBEDDING_DIM=3072

# Qdrant Cloud Configuration
NEXT_PUBLIC_QDRANT_CLOUD_URL=your_qdrant_cloud_url_here
NEXT_PUBLIC_QDRANT_CLOUD_API_KEY=your_qdrant_cloud_api_key_here

# Vector Store Configuration
NEXT_PUBLIC_VECTOR_STORE=qdrant
NEXT_PUBLIC_COLLECTION_NAME=psychiatry_therapy_v1_google-001

# App Configuration
NEXT_PUBLIC_APP_NAME=Psychiatry Therapy SuperBot
NEXT_PUBLIC_APP_VERSION=1.0.0

# Production Settings
NODE_ENV=production
</file>

<file path="vercel.json">
{
  "version": 2,
  "env": {
    "NODE_ENV": "production"
  },
  "build": {
    "env": {
      "NEXT_PUBLIC_APP_NAME": "Psychiatry Therapy SuperBot",
      "NEXT_PUBLIC_APP_VERSION": "1.0.0",
      "NEXT_PUBLIC_USE_LITELLM": "true"
    }
  },
  "functions": {
    "src/app/api/**/*.ts": {
      "maxDuration": 30
    }
  },
  "headers": [
    {
      "source": "/api/(.*)",
      "headers": [
        {
          "key": "Access-Control-Allow-Origin",
          "value": "*"
        },
        {
          "key": "Access-Control-Allow-Methods",
          "value": "GET, POST, PUT, DELETE, OPTIONS"
        },
        {
          "key": "Access-Control-Allow-Headers",
          "value": "Content-Type, Authorization"
        }
      ]
    }
  ]
}
</file>

<file path=".gitignore">
# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.

# dependencies
/node_modules
__pycache__
.pip
.env
.venv
/.pnp
.pnp.*
.yarn/*
!.yarn/patches
!.yarn/plugins
!.yarn/releases
!.yarn/versions

# testing
/coverage
cache
# next.js
/.next/
/out/

# production
/build

# misc
.DS_Store
*.pem

# debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*
.pnpm-debug.log*

# env files (can opt-in for committing if needed)
.env*
.env.local


# vercel
.vercel

# typescript
*.tsbuildinfo
next-env.d.ts

.vercel
WARP.md
logs.ps1
.vercel

.kiro/
.vscode
</file>

<file path="DOCKER-COMPOSE-TO-RENDER.md">
# Docker Compose to Render Migration Guide

This document explains how your `docker-compose.yml` configuration translates to Render deployment.

## üîÑ Configuration Mapping

### Docker Compose Configuration
```yaml
# docker-compose.yml
services:
  fastapi-litellm:
    build:
      context: .
      dockerfile: Dockerfile.fastapi
    container_name: rag-superbot-litellm-proxy
    ports:
      - "8000:8000"
    environment:
      - ONEMINAI_API_KEY=${ONEMINAI_API_KEY}
      - FASTAPI_HOST=0.0.0.0
      - FASTAPI_PORT=8000
      # ... other environment variables
    volumes:
      - ./fastapi_server.py:/app/fastapi_server.py
      - ./requirements.txt:/app/requirements.txt
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
    restart: unless-stopped
```

### Render Equivalent (render.yaml)
```yaml
services:
  - type: web
    name: psychiatry-therapy-superbot-api
    env: docker
    dockerfilePath: ./Dockerfile.fastapi
    healthCheckPath: /health
    envVars:
      - key: ONEMINAI_API_KEY
        sync: false  # Secret
      - key: FASTAPI_HOST
        value: "0.0.0.0"
      - key: FASTAPI_PORT
        value: "10000"  # Render default
      # ... other environment variables
```

## üîß Key Differences

| Docker Compose | Render | Notes |
|----------------|--------|-------|
| `ports: "8000:8000"` | Uses `PORT=10000` | Render uses port 10000 by default |
| `container_name` | `name` in render.yaml | Render manages container naming |
| `volumes` | Not needed | Render builds from git source |
| `networks` | Auto-managed | Render handles networking + SSL |
| `restart: unless-stopped` | Built-in | Render auto-restarts on failure |
| Manual SSL setup | Automatic HTTPS | Render provides free SSL certificates |
| `command` in docker-compose | `CMD` in Dockerfile | Render uses Dockerfile CMD, no startCommand |

## üåê Environment Variables

All your docker-compose environment variables work in Render:

### Via render.yaml (Public Variables)
```yaml
envVars:
  - key: FASTAPI_HOST
    value: "0.0.0.0"
  - key: FASTAPI_PORT
    value: "10000"
  - key: LITELLM_BASE_URL
    value: "https://api.1min.ai"
  - key: DEFAULT_MODEL
    value: "gemini-2.0-flash-lite"
  - key: MAX_TOKENS
    value: "4096"
  - key: TEMPERATURE
    value: "0.7"
  - key: CORS_ORIGINS
    value: "*"
  - key: CORS_ALLOW_CREDENTIALS
    value: "true"
```

### Via Render Dashboard (Secrets)
```bash
# Set in Render dashboard as environment variables
ONEMINAI_API_KEY=your_secret_key_here
```

## üöÄ Deployment Process

### Docker Compose (Local)
```bash
docker-compose up --build
```

### Render (Cloud)
```bash
# Method 1: Blueprint deployment
render blueprint launch

# Method 2: Git-based deployment (automatic)
git push origin main  # Auto-deploys if connected to GitHub
```

Both approaches:
1. Build the Docker image using `Dockerfile.fastapi`
2. Set environment variables
3. Start the FastAPI server on the correct port
4. Enable health checks at `/health`
5. Handle automatic restarts and scaling

## üìä Monitoring & Logs

### Docker Compose
```bash
docker-compose logs -f fastapi-litellm
docker stats
```

### Render
```bash
# Via CLI
render logs -s psychiatry-therapy-superbot-api --tail

# Via Dashboard
# Visit https://dashboard.render.com
# Click on your service ‚Üí Logs tab
```

## üîÑ Development Workflow

### Local Development (Docker Compose)
```bash
# Start services
docker-compose up

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

### Render Development
```bash
# Deploy changes (if using CLI)
render blueprint launch

# Or just push to git (if connected to GitHub)
git add .
git commit -m "Update API"
git push origin main  # Auto-deploys
```

## üéØ Benefits of Render vs Local Docker Compose

| Feature | Docker Compose | Render |
|---------|----------------|--------|
| **Deployment** | Manual server setup | Git-based auto-deployment |
| **Scaling** | Manual container management | Auto-scaling based on traffic |
| **Monitoring** | Basic Docker stats | Built-in metrics, alerts & logs |
| **SSL/HTTPS** | Manual setup required | Automatic SSL certificates |
| **Domain** | Manual DNS setup | Free .onrender.com + custom domains |
| **Logs** | Local only | Persistent cloud logs with search |
| **Backups** | Manual | Automatic |
| **Updates** | Manual rebuild | Git push = auto-deploy |
| **Health Checks** | Basic Docker health | Advanced health monitoring |
| **Zero Downtime** | Manual blue-green | Built-in zero-downtime deployments |

## üîß Migration Checklist

- [x] ‚úÖ Same Dockerfile (`Dockerfile.fastapi`)
- [x] ‚úÖ Same environment variables (with PORT=10000 for Render)
- [x] ‚úÖ Same health check endpoint (`/health`)
- [x] ‚úÖ Same FastAPI application code
- [x] ‚úÖ Same restart policies (automatic)
- [x] ‚úÖ Same CORS configuration
- [x] ‚úÖ **Plus** automatic HTTPS, custom domains, auto-scaling

## üöÄ Quick Migration Steps

1. **Keep your docker-compose.yml** (for local development)
2. **Create render.yaml** (defines cloud deployment)
3. **Deploy to Render** using Blueprint
4. **Set secrets** in Render dashboard
5. **Test both environments** to ensure consistency

```bash
# Local testing (same as before)
docker-compose up

# Render deployment (new)
render blueprint launch

# Both environments work identically!
```

## üåü Render-Specific Benefits

### Automatic Features
- ‚úÖ **Free SSL certificates** for all domains
- ‚úÖ **Auto-scaling** based on CPU/memory usage
- ‚úÖ **Zero-downtime deployments** with health checks
- ‚úÖ **Git integration** - push to deploy
- ‚úÖ **Environment management** with secrets
- ‚úÖ **Custom domains** with automatic SSL
- ‚úÖ **Global CDN** for static assets
- ‚úÖ **DDoS protection** included

### Developer Experience
- ‚úÖ **Real-time logs** with search and filtering
- ‚úÖ **Metrics dashboard** with CPU, memory, response times
- ‚úÖ **Deployment history** with rollback capability
- ‚úÖ **Preview deployments** for pull requests
- ‚úÖ **Team collaboration** with role-based access

## üîó URLs and Endpoints

### Local (Docker Compose)
```
http://localhost:8000/health
http://localhost:8000/v1/models
http://localhost:8000/v1/chat/completions
```

### Render (Cloud)
```

```

Your Docker Compose setup is now running in the cloud with Render's enterprise-grade infrastructure! üéâ

## üöÄ Next Steps

1. **Deploy**: `./deploy-render.sh`
2. **Set API Key**: Add `ONEMINAI_API_KEY` in Render dashboard
3. **Test**: Verify all endpoints work
4. **Update Frontend**: Point Vercel to your new Render URL
5. **Monitor**: Use Render dashboard for logs and metrics
</file>

<file path="Dockerfile.fastapi">
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies (minimal for Render free tier)
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Copy requirements first for better caching
COPY requirements.txt .

# Install Python dependencies with optimizations for Render free tier
# Use --only-binary to avoid compilation issues
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir --only-binary=all -r requirements.txt

# Copy application files
COPY fastapi_server.py .

# Expose port (Render will override this with PORT env var)
EXPOSE 8000

# Health check (will use the PORT env var at runtime)
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
  CMD curl -f http://localhost:${PORT:-8000}/health || exit 1

# Run the FastAPI server
CMD ["python", "fastapi_server.py"]
</file>

<file path="render-env-template.txt">
# Render Environment Variables Template
# Most variables are set in render.yaml, but secrets should be set in Render dashboard

# SECRET VARIABLES (Set in Render Dashboard)
# Go to your service ‚Üí Environment tab ‚Üí Add Environment Variable
ONEMINAI_API_KEY=your_1minai_api_key_here

# PUBLIC VARIABLES (Already defined in render.yaml)
# These are automatically set by render.yaml:
# FASTAPI_HOST=0.0.0.0
# PORT=10000 (Render sets this automatically - don't override)
# FASTAPI_RELOAD=false
# FASTAPI_LOG_LEVEL=info
# LITELLM_BASE_URL=https://api.1min.ai
# DEFAULT_MODEL=gemini-2.0-flash-lite
# MAX_TOKENS=4096
# TEMPERATURE=0.7
# CORS_ORIGINS=*
# CORS_ALLOW_CREDENTIALS=true
# HEALTH_CHECK_INTERVAL=30

# RENDER-SPECIFIC VARIABLES (Automatically set by Render)
# PORT=10000 (Render sets this automatically)
# RENDER_SERVICE_NAME=psychiatry-therapy-superbot-api
# RENDER_EXTERNAL_URL=

# How to set the secret in Render Dashboard:
# 1. Go to https://dashboard.render.com
# 2. Click on your service "psychiatry-therapy-superbot-api"
# 3. Go to "Environment" tab
# 4. Click "Add Environment Variable"
# 5. Key: ONEMINAI_API_KEY
# 6. Value: your_actual_1minai_api_key_here
# 7. Click "Save Changes"
</file>

<file path="RENDER-DEPLOYMENT-GUIDE.md">
# Render Deployment Guide

## üöÄ Quick Deploy to Render

Your FastAPI backend is now configured to work perfectly with Render's free tier.

### 1. Port Configuration Summary

| Environment | Port | Configuration |
|-------------|------|---------------|
| **Local Development** | 8000 | Set in `.env.local` via `FASTAPI_PORT=8000` |
| **Render Production** | 10000 | Automatic via Render's `PORT` environment variable |
| **Frontend Local** | Points to `http://localhost:8000` | Via `NEXT_PUBLIC_LITELLM_API_URL` |
| **Frontend Production** | Points to `https://your-service.onrender.com` | Update after deployment |

### 2. Deployment Steps

#### Step 1: Push to GitHub
```bash
git add .
git commit -m "Configure for Render deployment"
git push origin main
```

#### Step 2: Deploy to Render
1. Go to [Render Dashboard](https://dashboard.render.com)
2. Click **"New +"** ‚Üí **"Blueprint"**
3. Connect your GitHub repository
4. Render will detect `render.yaml` automatically
5. Click **"Apply"**

#### Step 3: Set Secret Environment Variable
1. Go to your service in Render dashboard
2. Click **"Environment"** tab
3. Add environment variable:
   - **Key**: `ONEMINAI_API_KEY`
   - **Value**: `your_actual_1minai_api_key_here`
4. Click **"Save Changes"**

### 3. Your Service URLs

After deployment, your service will be available at:


### 4. Update Frontend Configuration

After successful deployment, update your frontend to use the production API:

**For Production Deployment** (Vercel/Netlify):
```env
NEXT_PUBLIC_LITELLM_API_URL=
```

**For Local Development** (keep as is):
```env
NEXT_PUBLIC_LITELLM_API_URL=http://localhost:8000
```

### 5. Test Your Deployment

```bash
# Test health endpoint
curl 

# Test chat endpoint
curl -X POST /v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-2.0-flash-lite",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

### 6. Configuration Files Used

- `render.yaml` - Render service configuration  
- `simple_server.py` - Lightweight server using only Python standard library (for Render)
- `fastapi_server.py` - Full FastAPI server (for local development)
- `runtime.txt` - Specifies Python 3.13.4 to match Render's default

### 7. How Port Handling Works

Both servers automatically handle different environments:

**Local Development** (`fastapi_server.py`):
```python
port = int(os.getenv("PORT", os.getenv("FASTAPI_PORT", "8000")))
# Uses FASTAPI_PORT=8000 from .env.local
```

**Render Production** (`simple_server.py`):
```python
PORT = int(os.getenv("PORT", "10000"))
# Uses Render's automatic PORT=10000
```

### 8. Free Tier Considerations

- **Cold Starts**: Service sleeps after 15 minutes of inactivity
- **Build Time**: 15 minutes maximum
- **Memory**: 512MB runtime limit
- **No Custom Domains**: Use `.onrender.com` subdomain

### 9. Troubleshooting

**Build Fails**:
- Check `requirements-render.txt` has compatible versions
- Ensure `ONEMINAI_API_KEY` is set in Render dashboard

**Service Won't Start**:
- Check logs in Render dashboard
- Verify health check endpoint `/health` works

**API Calls Fail**:
- Verify `ONEMINAI_API_KEY` is correctly set
- Check CORS settings in `fastapi_server.py`

### 10. Upgrade Path

When ready to upgrade from free tier:
- **Starter ($7/month)**: No sleep, faster builds
- **Standard ($25/month)**: More resources, priority support

## ‚úÖ Ready to Deploy!

Your configuration is now optimized for both local development (port 8000) and Render production (port 10000). The FastAPI server automatically detects the environment and uses the correct port.
</file>

<file path="RENDER-FREE-TIER.md">
# Render Free Tier Deployment Guide

This guide explains how to deploy your Psychiatry Therapy SuperBot to Render's **free tier** without compilation issues.

## üÜì Free Tier Optimizations

### Issue: Compilation & Compatibility Errors
The original approach failed because:
- `pydantic-core` requires Rust compilation
- `aiohttp` requires C extension compilation  
- Older package versions have Python 3.13 compatibility issues
- Render's free tier has limited build resources

### Solution: Optimized Python Runtime
Instead of Docker, we use Render's **Python 3.13 runtime** with compatible packages:
- ‚úÖ No Docker compilation needed
- ‚úÖ Latest package versions with Python 3.13 compatibility
- ‚úÖ Pre-compiled wheels for all dependencies
- ‚úÖ Replaced aiohttp with httpx (no C extensions)
- ‚úÖ Faster builds on free tier
- ‚úÖ Same functionality

## üìã Configuration Changes

### Before (Docker Runtime)
```yaml
# render.yaml (Docker - caused compilation issues)
services:
  - type: web
    env: docker
    dockerfilePath: ./Dockerfile.fastapi
```

### After (Python Runtime - Free Tier Compatible)
```yaml
# render.yaml (Python - works on free tier)
services:
  - type: web
    env: python3
    buildCommand: "pip install -r requirements-render.txt"
    startCommand: "python fastapi_server.py"
    plan: free
```

## üì¶ Dependencies

### Original Requirements (Compilation Issues)
```txt
# requirements.txt (caused Rust compilation errors)
fastapi==0.104.1
pydantic==2.5.0  # ‚ùå Requires Rust compilation
```

### Free Tier Requirements (Python 3.13 Compatible)
```txt
# requirements-render.txt (Python 3.13 compatible)
fastapi==0.115.0
pydantic==2.9.2   # ‚úÖ Python 3.13 compatible
httpx==0.27.2     # ‚úÖ Replaces aiohttp (no C extensions)
uvicorn==0.32.0   # ‚úÖ Latest stable version
```

### Python Version (Render Default)
Render uses Python 3.13 by default, so we use packages compatible with it.

## üöÄ Deployment Process

### 1. Files Used
- `render.yaml` - Python runtime configuration
- `requirements-render.txt` - Free tier compatible dependencies
- `runtime.txt` - Specifies Python 3.11.9 (avoids 3.13 issues)
- `fastapi_server.py` - Updated to use httpx instead of aiohttp

### 2. Build Process
```bash
# Render automatically runs:
pip install -r requirements-render.txt
python fastapi_server.py
```

### 3. Environment Variables
Same as before - set in Render dashboard:
- `ONEMINAI_API_KEY` (secret)
- All other vars defined in `render.yaml`

## üîÑ Local vs Render

### Local Development (Docker Compose)
```bash
# Use full requirements for local development
docker-compose up  # Uses requirements.txt
```

### Render Deployment (Python Runtime)
```bash
# Uses optimized requirements for free tier
render blueprint launch  # Uses requirements-render.txt
```

## üéØ Benefits of Python Runtime

| Feature | Docker Runtime | Python Runtime |
|---------|----------------|----------------|
| **Build Speed** | Slower (compilation) | Faster (pre-compiled) |
| **Free Tier** | ‚ùå Compilation issues | ‚úÖ Works perfectly |
| **Dependencies** | Full versions | Optimized versions |
| **Functionality** | Same | Same |
| **Performance** | Same | Same |
| **Deployment** | Complex | Simple |

## üîß Troubleshooting Free Tier

### Common Issues & Solutions

#### 1. Build Timeout
```
Error: Build timed out
```
**Solution**: Use `requirements-render.txt` with lighter dependencies

#### 2. Memory Limit
```
Error: Build killed (out of memory)
```
**Solution**: Remove unnecessary dependencies or upgrade plan

#### 3. Package Not Found
```
Error: Could not find a version that satisfies the requirement
```
**Solution**: Use older, stable versions in `requirements-render.txt`

### Free Tier Limits
- **Build Time**: 15 minutes max
- **Memory**: 512MB during build
- **Runtime Memory**: 512MB
- **Sleep**: Services sleep after 15 minutes of inactivity

## üöÄ Quick Deploy

```bash
# Deploy to Render free tier
./deploy-render.sh

# Your API will be available at:
# 
```

## üìà Upgrade Path

When ready to upgrade:

1. **Starter Plan ($7/month)**:
   - No sleep
   - More build resources
   - Can use Docker runtime if preferred

2. **Standard Plan ($25/month)**:
   - More memory and CPU
   - Faster builds
   - Priority support

## ‚úÖ Free Tier Checklist

- [x] ‚úÖ Use `render.yaml` with `env: python3`
- [x] ‚úÖ Use `requirements-render.txt` for dependencies
- [x] ‚úÖ Use Python 3.13 compatible package versions
- [x] ‚úÖ Set `plan: free` in render.yaml
- [x] ‚úÖ Keep build command simple
- [x] ‚úÖ Replace aiohttp with httpx in code
- [x] ‚úÖ Set secrets in Render dashboard
- [x] ‚úÖ Test locally first

Your FastAPI server now deploys perfectly on Render's free tier! üéâ
</file>

<file path="runtime.txt">
python-3.13.4
</file>

<file path="deploy-render.sh">
#!/bin/bash

# Render Deployment Script for Psychiatry Therapy SuperBot
# This script helps you deploy to Render using their Blueprint feature

echo "üöÄ Deploying Psychiatry Therapy SuperBot to Render..."

# Check if render.yaml exists
if [ ! -f "render.yaml" ]; then
    echo "‚ùå render.yaml not found! Make sure you're in the project root directory."
    exit 1
fi

# Check if requirements-render.txt exists
if [ ! -f "requirements-render.txt" ]; then
    echo "‚ùå requirements-render.txt not found! This is required for Render deployment."
    exit 1
fi

echo "‚úÖ Configuration files found"

# Display deployment information
echo ""
echo "üìã Deployment Configuration:"
echo "   Service Name: psychiatry-therapy-superbot-api"
echo "   Runtime: Python 3"
echo "   Plan: Free Tier"
echo "   Port: 10000 (automatic)"
echo "   Health Check: /health"
echo ""

# Instructions for manual deployment
echo "üîß Manual Deployment Steps:"
echo ""
echo "1. Go to https://dashboard.render.com"
echo "2. Click 'New +' ‚Üí 'Blueprint'"
echo "3. Connect your GitHub repository"
echo "4. Render will automatically detect render.yaml"
echo "5. Set the secret environment variable:"
echo "   - Key: ONEMINAI_API_KEY"
echo "   - Value: your_1minai_api_key_here"
echo "6. Click 'Apply'"
echo ""

# Display post-deployment steps
echo "üìù After Deployment:"
echo ""
echo "1. Your API will be available at:"
echo "   =
echo ""
echo "2. Test the deployment:"==
echo ""
echo "3. Update your frontend configuration:"
echo "   NEXT_PUBLIC_LITELLM_API_URL==
echo ""

# Check if git is initialized and has commits
if [ -d ".git" ]; then
    echo "‚úÖ Git repository detected"
    
    # Check if there are uncommitted changes
    if [ -n "$(git status --porcelain)" ]; then
        echo "‚ö†Ô∏è  You have uncommitted changes. Consider committing them before deployment:"
        echo "   git add ."
        echo "   git commit -m 'Update configuration for Render deployment'"
        echo "   git push origin main"
    else
        echo "‚úÖ No uncommitted changes"
    fi
else
    echo "‚ö†Ô∏è  No git repository found. Make sure your code is pushed to GitHub."
fi

echo ""
echo "üéâ Ready for Render deployment!"
echo "   Visit: https://dashboard.render.com to deploy"
</file>

<file path="simple_server.py">
#!/usr/bin/env python3
"""
Enhanced server for Psychiatry Therapy SuperBot LiteLLM Proxy.
Includes real 1minAI API integration with Python standard library only.
"""

import os
import time
import json
import logging
from typing import Dict, Any, List
from datetime import datetime
import urllib.request
import urllib.parse
import urllib.error

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Simple HTTP server using built-in modules
from http.server import HTTPServer, BaseHTTPRequestHandler
from urllib.parse import urlparse, parse_qs
import threading

# Environment variables
ONEMINAI_API_KEY = os.getenv("ONEMINAI_API_KEY")
PORT = int(os.getenv("PORT", "10000"))

# 1minAI API integration using urllib (built-in)
def make_1minai_request(messages, model="gemini-2.0-flash-lite"):
    """Make request to 1minAI API using only built-in urllib"""
    if not ONEMINAI_API_KEY:
        raise Exception("ONEMINAI_API_KEY not configured")
    
    # Transform messages to prompt format
    prompt_parts = []
    for msg in messages:
        role = msg.get("role", "user")
        content = msg.get("content", "")
        if role == "system":
            prompt_parts.append(f"System: {content}")
        elif role == "assistant":
            prompt_parts.append(f"Assistant: {content}")
        else:
            prompt_parts.append(f"User: {content}")
    
    prompt = "\n\n".join(prompt_parts)
    
    # Map model names to 1minAI supported format
    model_mapping = {
        "gemini-2.0-flash-lite": "gemini-2.0-flash-lite",
        "gemini-2.0-flash": "gemini-2.0-flash",
        "gemini-1.5-flash": "gemini-1.5-flash",
        "gemini-1.5-pro": "gemini-1.5-pro",
        "gpt-4o-mini": "gpt-4o-mini",
        "gpt-4o": "gpt-4o",
        "claude-3-5-sonnet": "claude-3-5-sonnet",
        "claude-3-haiku": "claude-3-haiku"
    }
    
    mapped_model = model_mapping.get(model, "gemini-2.0-flash-lite")
    
    # Create 1minAI payload
    payload = {
        "type": "CHAT_WITH_AI",
        "model": mapped_model,
        "promptObject": {
            "prompt": prompt,
            "isMixed": False,
            "webSearch": False
        }
    }
    
    # Prepare request (same as original FastAPI server)
    url = "https://api.1min.ai/api/features"
    headers = {
        "API-KEY": ONEMINAI_API_KEY,
        "Content-Type": "application/json"
    }
    
    try:
        # Make request using urllib (matching v2 project format exactly)
        data = json.dumps(payload).encode('utf-8')
        req = urllib.request.Request(url, data=data, headers=headers)
        
        logger.info(f"Making request to: https://api.1min.ai/api/features")
        logger.info(f"Request payload: {payload}")
        logger.info(f"Request headers (masked): API-KEY={ONEMINAI_API_KEY[:10] if ONEMINAI_API_KEY else 'None'}..., Content-Type=application/json")
        logger.info(f"Using model: {mapped_model}")
        
        with urllib.request.urlopen(req, timeout=60) as response:
            logger.info(f"1minAI API response status: {response.status}")
            if response.status == 200:
                result = json.loads(response.read().decode('utf-8'))
                logger.info(f"1minAI API request successful for model: {model}")
                logger.info(f"1minAI API response: {result}")
                
                # Parse 1minAI response format (exact same as v2)
                ai_record = result.get("aiRecord", {})
                ai_record_detail = ai_record.get("aiRecordDetail", {})
                result_object = ai_record_detail.get("resultObject", [])
                
                # Extract response text (exact same as v2)
                response_text = ""
                if isinstance(result_object, list) and result_object:
                    response_text = str(result_object[0])
                else:
                    response_text = "No response generated"
                
                return response_text
            else:
                error_text = response.read().decode('utf-8')
                logger.error(f"1minAI API error: {response.status} - {error_text}")
                return "I'm experiencing technical difficulties. Please try again later."
                
    except urllib.error.HTTPError as e:
        error_body = e.read().decode('utf-8') if hasattr(e, 'read') else str(e)
        logger.error(f"1minAI API HTTP error: {e.code} - {error_body}")
        return f"1minAI API is currently unavailable (Error: {e.code}). Please check the API configuration."
    except urllib.error.URLError as e:
        logger.error(f"1minAI API connection error: {str(e)}")
        return "I'm currently unable to connect to my AI service. Please try again later."
    except Exception as e:
        logger.error(f"Unexpected error in 1minAI request: {str(e)}")
        return "I encountered an unexpected error. Please try again later."

class SimpleHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        path = urlparse(self.path).path
        
        if path == "/health":
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            response = {
                "status": "healthy",
                "timestamp": datetime.utcnow().isoformat(),
                "service": "psychiatry-therapy-superbot-api",
                "version": "1.0.0"
            }
            self.wfile.write(json.dumps(response).encode())
            
        elif path == "/v1/models":
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            models = {
                "object": "list",
                "data": [
                    {
                        "id": "gemini-2.0-flash-lite",
                        "object": "model",
                        "created": int(time.time()),
                        "owned_by": "1minai"
                    },
                    {
                        "id": "gemini-2.0-flash",
                        "object": "model",
                        "created": int(time.time()),
                        "owned_by": "1minai"
                    },
                    {
                        "id": "gemini-1.5-flash",
                        "object": "model",
                        "created": int(time.time()),
                        "owned_by": "1minai"
                    },
                    {
                        "id": "gemini-1.5-pro",
                        "object": "model",
                        "created": int(time.time()),
                        "owned_by": "1minai"
                    },
                    {
                        "id": "gpt-4o-mini",
                        "object": "model",
                        "created": int(time.time()),
                        "owned_by": "1minai"
                    },
                    {
                        "id": "gpt-4o",
                        "object": "model",
                        "created": int(time.time()),
                        "owned_by": "1minai"
                    },
                    {
                        "id": "claude-3-5-sonnet",
                        "object": "model",
                        "created": int(time.time()),
                        "owned_by": "1minai"
                    },
                    {
                        "id": "claude-3-haiku",
                        "object": "model",
                        "created": int(time.time()),
                        "owned_by": "1minai"
                    }
                ]
            }
            self.wfile.write(json.dumps(models).encode())
            
        elif path == "/":
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            response = {
                "service": "Psychiatry Therapy SuperBot LiteLLM Proxy",
                "version": "1.0.0",
                "status": "running",
                "endpoints": {
                    "health": "/health",
                    "chat_completions": "/v1/chat/completions",
                    "models": "/v1/models"
                }
            }
            self.wfile.write(json.dumps(response).encode())
        else:
            self.send_response(404)
            self.send_header('Content-type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            self.wfile.write(json.dumps({"error": "Not Found"}).encode())

    def do_POST(self):
        if self.path in ["/v1/chat/completions", "/chat/completions"]:
            try:
                content_length = int(self.headers['Content-Length'])
                post_data = self.rfile.read(content_length)
                request_data = json.loads(post_data.decode('utf-8'))
                
                # Extract request parameters
                messages = request_data.get("messages", [])
                model = request_data.get("model", "gemini-2.0-flash-lite")
                
                if not messages:
                    raise ValueError("Messages array is required")
                
                logger.info(f"Processing chat request with {len(messages)} messages for model: {model}")
                
                # Make real request to 1minAI API
                ai_response = make_1minai_request(messages, model)
                
                # Create OpenAI-compatible response
                response = {
                    "id": f"chatcmpl-{int(time.time())}",
                    "object": "chat.completion",
                    "created": int(time.time()),
                    "model": model,
                    "choices": [
                        {
                            "index": 0,
                            "message": {
                                "role": "assistant",
                                "content": ai_response
                            },
                            "finish_reason": "stop"
                        }
                    ],
                    "usage": {
                        "prompt_tokens": sum(len(msg.get("content", "").split()) for msg in messages),
                        "completion_tokens": len(ai_response.split()),
                        "total_tokens": sum(len(msg.get("content", "").split()) for msg in messages) + len(ai_response.split())
                    }
                }
                
                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.send_header('Access-Control-Allow-Origin', '*')
                self.end_headers()
                self.wfile.write(json.dumps(response).encode())
                
                logger.info(f"Successfully processed chat request")
                
            except Exception as e:
                logger.error(f"Error processing chat request: {e}")
                
                # Return error response in OpenAI format
                error_response = {
                    "id": f"chatcmpl-{int(time.time())}",
                    "object": "chat.completion",
                    "created": int(time.time()),
                    "model": request_data.get("model", "gemini-2.0-flash-lite") if 'request_data' in locals() else "gemini-2.0-flash-lite",
                    "choices": [
                        {
                            "index": 0,
                            "message": {
                                "role": "assistant",
                                "content": f"I apologize, but I encountered an error: {str(e)}. Please try again."
                            },
                            "finish_reason": "stop"
                        }
                    ],
                    "usage": {
                        "prompt_tokens": 10,
                        "completion_tokens": 20,
                        "total_tokens": 30
                    }
                }
                
                self.send_response(500)
                self.send_header('Content-type', 'application/json')
                self.send_header('Access-Control-Allow-Origin', '*')
                self.end_headers()
                self.wfile.write(json.dumps(error_response).encode())
        else:
            self.send_response(404)
            self.send_header('Content-type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            self.wfile.write(json.dumps({"error": "Not Found"}).encode())

    def do_OPTIONS(self):
        self.send_response(200)
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type, Authorization')
        self.end_headers()

    def log_message(self, format, *args):
        logger.info(f"{self.address_string()} - {format % args}")

if __name__ == "__main__":
    logger.info(f"Starting Psychiatry Therapy SuperBot API on port {PORT}")
    logger.info(f"1minAI API Key configured: {bool(ONEMINAI_API_KEY)}")
    if ONEMINAI_API_KEY:
        logger.info(f"API Key length: {len(ONEMINAI_API_KEY)} characters")
        logger.info(f"API Key starts with: {ONEMINAI_API_KEY[:10]}...")
    else:
        logger.warning("‚ö†Ô∏è  ONEMINAI_API_KEY environment variable not set!")
        logger.warning("Please set it in Render dashboard under Environment tab")
    
    server = HTTPServer(('0.0.0.0', PORT), SimpleHandler)
    
    try:
        logger.info(f"Server running at http://0.0.0.0:{PORT}")
        server.serve_forever()
    except KeyboardInterrupt:
        logger.info("Shutting down server...")
        server.shutdown()
</file>

<file path="fastapi_server.py">
#!/usr/bin/env python3
"""
FastAPI server for LiteLLM 1minAI proxy integration.
This server provides OpenAI-compatible endpoints for Psychiatry Therapy SuperBot.
"""

import os
import asyncio
import logging
import time
from typing import Dict, Any, Optional, List
from datetime import datetime
import json
import httpx
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

from fastapi import FastAPI, HTTPException, Request, Response
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
import uvicorn

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI(
    title="LiteLLM 1minAI Proxy for RAG Superbot",
    description="OpenAI-compatible proxy for 1minAI integration with RAG Superbot",
    version="1.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic models for request/response
class ChatMessage(BaseModel):
    role: str = Field(..., description="Message role (system, user, assistant)")
    content: str = Field(..., description="Message content")

class ChatCompletionRequest(BaseModel):
    model: str = Field(default="gemini-2.0-flash-lite", description="Model to use")
    messages: List[ChatMessage] = Field(..., description="List of chat messages")
    temperature: Optional[float] = Field(default=0.7, description="Sampling temperature")
    max_tokens: Optional[int] = Field(default=None, description="Maximum tokens to generate")
    stream: Optional[bool] = Field(default=False, description="Whether to stream response")

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Dict[str, Any]]
    usage: Dict[str, int]

class HealthResponse(BaseModel):
    status: str
    timestamp: str
    service: str = "litellm-1minai-proxy"
    version: str = "1.0.0"

# Global variables for configuration
ONEMINAI_API_KEY = os.getenv("ONEMINAI_API_KEY")
LITELLM_BASE_URL = os.getenv("LITELLM_BASE_URL", "https://api.1min.ai")

# 1minAI API integration functions
async def make_1minai_request(messages: List[ChatMessage], model: str, temperature: float = 0.7, max_tokens: Optional[int] = None) -> Dict[str, Any]:
    """
    Make a real request to 1minAI API using the correct endpoint and format.
    """
    if not ONEMINAI_API_KEY:
        raise HTTPException(
            status_code=500,
            detail="ONEMINAI_API_KEY not configured"
        )
    
    # Transform messages to prompt format
    prompt_parts = []
    for msg in messages:
        role = msg.role
        content = msg.content
        if role == "system":
            prompt_parts.append(f"System: {content}")
        elif role == "assistant":
            prompt_parts.append(f"Assistant: {content}")
        else:
            prompt_parts.append(f"User: {content}")
    
    prompt = "\n\n".join(prompt_parts)
    
    # Map model names to 1minAI supported format
    model_mapping = {
        "1minai-gpt-4o-mini": "gpt-4o-mini",
        "1minai-gpt-4o": "gpt-4o",
        "1minai-claude-3-5-sonnet": "claude-3-5-sonnet",
        "1minai-claude-3-haiku": "claude-3-haiku",
        "gpt-4o-mini": "gpt-4o-mini",
        "gpt-4o": "gpt-4o",
        "claude-3-5-sonnet": "claude-3-5-sonnet",
        "claude-3-haiku": "claude-3-haiku",
        "gemini-2.0-flash-lite": "gemini-2.0-flash-lite",
        "gemini-2.0-flash": "gemini-2.0-flash",
        "gemini-1.5-flash": "gemini-1.5-flash",
        "gemini-1.5-pro": "gemini-1.5-pro"
    }
    
    # Use mapped model name or fallback to gemini-2.0-flash-lite
    mapped_model = model_mapping.get(model, "gemini-2.0-flash-lite")
    
    # Create 1minAI payload
    payload = {
        "type": "CHAT_WITH_AI",
        "model": mapped_model,
        "promptObject": {
            "prompt": prompt,
            "isMixed": False,
            "webSearch": False
        }
    }
    
    headers = {
        "API-KEY": ONEMINAI_API_KEY,
        "Content-Type": "application/json"
    }
    
    try:
        logger.info(f"Making request to: https://api.1min.ai/api/features")
        logger.info(f"Request payload: {payload}")
        logger.info(f"Request headers (masked): API-KEY={ONEMINAI_API_KEY[:10]}..., Content-Type=application/json")
        logger.info(f"Using model: {mapped_model}")
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                "https://api.1min.ai/api/features",
                json=payload,
                headers=headers
            )
            logger.info(f"1minAI API response status: {response.status_code}")
            if response.status_code == 200:
                result = response.json()
                logger.info(f"1minAI API request successful for model: {model}")
                logger.info(f"1minAI API response: {result}")
                
                # Parse 1minAI response format
                ai_record = result.get("aiRecord", {})
                ai_record_detail = ai_record.get("aiRecordDetail", {})
                result_object = ai_record_detail.get("resultObject", [])
                
                # Extract response text
                response_text = ""
                if isinstance(result_object, list) and result_object:
                    response_text = str(result_object[0])
                else:
                    response_text = "No response generated"
                
                # Convert to OpenAI format
                openai_response = {
                    "id": f"chatcmpl-{int(time.time())}",
                    "object": "chat.completion",
                    "created": int(time.time()),
                    "model": model,
                    "choices": [
                        {
                            "index": 0,
                            "message": {
                                "role": "assistant",
                                "content": response_text
                            },
                            "finish_reason": "stop"
                        }
                    ],
                    "usage": {
                        "prompt_tokens": len(prompt.split()),
                        "completion_tokens": len(response_text.split()),
                        "total_tokens": len(prompt.split()) + len(response_text.split())
                    }
                }
                
                return openai_response
            else:
                error_text = response.text
                logger.error(f"1minAI API error: {response.status_code} - {error_text}")
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"1minAI API error: {error_text}"
                )
    except httpx.RequestError as e:
        logger.error(f"1minAI API connection error: {str(e)}")
        raise HTTPException(
            status_code=503,
            detail=f"1minAI API connection failed: {str(e)}"
        )
    except Exception as e:
        logger.error(f"Unexpected error in 1minAI request: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Internal error: {str(e)}"
        )

async def get_1minai_models() -> List[Dict[str, Any]]:
    """
    Get available models from 1minAI.
    """
    if not ONEMINAI_API_KEY:
        logger.warning("ONEMINAI_API_KEY not configured")
        return []
    
    # Return supported models
    models = [
        {
            "id": "gemini-2.0-flash-lite",
            "object": "model",
            "created": int(time.time()),
            "owned_by": "1minai"
        },
        {
            "id": "gemini-2.0-flash",
            "object": "model",
            "created": int(time.time()),
            "owned_by": "1minai"
        },
        {
            "id": "gemini-1.5-flash",
            "object": "model",
            "created": int(time.time()),
            "owned_by": "1minai"
        },
        {
            "id": "gemini-1.5-pro",
            "object": "model",
            "created": int(time.time()),
            "owned_by": "1minai"
        },
        {
            "id": "gpt-4o-mini",
            "object": "model",
            "created": int(time.time()),
            "owned_by": "1minai"
        },
        {
            "id": "gpt-4o",
            "object": "model", 
            "created": int(time.time()),
            "owned_by": "1minai"
        },
        {
            "id": "claude-3-5-sonnet",
            "object": "model",
            "created": int(time.time()),
            "owned_by": "1minai"
        },
        {
            "id": "claude-3-haiku",
            "object": "model",
            "created": int(time.time()),
            "owned_by": "1minai"
        },
    ]
    
    logger.info(f"Returning {len(models)} 1minAI models")
    return models

# Health check endpoint
@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint for service monitoring."""
    return HealthResponse(
        status="healthy",
        timestamp=datetime.utcnow().isoformat(),
        service="litellm-1minai-proxy-rag-superbot",
        version="1.0.0"
    )

# Chat completion endpoint
@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
@app.post("/chat/completions", response_model=ChatCompletionResponse)
async def chat_completions(request: ChatCompletionRequest):
    """
    OpenAI-compatible chat completions endpoint.
    Proxies requests to 1minAI.
    """
    try:
        # Validate API key
        if not ONEMINAI_API_KEY:
            raise HTTPException(
                status_code=500,
                detail="ONEMINAI_API_KEY not configured"
            )
        
        # Log request for debugging
        logger.info(f"Chat completion request for model: {request.model}")
        logger.info(f"Request messages: {len(request.messages)} messages")
        
        # Make real request to 1minAI API
        try:
            logger.info(f"Making request to 1minAI API for model: {request.model}")
            result = await make_1minai_request(
                messages=request.messages,
                model=request.model,
                temperature=request.temperature,
                max_tokens=request.max_tokens
            )
            
            logger.info(f"1minAI API response received")
            
            # Transform to response model
            response = ChatCompletionResponse(
                id=result.get("id", f"chatcmpl-{datetime.utcnow().timestamp()}"),
                created=result.get("created", int(datetime.utcnow().timestamp())),
                model=result.get("model", request.model),
                choices=result.get("choices", []),
                usage=result.get("usage", {})
            )
            
            logger.info(f"Successfully processed 1minAI request for model: {request.model}")
            return response
            
        except HTTPException as e:
            # Log the error and return a fallback response
            logger.error(f"1minAI API error: {e.detail}")
            logger.warning("Falling back to error response")
            
            # Return a fallback response
            response_id = f"chatcmpl-{datetime.utcnow().timestamp()}"
            fallback_response = ChatCompletionResponse(
                id=response_id,
                created=int(datetime.utcnow().timestamp()),
                model=request.model,
                choices=[
                    {
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": f"1minAI API is currently unavailable (Error: {e.detail}). Please check the API configuration."
                        },
                        "finish_reason": "stop"
                    }
                ],
                usage={
                    "prompt_tokens": 10,
                    "completion_tokens": 20,
                    "total_tokens": 30
                }
            )
            return fallback_response
            
        except Exception as e:
            logger.error(f"Error processing 1minAI request: {str(e)}")
            # Return a fallback response
            response_id = f"chatcmpl-{datetime.utcnow().timestamp()}"
            fallback_response = ChatCompletionResponse(
                id=response_id,
                created=int(datetime.utcnow().timestamp()),
                model=request.model,
                choices=[
                    {
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": f"Error processing request: {str(e)}"
                        },
                        "finish_reason": "stop"
                    }
                ],
                usage={
                    "prompt_tokens": 10,
                    "completion_tokens": 20,
                    "total_tokens": 30
                }
            )
            return fallback_response
        
    except HTTPException:
        # Re-raise HTTP exceptions
        raise
    except Exception as e:
        logger.error(f"Error in chat completions: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Internal server error: {str(e)}"
        )

# Models endpoint
@app.get("/v1/models")
async def list_models():
    """List available models from 1minAI."""
    try:
        # Get models
        models = await get_1minai_models()
        
        if models:
            logger.info(f"Retrieved {len(models)} models from 1minAI")
            return {
                "object": "list",
                "data": models
            }
        else:
            # Fallback to default model
            logger.warning("Failed to get models from 1minAI, using fallback")
            return {
                "object": "list",
                "data": [
                    {
                        "id": "gpt-4o-mini",
                        "object": "model",
                        "created": int(datetime.utcnow().timestamp()),
                        "owned_by": "1minai"
                    }
                ]
            }
    except Exception as e:
        logger.error(f"Error getting models: {str(e)}")
        # Return fallback models
        return {
            "object": "list",
            "data": [
                {
                    "id": "gpt-4o-mini",
                    "object": "model",
                    "created": int(datetime.utcnow().timestamp()),
                    "owned_by": "1minai"
                }
            ]
        }

# Root endpoint
@app.get("/")
async def root():
    """Root endpoint with service information."""
    return {
        "service": "LiteLLM 1minAI Proxy for RAG Superbot",
        "version": "1.0.0",
        "status": "running",
        "endpoints": {
            "health": "/health",
            "chat_completions": "/v1/chat/completions",
            "models": "/v1/models"
        }
    }

if __name__ == "__main__":
    # Configuration - Render uses PORT env var (default 10000), Railway uses PORT, fallback to FASTAPI_PORT
    host = os.getenv("FASTAPI_HOST", "0.0.0.0")
    port = int(os.getenv("PORT", os.getenv("FASTAPI_PORT", "8000")))
    reload = os.getenv("FASTAPI_RELOAD", "false").lower() == "true"
    
    # Detect deployment environment
    environment = "local"
    if os.getenv("RENDER_SERVICE_NAME"):
        environment = "render"
    elif os.getenv("RAILWAY_ENVIRONMENT"):
        environment = "railway"
    
    logger.info(f"Starting LiteLLM 1minAI Proxy server on {host}:{port}")
    logger.info(f"1minAI API Key configured: {bool(ONEMINAI_API_KEY)}")
    logger.info(f"Environment: {environment}")
    
    # Run the server
    uvicorn.run(
        "fastapi_server:app",
        host=host,
        port=port,
        reload=reload,
        log_level="info"
    )
</file>

<file path="DEPLOYMENT.md">
# Deployment Guide - Psychiatry Therapy SuperBot

This guide covers deploying the Psychiatry Therapy SuperBot with:
- **Backend**: Render (Docker Compose equivalent - FastAPI LiteLLM Proxy)
- **Frontend**: Vercel (Next.js Application)

## üèóÔ∏è Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Vercel        ‚îÇ    ‚îÇ  Render          ‚îÇ    ‚îÇ   1minAI        ‚îÇ
‚îÇ   (Frontend)    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  (Docker Compose ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   API           ‚îÇ
‚îÇ   Next.js App   ‚îÇ    ‚îÇ   equivalent)    ‚îÇ    ‚îÇ   (AI Models)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üê≥ Docker Compose to Render

Render deploys your **FastAPI server** in the cloud with free tier optimizations:
- ‚úÖ Python 3 runtime (free tier compatible, no compilation issues)
- ‚úÖ Same environment variables (with PORT=10000 for Render)
- ‚úÖ Same FastAPI server functionality
- ‚úÖ Same health checks
- ‚úÖ **Plus** auto-scaling, SSL, monitoring, and zero-downtime deployments

**Guides:**
- [RENDER-FREE-TIER.md](./RENDER-FREE-TIER.md) - Free tier deployment guide
- [DOCKER-COMPOSE-TO-RENDER.md](./DOCKER-COMPOSE-TO-RENDER.md) - Detailed comparison

## üìã Prerequisites

1. **Render Account** (sign up at https://render.com)
2. **Vercel Account** 
3. **1minAI API Key** (get from https://1min.ai)
4. **Google Gemini API Key** (for embeddings/fallback)
5. **Qdrant Cloud Account** (for vector database)
6. **Git Repository** (GitHub, GitLab, or Bitbucket)

## üöÄ Step 1: Deploy Docker Compose Backend to Render

Render will deploy your Docker Compose configuration using Blueprint (render.yaml) with the same Dockerfile and environment variables.

### 1.1 Install Render CLI (Optional)

```bash
# macOS
brew install render

# Linux
curl -fsSL https://cli.render.com/install | sh

# Or use web dashboard (no CLI needed)
```

### 1.2 Login to Render

```bash
render auth login
# Or use the web dashboard at https://dashboard.render.com
```

### 1.3 Prepare Git Repository

```bash
# Make sure your code is in a git repository
git init
git add .
git commit -m "Initial commit for Render deployment"

# Push to GitHub/GitLab (or connect via Render dashboard)
git remote add origin https://github.com/yourusername/your-repo.git
git push -u origin main
```

### 1.4 Deploy Using Blueprint

Render uses `render.yaml` (Blueprint) to deploy your docker-compose equivalent:

#### Method 1: Automated Script
```bash
chmod +x deploy-render.sh
./deploy-render.sh
```

#### Method 2: Manual Blueprint Deployment
```bash
render blueprint launch
```

#### Method 3: Web Dashboard
1. Go to https://dashboard.render.com
2. Click "New" ‚Üí "Blueprint"
3. Connect your Git repository
4. Render will automatically detect `render.yaml`
5. Click "Apply" to deploy

### 1.5 Set Secret Environment Variables

After deployment, set your API key in the Render dashboard:

1. Go to your service in Render dashboard
2. Click "Environment" tab
3. Add secret variable:
   - `ONEMINAI_API_KEY`: Your 1minAI API key

All other environment variables are already set in `render.yaml`.

### 1.6 Deployment Process

**What happens during Render deployment:**
- Render reads your `render.yaml` Blueprint configuration
- Uses Python 3 runtime (optimized for free tier, no external dependencies)
- No pip install needed - uses Python standard library only
- Starts the server with `python simple_server.py`
- Sets up environment variables (same as your docker-compose.yml)
- Deploys with automatic HTTPS, scaling, and zero-downtime deployments

### 1.7 Get Your Render URL

After deployment, Render provides you with a URL like:
- `
- Plus free SSL certificate and optional custom domains

### 1.8 Test Your Render Deployment

```bash
# Test health endpoint
curl 

# Test models endpoint
curl 

# Test chat completion
curl -X POST 
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-2.0-flash-lite",
    "messages": [{"role": "user", "content": "Hello, how are you?"}]
  }'
```

## üåê Step 2: Deploy Frontend to Vercel

### 2.1 Install Vercel CLI

```bash
npm install -g vercel
```

### 2.2 Login to Vercel

```bash
vercel login
```

### 2.3 Set Environment Variables in Vercel

Go to your Vercel dashboard and add these environment variables:

```bash
# LiteLLM Configuration (Primary)
NEXT_PUBLIC_USE_LITELLM=true
NEXT_PUBLIC_LITELLM_API_URL=https://rag-superbot-litellm-v3-psych.onrender.com
NEXT_PUBLIC_LITELLM_MODEL=gemini-2.0-flash-lite
NEXT_PUBLIC_LITELLM_TEMPERATURE=0.7
NEXT_PUBLIC_LITELLM_MAX_TOKENS=2048

# Google Gemini Configuration (Fallback/Embeddings)
NEXT_PUBLIC_GOOGLE_API_KEY=your_gemini_api_key_here
NEXT_PUBLIC_GEMINI_MODEL=gemini-2.5-flash
NEXT_PUBLIC_GEMINI_TEMPERATURE=0.7
NEXT_PUBLIC_GEMINI_MAX_TOKENS=2048
NEXT_PUBLIC_EMBEDDING_MODEL=gemini-embedding-001
NEXT_PUBLIC_EMBEDDING_DIM=3072

# Qdrant Cloud Configuration
NEXT_PUBLIC_QDRANT_CLOUD_URL=your_qdrant_cloud_url_here
NEXT_PUBLIC_QDRANT_CLOUD_API_KEY=your_qdrant_cloud_api_key_here

# Vector Store Configuration
NEXT_PUBLIC_VECTOR_STORE=qdrant
NEXT_PUBLIC_COLLECTION_NAME=psychiatry_therapy_v1_google-001

# App Configuration
NEXT_PUBLIC_APP_NAME=Psychiatry Therapy SuperBot
NEXT_PUBLIC_APP_VERSION=1.0.0

# Production Settings
NODE_ENV=production
```

### 2.4 Deploy to Vercel

```bash
# Navigate to frontend directory
cd frontend

# Install dependencies
npm install --legacy-peer-deps

# Deploy to Vercel
vercel --prod
```

## üîß Step 3: Configuration & Testing

### 3.1 Update Frontend Configuration

Make sure your frontend is pointing to the correct Render URL:

```javascript
// In your frontend code, the NEXT_PUBLIC_LITELLM_API_URL should be:
const LITELLM_API_URL = process.env.NEXT_PUBLIC_LITELLM_API_URL || 'https://rag-superbot-litellm-v3-psych.onrender.com';
```

### 3.2 Test End-to-End Flow

1. **Visit your Vercel app URL**
2. **Try a chat message** - it should go through this flow:
   - Frontend (Vercel) ‚Üí Render (FastAPI) ‚Üí 1minAI API ‚Üí Response back
3. **Check browser network tab** to verify API calls are working
4. **Test different pipeline modes** to ensure all functionality works

### 3.3 Monitor and Debug

#### Render Logs
```bash
# View real-time logs
render logs -s psychiatry-therapy-superbot-api --tail

# Or use the web dashboard
# Visit https://dashboard.render.com ‚Üí Your Service ‚Üí Logs
```

#### Vercel Logs
- Check the Vercel dashboard for function logs
- Use browser developer tools for frontend debugging

## üõ†Ô∏è Automated Deployment Scripts

### Deploy Backend (Render)
```bash
chmod +x deploy-render.sh
./deploy-render.sh
```

### Deploy Frontend (Vercel)
```bash
chmod +x deploy-vercel.sh
./deploy-vercel.sh
```

## üîí Security Considerations

### Render Security
- ‚úÖ Environment variables encrypted
- ‚úÖ HTTPS by default with free SSL certificates
- ‚úÖ Private networking available
- ‚úÖ Automatic security headers
- ‚úÖ Built-in DDoS protection
- ‚úÖ SOC 2 Type II compliant

### Vercel Security
- ‚úÖ Environment variables encrypted
- ‚úÖ HTTPS by default
- ‚úÖ Edge network for performance
- ‚úÖ Automatic security headers

## üìä Monitoring & Analytics

### Render Analytics
- Monitor CPU and memory usage with detailed metrics
- Track request volume and response times
- View application logs in real-time with search
- Set up alerts for downtime or errors
- Performance insights and recommendations

### Vercel Analytics
- Monitor frontend performance
- Track user interactions
- Monitor build and deployment status

## üö® Troubleshooting

### Common Issues

#### 1. CORS Errors
```
Error: CORS policy blocked
```
**Solution**: Verify CORS headers in Cloudflare Worker and Vercel configuration

#### 2. API Key Issues
```
Error: ONEMINAI_API_KEY not configured
```
**Solution**: Ensure environment variable is set in Render dashboard under Environment tab

#### 3. Render Blueprint Issues
```
Error: docker runtime must not have startCommand
```
**Solution**: Remove `startCommand` from render.yaml - Render uses the `CMD` from Dockerfile

#### 4. Port Issues
```
Error: Service not responding on expected port
```
**Solution**: Ensure your app listens on `PORT` environment variable (Render sets this to 10000)

#### 5. Free Tier Compilation Issues
```
Error: pydantic-core compilation failed / Rust compilation error
Error: aiohttp compilation failed / C extension error
```
**Solution**: Use `requirements-render.txt` with pre-compiled packages and Python 3.11 runtime

#### 6. Python Version Issues
```
Error: ForwardRef._evaluate() missing 1 required keyword-only argument
```
**Solution**: Use newer package versions that are Python 3.13 compatible in `requirements-render.txt`

#### 7. Memory Issues on Free Tier
```
Error: Build killed / Out of memory
```
**Solution**: Use lighter dependencies in `requirements-render.txt` or upgrade to paid plan

#### 3. Environment Variables Not Loading
```
Error: NEXT_PUBLIC_LITELLM_API_URL not found
```
**Solution**: Check Vercel dashboard environment variables

#### 4. Build Failures
```
Error: Build failed
```
**Solution**: Check Node.js version compatibility and dependencies

### Debug Commands

```bash
# Test Render deployment locally (using Docker)
docker build -f Dockerfile.fastapi -t psychiatry-therapy-superbot .
docker run -p 10000:10000 --env-file .env psychiatry-therapy-superbot

# View Render logs
render logs -s psychiatry-therapy-superbot-api --tail

# Test Vercel build locally
cd frontend && npm run build

# Check environment variables
vercel env ls
# Check Render environment variables in dashboard

# View deployment logs
vercel logs YOUR_DEPLOYMENT_URL
render logs -s psychiatry-therapy-superbot-api
```

## üìà Performance Optimization

### Render Optimization
- Use Render's auto-scaling features (automatic)
- Monitor resource usage and upgrade plan if needed
- Implement proper caching in FastAPI
- Use Render's built-in load balancing and CDN
- Enable zero-downtime deployments

### Vercel Optimization
- Enable Edge Functions for better performance
- Use Image Optimization for assets
- Implement proper caching headers

## üîÑ CI/CD Pipeline (Optional)

### GitHub Actions for Automated Deployment

Create `.github/workflows/deploy.yml`:

```yaml
name: Deploy Psychiatry Therapy SuperBot

on:
  push:
    branches: [main]

jobs:
  deploy-backend:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: '18'
      - name: Install Render CLI
        run: |
          curl -fsSL https://cli.render.com/install | sh
          echo "$HOME/.local/bin" >> $GITHUB_PATH
      - name: Deploy to Render
        run: render blueprint launch --yes
        env:
          RENDER_API_KEY: ${{ secrets.RENDER_API_KEY }}

  deploy-frontend:
    runs-on: ubuntu-latest
    needs: deploy-backend
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: '18'
      - name: Deploy to Vercel
        run: |
          cd frontend
          npm install --legacy-peer-deps
          vercel --prod --token ${{ secrets.VERCEL_TOKEN }}
        env:
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
```

## üìû Support

If you encounter issues:

1. Check the troubleshooting section above
2. Review Cloudflare Worker and Vercel logs
3. Verify all environment variables are set correctly
4. Test API endpoints individually

---

üéâ **Congratulations!** Your Psychiatry Therapy SuperBot is now deployed on Render and Vercel, ready to help users with mental health questions using advanced AI capabilities!

## üöÄ Render Benefits

- **Easy Docker Deployment**: Your existing FastAPI server deploys seamlessly
- **Auto-scaling**: Render automatically scales based on demand
- **Built-in Monitoring**: Real-time logs, metrics, and performance insights
- **Zero-downtime Deployments**: Git-based deployments with health checks
- **Environment Management**: Secure environment variable handling
- **Free SSL Certificates**: Automatic HTTPS for all domains
- **Custom Domains**: Easy custom domain setup with automatic SSL
- **Database Integration**: Easy integration with PostgreSQL, Redis, etc.
- **Global CDN**: Built-in content delivery network

## üì± Quick Render Commands

```bash
# View logs
render logs -s psychiatry-therapy-superbot-api --tail

# Check service status
render services list

# Open service in browser
render open -s psychiatry-therapy-superbot-api

# Deploy latest changes
render blueprint launch

# View service details
render services get psychiatry-therapy-superbot-api
```
</file>

<file path="requirements-render.txt">
# Render Free Tier Compatible Dependencies
# These versions avoid Rust compilation issues on Render's free tier
--only-binary=pydantic,pydantic-core
fastapi==0.121.0
uvicorn==0.24.0
pydantic==2.12.4 
python-multipart==0.0.6
python-dotenv==1.0.0


# Using httpx instead of aiohttp to avoid C extension compilation
httpx==0.25.2

# Note: These versions are specifically chosen to work on Render's free tier
# without requiring Rust or C compilation
</file>

<file path="render.yaml">
# Render Blueprint for Psychiatry Therapy SuperBot (Free Tier Optimized)
# Using Python runtime instead of Docker to avoid compilation issues

services:
  # FastAPI LiteLLM Proxy Service
  - type: web
    name: psychiatry-therapy-superbot-api
    env: python3
    plan: free  # Free tier
    region: oregon  # Choose region closest to your users
    branch: main
    buildCommand: "python --version && echo 'Using Python standard library only - no dependencies to install'"
    startCommand: "python simple_server.py"
    healthCheckPath: /health
    
    # Environment Variables (same as docker-compose.yml)
    envVars:
      - key: FASTAPI_HOST
        value: "0.0.0.0"
      # Note: Render automatically sets PORT=10000, no need to override
      - key: FASTAPI_RELOAD
        value: "false"
      - key: FASTAPI_LOG_LEVEL
        value: "info"
      - key: LITELLM_BASE_URL
        value: "https://api.1min.ai"
      - key: DEFAULT_MODEL
        value: "gemini-2.0-flash-lite"
      - key: MAX_TOKENS
        value: "4096"
      - key: TEMPERATURE
        value: "0.7"
      - key: CORS_ORIGINS
        value: "*"
      - key: CORS_ALLOW_CREDENTIALS
        value: "true"
      - key: HEALTH_CHECK_INTERVAL
        value: "30"
      # Secret environment variables (set in Render dashboard)
      - key: ONEMINAI_API_KEY
        sync: false  # This will be set as a secret in Render dashboard

    # Auto-deploy on git push
    autoDeploy: true
    
    # Custom domains (optional)
    # domains:
    #   - psychiatry-therapy-superbot-api.yourdomain.com
</file>

</files>
